{"./":{"url":"./","title":"前言","keywords":"","body":"Introduction 面试资料 前言 安装技术文档 GitBook Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-02-13 21:43:51 "},"Chapter1/":{"url":"Chapter1/","title":"安装软件章节","keywords":"","body":"第一章 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-02-12 23:35:54 "},"Chapter1/Gitbook简明安装.html":{"url":"Chapter1/Gitbook简明安装.html","title":"Gitbook简明安装","keywords":"","body":"Gitbook简明安装 1.Gitbook 简介 Gitbook (opens new window)是一个使用 Git 和 Markdown 来构建电子书籍的开源工具。它既可以生成一个静态网站，也可以输出内容作为电子书（ePub，Mobi，PDF）。Gitbook 可以在本地、Github、VPS 等平台上部署，本文采用的是借助 Node.js 在本地部署然后推送到 Github Page 托管的方式。 环境： Node.js 环境：Gitbook 在本地部署时所需要的环境。 Markdown 编辑器：推荐使用 Typora，用来编写 Gitbook 文档的具体内容。 Git 工具 +Github 账号：用于将 Gitbook 托管到 Github 存储库上。 翻墙工具：如果没有，可以私信我推荐一个我正在用的 2.Gitbook本地部署 安装node.js 因为 GitBook 是基于 Node.js 的，所以我们首先需要下载安装 Node.js (opens new window)（对应平台的.msi版本即可）注意node版本太高会影响后续安装。 测试：打开cmd，输入node -v和npm -v，若显示node和npm的版本号即安装成功。作者的分别是v20.18.0和10.8.2 截止到目前的 Gitbook V3.2.3版本，需要使用NodeJs的v10+版本，否则会产生各种报错。 这里建议下载v10.23.1版本，官网最新版本我试了也是不行的。Index of /download/release/v10.23.1/ 安装Gitbook npm install -g gitbook-cli # cmd中运行 安装完之后，就会多了一个 gitbook 命令（如果没有，请确认上面的命令是否加了 -g） 安装Git 官网下载 Git：https://www.git-scm.com/download/win(opens new window) 安装过程中一路 next 即可，安装成功后桌面右键单击出现 Git GUI Here 以及 Git Bash Here 安装typora 其他md编辑器也行，typora收费了，但有教程破解 官网下载 Typora：https://www.typora.io/(opens new window) 说明：Typora 可以配合 pandoc 插件将 Markdown 转换成其他格式的文件 3.Gitbook本地使用 初始化Gitbook 新建一个存储Gitbook的文件夹，切换到该目录下，然后初始化 gitbook init # 执行完后，你会看到多了两个文件 —— README.md 和 SUMMARY.md，它们的作用如下： README.md —— 书籍的介绍写在这个文件里 SUMMARY.md —— 书籍的目录结构在这里配置 用md编辑器编辑一个示例 编辑SUMMARY.md # 目录 - [前言](README.md) - [第一章](Chapter1/README.md) - [第1节：衣](Chapter1/衣.md) - [第2节：食](Chapter1/食.md) - [第3节：住](Chapter1/住.md) - [第4节：行](Chapter1/行.md) - [第二章](Chapter2/README.md) - [第三章](Chapter3/README.md) - [第四章](Chapter4/README.md) 在本地部署Gitbook 回到命令行，在 Gitbook 的工作区文件夹中再次执行 gitbook init 命令。GitBook 会查找 SUMMARY.md 文件中描述的目录和文件，如果没有则会将其创建。 接着我们再执行 gitbook serve 命令，将其部署在本地，打开 Chrome 访问：http://localhost:4000 (opens new window)，即可看到本地部署的 Gitbook（注：serve 命令可以指定端口 gitbook serve --port 2333） 当你写得差不多，你可以执行 gitbook build 命令构建书籍，默认将生成的静态网站输出到 _book 目录。实际上，这一步也包含在 gitbook serve 里面（注：build 命令可以指定路径 gitbook build [书籍路径] [输出路径]，如果你想查看输出目录详细的记录，可使用 gitbook build ./ --log=debug --debug 来查看） 问题： node.js版本问题 问题描述：编译时报错 polyfills.js:287 if (cb) cb.apply(this, arguments) TypeError: cb.apply is not a function 产生原因：因开发前端项目时升级了node.js版本所致，不支持这个版本的gitbook。 解决办法：一种是把node.js降级回去，第二种方式是注释掉报错代码（这个函数的作用是用来修复node.js的一些bug，用处不大） 打开C:\\Users\\xxx\\AppData\\Roaming\\npm\\node_modules\\gitbook-cli\\node_modules\\npm\\node_modules\\graceful-fs\\polyfills.js文件，找到287行的statFix函数，把它的调用处注释掉即可。 4.Gitbook配置文件 如果你想对你的网站有更详细的个性化配置或使用插件，那么需要使用配置文件。配置文件写完后，需要重启服务或者重新打包才能应用配置。Gitbook 的配置文件是book.json， 请在项目的根目录处自行创建。 4.1 配置文件主要参数 title：标题 author：作者 description：描述，对应 Gitbook 网站的 description language：使用的语言，zh-hans 是简体中文，会对应到页面的 structure：指定 Readme、Summary、Glossary 和 Languages 对应的文件名 plugins：使用的插件列表，所有的插件都在这里写出来，然后使用 gitbook install 来安装。 pluginsConfig：插件的配置信息，如果插件需要配置参数，那么在这里填写。 links：目前可以给侧导航栏添加链接信息 styles：自定义页面样式，各种格式对应各自的 css 文件 4.2 Gitbook 的常用插件 4.2.1 Gitbook 插件简介 [1] 为什么要用插件？ Gitbook 默认自带以下 5 个插件：highlight：代码高亮、search：导航栏查询功能（不支持中文）、sharing：右上角分享功能、font-settings：字体设置（最上方的\"A\"符号）、livereload：为 Gitbook 实时重新加载。Gitbook 插件可以解决一些网站不太方便的地方，如侧边栏导航不能收缩，自带搜索不支持中文等问题。 [2] 去哪里找插件？ 除了下文推荐的插件之外，Gitbook 还支持许多其他插件，可以从NPM (opens new window)上搜索 Gitbook 相关的插件。 [3] 插件安装方法： Step1：在项目的根目录中创建 book.json 文件，然后在 plugins 参数中添加插件名。 Step2：使用 gitbook install 来安装插件，重启服务 gitbook serve 或者重新打包 gitbook build 就能看见效果。 注意： 编写 json 时字符串不能用“单引号”括起，最后的那个不能有“逗号”。 如果要卸载自带的 font-settings，插件处应写成 -fontsettings，中间不要加 -。 gitbook install 命令有时会出现问题，多试几次可能就好了。 gitbook install 命令安装慢，而且是全部插件都安装一遍，如果只安装一个插件的话建议使用npm命令安装。 4.2.2 Gitbook 插件推荐 [1] 支持中文的搜索框 { \"plugins\": [ \"-lunr\", \"-search\", \"search-pro\" ], } [2] 左侧章节目录可折叠 { \"plugins\": [ \"expandable-chapters-small\" ], } [3] 侧边栏宽度可调节 { \"plugins\": [ \"splitter\" ], } [4] 回到顶部按钮 { \"plugins\": [ \"back-to-top-button\" ], } [5] 右上角添加 github 图标跳转 { \"plugins\": [ \"-sharing\", \"github\" ], \"pluginsConfig\": { \"github\": { \"url\": \"项目仓库地址\" }, } } [6] 隐藏元素 可以隐藏不想看到的元素，hide-element 是通过 HTML 元素的 class 名字来查找要隐藏的元素，想要隐藏元素找到元素的样式类名加到插件配置里面就可以隐藏元素了。 { \"plugins\": [ \"hide-element\" ], \"pluginsConfig\": { \"hide-element\": { \"elements\": [\".gitbook-link\"] }, } } 经过该配置之后，导航栏中 Published by GitBook 就被隐藏了。 [7] 代码块行号、复制 { \"plugins\": [ \"code\" ], } [8] 修改标题栏图标 { \"plugins\": [ \"favicon\" ], \"pluginsConfig\": { \"favicon\": { \"shortcut\": \"assets/images/favicon.ico\", \"bookmark\": \"assets/images/favicon.ico\", \"appleTouch\": \"assets/images/apple-touch-icon.png\", \"appleTouchMore\": { \"120x120\": \"assets/images/apple-touch-icon-120x120.png\", \"180x180\": \"assets/images/apple-touch-icon-180x180.png\" } } } } shortcut通常可以被所有可以显示favicon的浏览器读取。 bookmark是在收藏夹中显示自己的图标。 apple-touch-icon是一个类似网站favicon的图标文件，用来在iphone和iPad上创建快捷键时使用。apple-touch-icon这个文件应当是png格式，默认：57x57像素大小。如果准备的文件不是57x57的话，它会自己缩放的。 [9] 添加悬浮目录 { \"plugins\" : [ \"page-toc-button\" ], \"pluginsConfig\": { \"page-toc-button\": { \"maxTocDepth\": 2, \"minTocSize\": 2 }, } } maxTocDepth：标题的最大深度（最大支持到 2，即为 h1+h2+h3） minTocSize：显示 toc 按钮的最小 toc 条目数 [10] 添加版权信息和最后修改时间 { \"plugins\": [ \"tbfed-pagefooter\" ], \"pluginsConfig\": { \"tbfed-pagefooter\": { \"copyright\":\"Copyright &copy 版权信息\", \"modify_label\": \"该文件修订时间：\", \"modify_format\": \"YYYY-MM-DD HH:mm:ss\" }, } } [11] 添加RSS订阅 { \"plugins\": [ \"rss\" ], \"pluginsConfig\": { \"rss\":{ \"title\": \"标题\", \"description\": \"描述信息\", \"author\": \"作者\", \"feed_url\": \"https://xxx.xxx.xxx/rss\", \"site_url\": \"https://xxx.xxx.xxx/\" } } } 4.3 成品配置 以下是我配好的 book.json 文件，仅供参考。 { \"title\": \"标题\", \"description\": \"描述信息\", \"author\": \"作者\", \"output.name\": \"site\", \"language\": \"zh-hans\", \"gitbook\": \"3.2.3\", \"root\": \".\", \"links\": { \"sidebar\": { \"sidebar标题\": \"sidebar地址\" } }, \"plugins\": [ \"-lunr\", \"-search\", \"-sharing\", \"-fontsettings\", \"highlight\", \"livereload\", \"search-pro\", \"expandable-chapters-small\", \"splitter\", \"back-to-top-button\", \"github\", \"hide-element\", \"code\", \"custom-favicon\", \"page-toc-button\", \"tbfed-pagefooter\", \"rss\" ], \"pluginsConfig\": { \"github\": { \"url\": \"项目仓库地址\" }, \"hide-element\": { \"elements\": [\".gitbook-link\"] }, \"favicon\": { \"shortcut\": \"assets/images/favicon.ico\", \"bookmark\": \"assets/images/favicon.ico\", \"appleTouch\": \"assets/images/apple-touch-icon.png\", \"appleTouchMore\": { \"120x120\": \"assets/images/apple-touch-icon-120x120.png\", \"180x180\": \"assets/images/apple-touch-icon-180x180.png\" } }, \"page-toc-button\": { \"maxTocDepth\": 2, \"minTocSize\": 2 }, \"tbfed-pagefooter\": { \"copyright\":\"Copyright &copy 版权信息\", \"modify_label\": \"该文件修订时间：\", \"modify_format\": \"YYYY-MM-DD HH:mm:ss\" }, \"rss\":{ \"title\": \"标题\", \"description\": \"描述信息\", \"author\": \"作者\", \"feed_url\": \"https://xxx.xxx.xxx/rss\", \"site_url\": \"https://xxx.xxx.xxx/\" } } } 5.托管到Github Page 这部分需要使用 Git 和 Github 网站 由于 Gitbook 生成的项目跟文档的源码是两个部分，所以可以把文档放到 master 分支上，部署的网站放到 gh-pages 分支。 5.1 本地项目提交到 Github 仓库 [1] 创建git忽略文件 有些文件是不需要纳入到版本控制之中的，在项目根目录创建一个名为.gitignore的git忽略文件，编辑内容如下： # 忽略gitbook生成的项目目录 _book 下面是一些.gitignore文件忽略的匹配规则： *.a # 忽略所有 .a 结尾的文件 !lib.a # 但 lib.a 除外 /TODO # 仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODO build/ # 忽略 build/ 目录下的所有文件 doc/*.txt # 会忽略 doc/notes.txt 但不包括 doc/server/arch.txt 注意：.gitignore只能忽略那些原来没有被track的文件，如果某些文件已经被纳入了版本管理中，则修改.gitignore文件是无效的。那么解决方法就是先把本地缓存删除（改变成未track状态），然后再提交。 [2] SSH KEY 由于本地 Git 仓库和 Github 仓库之间的传输是通过 SSH 加密的，所以连接时需要设置一下： Step1：先看一下 C:\\Users\\xxx 有没有.ssh 目录，有的话看下里面有没有 id_rsa 和 id_rsa.pub 这两个文件，有就跳到下一步，没有就通过下面命令创建: $ ssh-keygen -t rsa -C \"xxx@xxx.com\" 然后一路回车，这时你就会在用户下的.ssh 目录里找到 id_rsa 和 id_rsa.pub 这两个文件 Step2：登录 Github,找到右上角的图标，打开点进里面的 Settings，再选中里面的 SSH and GPG KEYS，点击右上角的 New SSH key，然后 Title 里面随便填，再把刚才 id_rsa.pub 里面的内容复制到 Title 下面的 Key 内容框里面，最后点击 Add SSH key，这样就完成了 SSH Key 的加密。 [3] 初次部署 [1] 设置用户名和邮箱 git config --global user.name \"Your Name\" git config --global user.email \"you@example.com\" [2] 连接到远程仓库并设置记住用户名和密码 git remote add origin URL # 连接到远程仓库并创建别名 git config --global credential.helper store # 设置自动记住用户名和密码（第一次push会被记住，后续无需再输入） [3] 解决使用git add命令时报错LF will be replaced by CRLF的问题 git config auto.crlf true [4] 将项目从本地提交到仓库 git init # 创建git工作区 git add . # 提交所有文件到暂存区 git status # 查看git状态 git commit -m 'description' # 提交到仓库，''内的是描述信息 [5] 将代码推送至远程仓库 git pull origin master --allow-unrelated-histories # 取回远程仓库分支的更新，再与本地的分支合并 git push -u origin master # 将本地仓库推送至远程仓库 说明： 用户名和邮箱设置：在 github 仓库主页显示谁提交了该文件，要根据 github 的注册信息来填写，不要填错了。 git init 后，在文件夹内生成.git 文件（如果没有，则 查看——勾选“隐藏的项目”） 仓库地址可在 clone or download 按钮下取得（即仓库的浏览器地址栏末尾加 .git） 使用 git pull origin master 命令报错：fatal:refusing to merge unrelated histories 错误原因：如果合并了两个不同的开始提交的仓库，在新的 git 会发现这两个仓库可能不是同一个，为了防止开发者上传错误，于是就出现了此提示。 解决办法：如我在 Github 新建一个仓库，写了 License，然后把本地一个写了很久仓库上传。这时会发现 github 的仓库和本地的没有一个共同的 commit 所以 git 不让提交，认为是写错了 origin ，如果开发者确定是这个 origin 就可以使用 --allow-unrelated-histories 告诉 git 允许不相关历史合并 使用 git push origin master 命令报错：error:failed to push some refs to URL 错误原因：直接在 GitHub 上修改后，内容已经和本地不一致了，必须要合并（merge） 解决办法：先使用 git pull origin master 命令下载到本地并合并，自动弹出的 vim 编辑器（按 i 进行编辑，说明为什么合并，可选择不修改，ESC 进入命令行模式然后输入 :wq 退出），再 git push origin master 使用 git add .命令报错：warning: LF will be replaced by CRLF 原因：在 Unix 系统中，行尾用换行（LF）表示。在窗口中，用回车（CR）和换行（LF）（CRLF）表示一行。当您从 unix 系统上载的 git 中获取代码时，它们将只有 LF。 解决办法：如果您是在 Windows 计算机上工作的单个开发人员，并且您不关心 git 自动将 LF 替换为 CRLF，则可以通过在 git 命令行中键入以下内容来关闭此警告 git config core.autocrlf true [4] 后续使用 git config auto.crlf true # 解决使用git add命令时报错LF will be replaced by CRLF的问题 git add . # 提交所有文件到暂存区 git status # 查看git状态 git commit -m 'description' # 提交到仓库，''内的是描述信息 git pull origin master # 取回远程仓库分支的更新，再与本地的分支合并 git push origin master # 将本地仓库推送至远程仓库 为了提交方便，可以创建一个脚本文件 commit.sh 来自动执行，内容如下： 目录结构： --shell ----commit.sh ----deploy.sh --content --gh-pages # 切换到上一级目录 echo '切换到上一级目录\\n' cd .. # 解决使用git add命令时报错LF will be replaced by CRLF的问题 echo '执行命令：git config auto.crlf true\\n' git config auto.crlf true # 保存所有的修改 echo '执行命令：git add -A\\n' git add -A # 把修改的文件提交 echo \"执行命令：git commit -m 'update gitbook'\\n\" git commit -m 'update gitbook' # 将本地仓库推送至远程仓库 echo '执行命令：git push origin main\\n' git push origin main # 返回到上一次的工作目录 echo \"回到刚才工作目录\" cd - 编写好后，在终端运行以下命令即可： $ bash commit.sh 5.2 上传到 Github 仓库的 gh-pages 分支 打包命令太多，为了部署方便，可以创建一个脚本文件 deploy.sh 来自动执行，内容如下： # 切换到上一级目录 echo '切换到上一级目录\\n' cd ../gh-pages git checkout gh-pages # 解决使用git add命令时报错LF will be replaced by CRLF的问题 # echo '执行命令：git config auto.crlf true\\n' # git config auto.crlf true # 保存所有的修改 echo \"执行命令：git add -A\" git add -A # 把修改的文件提交 echo \"执行命令：commit -m 'deploy gitbook'\" git commit -m 'deploy gitbook' # 发布到 https://.github.io/ echo \"执行命令：git push -f 仓库地址.git main:gh-pages\" git push git@github.com:aspire-zero/BigData.git gh-pages # 返回到上一次的工作目录 echo \"回到刚才工作目录\" cd - 注意脚本文件代码中仓库地址要替换成你自己的地址。 文件保存后，在终端执行如下命令，把生成的项目推送到 github 仓库上的 gh-pages 分支： $ bash deploy.sh 执行成功后，打开你的 github 仓库，然后选择 branch 分支，会发现多了一个 gh-pages 分支，打开这个分之后，里面会有一个 index.html 文件，说明部署的代码上传成功了。 5.3 查看 GitHub Page 网站 在 Github 网站上的项目仓库右侧，有一个 Environments，点进去之后再点击 View deployment，即可查看部署好的 GitHub Page。至此，我们的 Gitbook 就已经搭建完成了。 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-02-13 21:50:05 "},"Chapter2/":{"url":"Chapter2/","title":"大数据","keywords":"","body":"第二章 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-02-12 23:35:54 "},"Chapter3/":{"url":"Chapter3/","title":"Java","keywords":"","body":"第三章 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-02-12 23:35:54 "},"Chapter3/Java基础.html":{"url":"Chapter3/Java基础.html","title":"Java基础","keywords":"","body":"基础概念 1. Java 语言有哪些特点? 简单易学（语法简单，上手容易）； 面向对象（封装，继承，多态）； 平台无关性（ Java 虚拟机实现平台无关性）； 支持多线程（ C++ 语言没有内置的多线程机制，因此必须调用操作系统的多线程功能来进行多线程程序设计，而 Java 语言却提供了多线程支持）； 可靠性（具备异常处理和自动内存管理机制）； 安全性（Java 语言本身的设计就提供了多重安全防护机制如访问权限修饰符、限制程序直接访问操作系统资源）； 高效性（通过 Just In Time 编译器等技术的优化，Java 语言的运行效率还是非常不错的）； 支持网络编程并且很方便； 编译与解释并存； 生态强大 内存管理：Java有自己的垃圾回收机制，自动管理内存和回收不再使用的对象。这样，开发者不需要手动管理内存，从而减少内存泄漏和其他内存相关的问题。 2. Java SE vs Java EE 简单来说，Java SE （Standard Edition）是 Java 的基础版本，Java EE（Enterprise Edition）是 Java 的高级版本。 Java SE 更适合开发桌面应用程序或简单的服务器应用程序，Java EE 更适合开发复杂的企业级应用程序或 Web 应用程序。 3. JVM vs JDK vs JRE 3.1 JVM Java 虚拟机（Java Virtual Machine, JVM）是运行 Java 字节码的虚拟机。JVM 有针对不同系统的特定实现（Windows，Linux，macOS），目的是使用相同的字节码，它们都会给出相同的结果。字节码和不同系统的 JVM 实现是 Java 语言“一次编译，随处可以运行”的关键所在。 JVM 并不是只有一种！只要满足 JVM 规范，每个公司、组织或者个人都可以开发自己的专属 JVM。 3.2 JDK和JRE JDK（Java Development Kit）是一个功能齐全的 Java 开发工具包，供开发者使用，用于创建和编译 Java 程序。 它包含了 JRE（Java Runtime Environment），以及编译器 javac 和其他工具，如 javadoc（文档生成器）、jdb（调试器）、jconsole（监控工具）、javap（反编译工具）等。 JRE 是运行已编译 Java 程序所需的环境，主要包含以下两个部分： JVM : 也就是我们上面提到的 Java 虚拟机。 Java 基础类库（Class Library）：一组标准的类库，提供常用的功能和 API（如 I/O 操作、网络通信、数据结构等）。 简单来说：JRE 只包含运行 Java 程序所需的环境和类库，而 JDK 不仅包含 JRE，还包括用于开发和调试 Java 程序的工具。 4.什么是字节码?采用字节码的好处是什么? Java 是编译与解释共存的语言 。 在 Java 中，JVM 可以理解的代码就叫做字节码（即扩展名为 .class 的文件），它不面向任何特定的处理器，只面向虚拟机。 Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。 所以， Java 程序运行时相对来说还是高效的（不过，和 C、 C++，Rust，Go 等语言还是有一定差距的），而且，由于字节码并不针对一种特定的机器，因此，Java 程序无须重新编译便可在多种不同操作系统的计算机上运行。 有些方法和代码块是经常需要被调用的(也就是所谓的热点代码)，所以后面引进了 JIT（Just in Time Compilation） 编译器，而 JIT 属于运行时编译。当 JIT 编译器完成第一次编译后，其会将字节码对应的机器码保存下来，下次可以直接使用。 著作权归JavaGuide(javaguide.cn)所有 基于MIT协议 原文链接：https://javaguide.cn/java/basis/java-basic-questions-01.html 5. 为什么说 Java 语言“编译与解释并存”？ 编译型：编译型语言 会通过编译器将源代码一次性翻译成可被该平台执行的机器码。一般情况下，编译语言的执行速度比较快，开发效率比较低。常见的编译性语言有 C、C++、Go、Rust 等等。 解释型：解释型语言会通过解释器一句一句的将代码解释（interpret）为机器代码后再执行。解释型语言开发效率比较快，执行速度比较慢。常见的解释性语言有 Python、JavaScript、PHP 等等。 因为 Java 程序要经过先编译，后解释两个步骤，由 Java 编写的程序需要先经过编译步骤，生成字节码（.class 文件），这种字节码必须由 Java 解释器来解释执行。 6. Java 和 C++ 和 Python的区别? Java 不提供指针来直接访问内存，程序内存更加安全 Java 的类是单继承的，C++ 支持多重继承；虽然 Java 的类不可以多继承，但是接口可以多继承。 Java 有自动内存管理垃圾回收机制(GC)，不需要程序员手动释放无用内存。 C ++同时支持方法重载和操作符重载，但是 Java 只支持方法重载（操作符重载增加了复杂性，这与 Java 最初的设计思想不符）。 Java是一种已编译的编程语言，Java编译器将源代码编译为字节码，而字节码则由Java虚拟机执行 python是一种解释语言，翻译时会在执行程序的同时进行翻译。 Java语法 1. 注释有哪几种形式？ 单行注释：//通常用于解释方法内某单行代码的作用。 多行注释：/* */通常用于解释一段代码的作用。 文档注释：`/** */ 结束`通常用于生成 Java 开发文档。 代码的注释不是越详细越好。实际上好的代码本身就是注释，我们要尽量规范和美化自己的代码来减少不必要的注释。 若编程语言足够有表达力，就不需要注释，尽量通过代码来阐述。 2. 标识符和关键字的区别是什么？ 标识符：是用户为类、变量、方法、包等程序元素所取的名字，比如Person类。 关键字：是 Java 语言中具有特定含义和用途的保留字，它们被 Java 编译器赋予了特殊的意义，用于表示程序的结构、数据类型、控制流程等重要元素，不能用作其他用途，用户不能自定义与关键字相同的标识符。例如，public、class、int、if、else等都是关键字。 虽然 true, false, 和 null 看起来像关键字但实际上他们是字面值，同时你也不可以作为标识符来使用。 3. Java 语言关键字有哪些？ 4. 自增自减 前缀形式（例如 ++a 或 --a）：先自增/自减变量的值，然后再使用该变量，例如，b = ++a 先将 a 增加 1，然后把增加后的值赋给 b。 后缀形式（例如 a++ 或 a--）：先使用变量的当前值，然后再自增/自减变量的值。例如，b = a++ 先将 a 的当前值赋给 b，然后再将 a 增加 1。 5. 移位运算符 使用移位运算符的主要原因： 高效：移位运算符直接对应于处理器的移位指令。现代处理器具有专门的硬件指令来执行这些移位操作，这些指令通常在一个时钟周期内完成。相比之下，乘法和除法等算术运算在硬件层面上需要更多的时钟周期来完成。 节省内存：通过移位操作，可以使用一个整数（如 int 或 long）来存储多个布尔值或标志位，从而节省内存。 Java 中有三种移位运算符： :左移运算符，向左移若干位，高位丢弃，低位补零。x ,相当于 x 乘以 2 的 n 次方(不溢出的情况下)。 >> :带符号右移，向右移若干位，高位补符号位，低位丢弃。正数高位补 0,负数高位补 1。x >> n,相当于 x 除以 2 的 n 次方。 >>> :无符号右移，忽略符号位，空位都以 0 补齐。 移位操作符实际上支持的类型只有int和long，编译器在对short、byte、char类型进行移位前，都会将其转换为int类型再操作。 如果移位的位数超过数值所占有的位数会怎样？ 也就是说左移/右移 32 位相当于不进行移位操作（32%32=0），左移/右移 42 位相当于左移/右移 10 位（42%32=10）。 6. continue、break 和 return 的区别是什么？ continue：指跳出当前的这一次循环，继续下一次循环。 break：指跳出整个循环体，继续执行循环下面的语句。 return 用于跳出所在方法，结束该方法的运行。 7. 为什么 Java 不引入引用传递呢？ Java中只有值传递。 Java 之父 James Gosling 在设计之初就看到了 C、C++ 的许多弊端，所以才想着去设计一门新的语言 Java。在他设计 Java 的时候就遵循了简单易用的原则，摒弃了许多开发者一不留意就会造成问题的“特性”，语言本身的东西少了，开发者要学习的东西也少了。 Java 中将实参传递给方法（或函数）的方式是 值传递： 如果参数是基本类型的话，很简单，传递的就是基本类型的字面量值的拷贝，会创建副本。 如果参数是引用类型，传递的就是实参所引用的对象在堆中地址值的拷贝，同样也会创建副本。 https://javaguide.cn/java/basis/why-there-only-value-passing-in-java.html#%E5%BC%95%E7%94%A8%E4%BC%A0%E9%80%92%E6%98%AF%E6%80%8E%E4%B9%88%E6%A0%B7%E7%9A%84 基本数据类型 1. Java 中的几种基本数据类型了解么？ 6 种数字类型： 4 种整数型：byte、short、int、long 2 种浮点型：float、double 1 种字符类型：char 1 种布尔型：boolean。 这八种基本类型都有对应的包装类分别为：Byte、Short、Integer、Long、Float、Double、Character、Boolean 。 在Java中char占两个字节，但是在C语言中，占一个字节。 Java 里使用 long 类型的数据一定要在数值后面加上 L，否则将作为整型解析。 Java 里使用 float 类型的数据一定要在数值后面加上 f 或 F，否则将无法通过编译。 char a = 'h'char :单引号，String a = \"hello\" :双引号。 在计算机中，整数通常以补码的形式存储和运算， 在 8 位二进制补码表示中，最高位为符号位，0 表示正数，1 表示负数。对于正数，其补码与原码相同；对于负数，其补码是原码除符号位外各位取反，然后末位加 1。 最小值：byte类型能表示的最小值是 - 128，其补码表示为10000000。这是因为按照补码规则，10000000表示的是 - 128，而不是 - 0（在计算机中不存在 - 0 的概念）。 最大值：byte类型能表示的最大值是 127，其补码表示为01111111，对应的十进制数就是 127。 2. 基本类型和包装类型的区别？ 用途：除了定义一些常量和局部变量之外，我们在其他地方比如方法参数、对象属性中很少会使用基本类型来定义变量。并且，包装类型可用于泛型，而基本类型不可以。 存储方式：基本数据类型的局部变量存放在 Java 虚拟机栈中的局部变量表中，基本数据类型的成员变量（未被 static 修饰 ）存放在 Java 虚拟机的堆中。包装类型属于对象类型，我们知道几乎所有对象实例都存在于堆中。 占用空间：相比于包装类型（对象类型）， 基本数据类型占用的空间往往非常小。 默认值：成员变量包装类型不赋值就是 null ，而基本类型有默认值且不是 null。 比较方式：对于基本数据类型来说，== 比较的是值。对于包装数据类型来说，== 比较的是对象的内存地址。所有整型包装类对象之间值的比较，全部使用 equals() 方法。 基本数据类型存放在栈中是一个常见的误区！ 基本数据类型的存储位置取决于它们的作用域和声明方式。如果它们是局部变量，那么它们会存放在栈中；如果它们是成员变量，那么它们会存放在堆/方法区/元空间中。 3. 为什么要引入包装类，为什么要保留基本类 包装类： 对象封装有很多好处，可以把属性也就是数据跟处理这些数据的方法结合在一起，比如Integer就有parseInt()等方法来专门处理int型相关的数据。 Java中绝大部分方法或类都是用来处理类类型对象的，如ArrayList集合类就只能以类作为他的存储对象，而这时如果想把一个int型的数据存入list是不可能的，必须把它包装成类，也就是Integer才能被List所接受。所以Integer的存在是很必要的。 在Java中，泛型只能使用引用类型，而不能使用基本类型 基本类型： 包装类是引用类型，对象的引用和对象本身是分开存储的，而对于基本类型数据，变量对应的内存块直接存储数据本身，因此基本类型数据在读写效率方面，要比包装类高效。 基本类型更省空间。 4. 包装类型的缓存机制了解么？ Byte,Short,Integer,Long 这 4 种包装类默认创建了数值 [-128，127] 的相应类型的缓存数据，Character 创建了数值在 [0,127] 范围的缓存数据，Boolean 直接返回 True or False。 两种浮点数类型的包装类 Float,Double 并没有实现缓存机制。 Integer i1 = 33; Integer i2 = 33; System.out.println(i1 == i2);// 输出 true Float i11 = 333f; Float i22 = 333f; System.out.println(i11 == i22);// 输出 false Double i3 = 1.2; Double i4 = 1.2; System.out.println(i3 == i4);// 输出 false Integer i1 = 40; Integer i2 = new Integer(40); System.out.println(i1==i2);//输出 false /* Integer i1=40 这一行代码会发生装箱，也就是说这行代码等价于 Integer i1=Integer.valueOf(40) 。因此，i1 直接使用的是缓存中的对象。而Integer i2 = new Integer(40) 会直接创建新的对象。*/ 5. 自动装箱与拆箱了解吗？原理是什么？ 装箱：将基本类型用它们对应的引用类型包装起来； 拆箱：将包装类型转换为基本数据类型； 装箱其实就是调用了 包装类的valueOf()方法，拆箱其实就是调用了 xxxValue()方法。 Integer i = 10; //装箱 int n = i; //拆箱 Integer i = 10 等价于 Integer i = Integer.valueOf(10) int n = i 等价于 int n = i.intValue(); 如果频繁拆装箱的话，也会严重影响系统的性能。我们应该尽量避免不必要的拆装箱操作。 6. 为什么浮点数运算的时候会有精度丢失的风险？ 计算机是二进制的，而且计算机在表示一个数字时，宽度是有限的，无限循环的小数存储在计算机时，只能被截断，所以就会导致小数精度发生损失的情况。 float a = 2.0f - 1.9f; float b = 1.8f - 1.7f; System.out.printf(\"%.9f\",a);// 0.100000024 System.out.println(b);// 0.099999905 System.out.println(a == b);// false 7. 如何解决浮点数运算的精度丢失问题？ BigDecimal 可以实现对浮点数的运算，不会造成精度丢失。 通常情况下，大部分需要浮点数精确运算结果的业务场景（比如涉及到钱的场景）都是通过 BigDecimal 来做的。 BigDecimal a = new BigDecimal(\"1.0\"); BigDecimal b = new BigDecimal(\"1.00\"); BigDecimal c = new BigDecimal(\"0.8\"); BigDecimal x = a.subtract(c); BigDecimal y = b.subtract(c); System.out.println(x); /* 0.2 */ System.out.println(y); /* 0.20 */ // 比较内容，不是比较值 System.out.println(Objects.equals(x, y)); /* false */ // 比较值相等用相等compareTo，相等返回0 System.out.println(0 == x.compareTo(y)); /* true */ 8. 超过 long 整型的数据应该如何表示？ 使用BigInteger 内部使用 int[] 数组来存储任意大小的整形数据。 但相对于常规整数类型的运算来说，BigInteger 运算的效率会相对较低。 变量和方法 1. 成员变量与局部变量的区别？ 语法形式：从语法形式上看，成员变量是属于类的，而局部变量是在代码块或方法中定义的变量或是方法的参数；成员变量可以被 public,private,static 等修饰符所修饰，而局部变量不能被访问控制修饰符及 static 所修饰；但是，成员变量和局部变量都能被 final 所修饰。 存储方式：从变量在内存中的存储方式来看，如果成员变量是使用 static 修饰的，那么这个成员变量是属于类的，如果没有使用 static 修饰，这个成员变量是属于实例的。而对象存在于堆内存，局部变量则存在于栈内存。 生存时间：从变量在内存中的生存时间上看，成员变量是对象的一部分，它随着对象的创建而存在，而局部变量随着方法的调用而自动生成，随着方法的调用结束而消亡。 默认值：从变量是否有默认值来看，成员变量如果没有被赋初始值，则会自动以类型的默认值而赋值（一种情况例外:被 final 修饰的成员变量也必须显式地赋值），而局部变量则不会自动赋值。 为什么成员变量有默认值？ 局部变量没赋值很好判断，可以直接报错。而成员变量可能是运行时赋值，无法判断，误报“没默认值”又会影响用户体验，所以采用自动赋默认值。 2. 静态变量有什么作用？ 静态变量也就是被 static 关键字修饰的变量。它可以被类的所有实例共享，无论一个类创建了多少个对象，它们都共享同一份静态变量。也就是说，静态变量只会被分配一次内存，即使创建多个对象，这样可以节省内存。 通常情况下，静态变量会被 final 关键字修饰成为常量 3. 字符型常量和字符串常量的区别? 字符常量是单引号引起的一个字符，字符串常量是双引号引起的 0 个或若干个字符。 含义 : 字符常量相当于一个整型值( ASCII 值),可以参加表达式运算; 字符串常量代表一个地址值(该字符串在内存中存放位置)。 占内存大小：字符常量只占 2 个字节; 字符串常量占若干个字节。 4. 什么是方法的返回值?方法有哪几种类型？ 获取到的某个方法体中的代码执行后产生的结果 1、无参数无返回值的方法 2、有参数无返回值的方法 3、有返回值无参数的方法 4、有返回值有参数的方法 5. 静态方法为什么不能调用非静态成员? 静态方法是属于类的，在类加载的时候就会分配内存，可以通过类名直接访问。而非静态成员属于实例对象，只有在对象实例化之后才存在，需要通过类的实例对象去访问。 在类的非静态成员不存在的时候静态方法就已经存在了，此时调用在内存中还不存在的非静态成员，属于非法操作。 6. 静态方法和实例方法有何不同？ 1、调用方式 在外部调用静态方法时，可以使用 类名.方法名 的方式，调用静态方法可以无需创建对象 。 2、访问类成员是否存在限制 静态方法在访问本类的成员时，只允许访问静态成员（即静态成员变量和静态方法），不允许访问实例成员（即实例成员变量和实例方法），而实例方法不存在这个限制。 7. 重载和重写有什么区别？ 重载：如果多个方法(比如 StringBuilder 的构造方法)有相同的名字、不同的参数， 便产生了重载。 重写：重写就是子类对父类方法的重新改造，外部样子不能改变，内部逻辑可以改变。 重写需要：“两同”即方法名相同、形参列表相同； “两小”指的是子类方法返回值类型应比父类方法返回值类型更小或相等，子类方法声明抛出的异常类应比父类方法声明抛出的异常类更小或相等； “一大”指的是子类方法的访问权限应比父类方法的访问权限更大或相等。 区别： 注意：如果方法的返回类型是 void 和基本数据类型，则返回值重写时不可修改。但是如果方法的返回值是引用类型，重写时是可以返回该引用类型的子类的。 8. 什么是可变长参数？ 可变参数只能作为函数的最后一个参数，但其前面可以有也可以没有任何其他参数。 优先匹配固定参数的方法 Java 的可变参数编译后实际会被转换成一个数组 public static void method2(String arg1, String... args) { //...... } 面向对象基础 1.面向对象和面向过程的区别 面向过程编程（POP）：面向过程把解决问题的过程拆成一个个方法，通过一个个方法的执行解决问题。 面向对象编程（OOP）：面向对象会先抽象出对象，然后用对象执行方法的方式解决问题 相比较于 POP，OOP 开发的程序一般具有下面这些优点： 易维护：由于良好的结构和封装性，OOP 程序通常更容易维护。 易复用：通过继承和多态，OOP 设计使得代码更具复用性，方便扩展功能。 易扩展：模块化设计使得系统扩展变得更加容易和灵活。 POP 的编程方式通常更为简单和直接，适合处理一些较简单的任务。 POP 和 OOP 的性能差异主要取决于它们的运行机制，而不仅仅是编程范式本身。因此，简单地比较两者的性能是一个常见的误区 2. 创建一个对象用什么运算符?对象实体与对象引用有何不同? new 运算符，new 创建对象实例（对象实例在堆内存中），对象引用指向对象实例（对象引用存放在栈内存中）。 一个对象引用可以指向 0 个或 1 个对象（一根绳子可以不系气球，也可以系一个气球）； 一个对象可以有 n 个引用指向它（可以用 n 条绳子系住一个气球）。 3. 对象的相等和引用相等的区别 对象的相等一般比较的是内存中存放的内容是否相等。 引用相等一般比较的是他们指向的内存地址是否相等。 String str1 = \"hello\"; String str2 = new String(\"hello\"); String str3 = \"hello\"; // 使用 == 比较字符串的引用相等 System.out.println(str1 == str2);//false System.out.println(str1 == str3);//true // 使用 equals 方法比较字符串的相等 System.out.println(str1.equals(str2));//true System.out.println(str1.equals(str3));//true 4. 如果一个类没有声明构造方法，该程序能正确执行吗? 如果一个类没有声明构造方法，也可以执行！因为一个类即使没有声明构造方法也会有默认的不带参数的构造方法. 5. 构造方法有哪些特点？是否可被 override? 名称与类名相同：构造方法的名称必须与类名完全一致。 没有返回值：构造方法没有返回类型，且不能使用 void 声明。 自动执行：在生成类的对象时，构造方法会自动执行，无需显式调用。 构造方法不能被重写（override），但可以被重载（overload）。因此，一个类中可以有多个构造方法，这些构造方法可以具有不同的参数列表，以提供不同的对象初始化方式。 6. 面向对象三大特征 封装：一个对象的状态信息（也就是属性）隐藏在对象内部，不允许外部对象直接访问对象的内部信息。但是可以提供一些可以被外界访问的方法来操作属性。 继承：通过使用继承，可以快速地创建新的类，可以提高代码的重用，程序的可维护性，节省大量创建新类的时间。 子类拥有父类对象所有的属性和方法（包括私有属性和私有方法），但是父类中的私有属性和方法子类是无法访问，只是拥有。 子类可以拥有自己属性和方法，即子类可以对父类进行扩展。 子类可以用自己的方式实现父类的方法。（以后介绍）。 多态： 重载 重写 接口多态：一个对象具有多种的状态，具体表现为父类的引用指向子类的实例（图形方法可以有三角形、圆形多个实例） 7. 接口和抽象类有什么共同点和区别？ 接口是更纯粹的抽象类，接口中只能定义静态变量和抽象方法，而抽象类可以定义成员变量和抽象方法。 接口和抽象类的共同点 实例化：接口和抽象类都不能直接实例化，只能被实现（接口）或继承（抽象类）后才能创建具体的对象。 抽象方法：接口和抽象类都可以包含抽象方法。抽象方法没有方法体，必须在子类或实现类中实现。 接口和抽象类的区别 设计目的：接口主要用于对类的行为进行约束，你实现了某个接口就具有了对应的行为。抽象类主要用于代码复用，强调的是所属关系。 继承和实现：一个类只能继承一个类（包括抽象类），因为 Java 不支持多继承。但一个类可以实现多个接口，一个接口也可以继承多个其他接口。 成员变量：接口中的成员变量只能是 public static final 类型的（静态变量），不能被修改且必须有初始值。抽象类的成员变量可以有任何修饰符（private, protected, public），可以在子类中被重新定义或赋值。 方法： Java 8 之前，接口中的方法默认是 public abstract ，（因此可以省略）也就是只能有方法声明。自 Java 8 起，可以在接口中定义 default（默认） 方法和 static （静态）方法。 自 Java 9 起，接口可以包含 private 方法。 抽象类可以包含抽象方法和非抽象方法。抽象方法没有方法体，必须在子类中实现。非抽象方法有具体实现，可以直接在抽象类中使用或在子类中重写。 9.为什么java8之后要有引入default？ 可以在接口中添加带有默认实现的方法，实现该接口的类如果没有重写这个default方法，就会使用接口中提供的默认实现，这样就不会影响到现有的实现类，使得接口可以在不破坏现有代码的情况下进行扩展。 10. 深拷贝和浅拷贝区别了解吗？什么是引用拷贝？ 浅拷贝：浅拷贝会在堆上创建一个新的对象（区别于引用拷贝的一点），不过，如果原对象内部的属性是引用类型的话，浅拷贝会直接复制内部对象的引用地址，也就是说拷贝对象和原对象共用同一个内部对象。 深拷贝：深拷贝会完全复制整个对象，包括这个对象所包含的内部对象。 引用拷贝：引用拷贝就是两个不同的引用指向同一个对象。 11. java创建对象有哪些方式？ 使用new关键字：通过new关键字直接调用类的构造方法来创建对象。 使用Class类的newInstance()方法：通过反射机制，可以使用Class类的newInstance()方法创建对象。 public class MyClass { public MyClass() { // Constructor } } public class Main { public static void main(String[] args) throws Exception { Class clazz = MyClass.class; MyClass obj = (MyClass) clazz.newInstance(); } } 使用Constructor类的newInstance()方法：同样是通过反射机制，可以使用Constructor类的newInstance()方法创建对象。 使用clone()方法：如果类实现了Cloneable接口，可以使用clone()方法复制对象。 使用反序列化：通过将对象序列化到文件或流中，然后再进行反序列化来创建对象。 12. New出的对象什么时候回收？ Java的垃圾回收器（Garbage Collector）负责回收，垃圾回收器的工作是在程序运行过程中自动进行的，它会周期性地检测不再被引用的对象，并将其回收释放内存。 某个对象的引用计数为0时，表示该对象不再被引用，可以被回收。 Object 1. Object 类的常见方法有哪些？ /** * native 方法，用于返回当前运行时对象的 Class 对象，使用了 final 关键字修饰，故不允许子类重写。 */ public final native Class getClass() /** * native 方法，用于返回对象的哈希码，主要使用在哈希表中，比如 JDK 中的HashMap。 */ public native int hashCode() /** * 用于比较 2 个对象的内存地址是否相等，String 类对该方法进行了重写以用于比较字符串的值是否相等。 */ public boolean equals(Object obj) /** * native 方法，用于创建并返回当前对象的一份拷贝。 */ protected native Object clone() throws CloneNotSupportedException /** * 返回类的名字实例的哈希码的 16 进制的字符串。建议 Object 所有的子类都重写这个方法。 */ public String toString() /** * native 方法，并且不能重写。唤醒一个在此对象监视器上等待的线程(监视器相当于就是锁的概念)。如果有多个线程在等待只会任意唤醒一个。 */ public final native void notify() /** * native 方法，并且不能重写。跟 notify 一样，唯一的区别就是会唤醒在此对象监视器上等待的所有线程，而不是一个线程。 */ public final native void notifyAll() /** * native方法，并且不能重写。暂停线程的执行。注意：sleep 方法没有释放锁，而 wait 方法释放了锁 ，timeout 是等待时间。 */ public final native void wait(long timeout) throws InterruptedException /** * 多了 nanos 参数，这个参数表示额外时间（以纳秒为单位，范围是 0-999999）。 所以超时的时间还需要加上 nanos 纳秒。。 */ public final void wait(long timeout, int nanos) throws InterruptedException /** * 跟之前的2个wait方法一样，只不过该方法一直等待，没有超时时间这个概念 */ public final void wait() throws InterruptedException /** * 实例被垃圾回收器回收的时候触发的操作 */ protected void finalize() throws Throwable { } 2. == 和 equals() 的区别 == 对于基本类型和引用类型的作用效果是不同的： 对于基本数据类型来说，== 比较的是值。 对于引用数据类型来说，== 比较的是对象的内存地址。 因为 Java 只有值传递，所以，对于 == 来说，不管是比较基本数据类型，还是引用数据类型的变量，其本质比较的都是值，只是引用类型变量存的值是对象的地址。 equals() 不能用于判断基本数据类型的变量，只能用来判断两个对象是否相等。equals()方法存在于Object类中，而Object类是所有类的直接或间接父类，因此所有的类都有equals()方法。 //Object的equals方法。 public boolean equals(Object obj) { return (this == obj); } equals() 方法存在两种使用情况： 类没有重写 equals()方法：通过equals()比较该类的两个对象时，等价于通过“==”比较这两个对象，使用的默认是 Object类equals()方法。 类重写了 equals()方法：一般我们都重写 equals()方法来比较两个对象中的属性是否相等；若它们的属性相等，则返回 true(即，认为这两个对象相等)。 String a = new String(\"ab\"); // a 为一个引用 String b = new String(\"ab\"); // b为另一个引用,对象的内容一样 String aa = \"ab\"; // 放在常量池中 String bb = \"ab\"; // 从常量池中查找 System.out.println(aa == bb);// true System.out.println(a == b);// false System.out.println(a.equals(b));// true System.out.println(42 == 42.0);// true 3. hashCode() 有什么用？ hashCode() 的作用是获取哈希码（int 整数），也称为散列码。这个哈希码的作用是确定该对象在哈希表中的索引位置。 4. 为什么要有 hashCode？ 当你把对象加入 HashSet 时，HashSet 会先计算对象的 hashCode 值来判断对象加入的位置，同时也会与其他已经加入的对象的 hashCode 值作比较，如果没有相符的 hashCode，HashSet 会假设对象没有重复出现。但是如果发现有相同 hashCode 值的对象，这时会调用 equals() 方法来检查 hashCode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让其加入操作成功。如果不同的话，就会重新散列到其他位置。这样我们就大大减少了 equals 的次数，相应就大大提高了执行速度。 有了 hashCode() 之后，判断元素是否在对应容器中的效率会更高（参考添加元素进HashSet的过程）！ 那为什么不只提供 hashCode() 方法呢？ 这是因为两个对象的hashCode 值相等并不代表两个对象就相等。 5. 为什么重写 equals() 时必须重写 hashCode() 方法？ 因为两个相等的对象的 hashCode 值必须是相等。也就是说如果 equals 方法判断两个对象是相等的，那这两个对象的 hashCode 值也要相等。 如果重写 equals() 时没有重写 hashCode() 方法的话就可能会导致 equals 方法判断是相等的两个对象，hashCode 值却不相等。 String 1. String、StringBuffer、StringBuilder 的区别？ 可变性 String 是不可变的（后面会详细分析原因）。 StringBuilder 与 StringBuffer 都继承自 AbstractStringBuilder 类，在 AbstractStringBuilder 中也是使用字符数组保存字符串，不过没有使用 final 和 private 关键字修饰，最关键的是这个 AbstractStringBuilder 类还提供了很多修改字符串的方法比如 append 方法。 线程安全性 String 中的对象是不可变的，也就可以理解为常量，线程安全。 StringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。 StringBuilder 并没有对方法进行加同步锁，所以是非线程安全的。 性能 每次对 String 类型进行改变的时候，都会生成一个新的 String 对象，然后将指针指向新的 String 对象。 StringBuffer 每次都会对 StringBuffer 对象本身进行操作，而不是生成新的对象并改变对象引用。 相同情况下使用 StringBuilder 相比使用 StringBuffer 仅能获得 10%~15% 左右的性能提升，但却要冒多线程不安全的风险。 2. String 为什么是不可变的? 如何保证不可变：保存字符串的数组被 final 修饰且为私有的，并且String 类没有提供/暴露修改这个字符串的方法。 为什么要这么设计： 为了安全，字符串经常被用作参数传递。由于字符串不可变，方法内部无法修改传入的字符串内容，从而保证了数据的安全性和一致性。 在多线程环境下，如果字符串是可变的，多个线程同时访问和修改同一个字符串对象时，就需要进行额外的同步操作来保证数据的完整性，这会增加编程的复杂性和性能开销。 3. 字符串拼接用“+” 还是 StringBuilder? Java 语言本身并不支持运算符重载，“+”和“+=”是专门为 String 类重载过的运算符，也是 Java 中仅有的两个重载过的运算符。 字符串对象通过“+”的字符串拼接方式，实际上是通过 StringBuilder 调用 append() 方法实现的，拼接完成之后调用 toString() 得到一个 String 对象，是创建了一个新对象！！！ 。 不过，在循环内使用“+”进行字符串的拼接的话，存在比较明显的缺陷：编译器不会创建单个 StringBuilder 以复用，会导致创建过多的 StringBuilder 对象。 4. String#equals() 和 Object#equals() 有何区别？ String 中的 equals 方法是被重写过的，比较的是 String 字符串的值是否相等。 Object 的 equals 方法是比较的对象的内存地址。 5. 字符串常量池的作用了解吗？ 字符串常量池 是 JVM 为了提升性能和减少内存消耗针对字符串（String 类）专门开辟的一块区域，主要目的是为了避免字符串的重复创建。 // 在字符串常量池中创建字符串对象 ”ab“ // 将字符串对象 ”ab“ 的引用赋值给 aa String aa = \"ab\"; // 直接返回字符串常量池中字符串对象 ”ab“，赋值给引用 bb String bb = \"ab\"; System.out.println(aa==bb); // true 6. String s1 = new String(\"abc\");这句话创建了几个字符串对象？ 字符串常量池中不存在 \"abc\"：会创建 2 个 字符串对象。一个在字符串常量池中，由 ldc 指令触发创建。一个在堆中，由 new String() 创建，并使用常量池中的 \"abc\" 进行初始化。 字符串常量池中已存在 \"abc\"：会创建 1 个 字符串对象。该对象在堆中，由 new String() 创建，并使用常量池中的 \"abc\" 进行初始化。 7. String#intern 方法有什么作用? 常量池中已有相同内容的字符串对象：如果字符串常量池中已经有一个与调用 intern() 方法的字符串内容相同的 String 对象，intern() 方法会直接返回常量池中该对象的引用。 常量池中没有相同内容的字符串对象：如果字符串常量池中还没有一个与调用 intern() 方法的字符串内容相同的对象，intern() 方法会将当前字符串对象的引用添加到字符串常量池中，并返回该引用。 // s1 指向字符串常量池中的 \"Java\" 对象 String s1 = \"Java\"; // s2 也指向字符串常量池中的 \"Java\" 对象，和 s1 是同一个对象 String s2 = s1.intern(); // 在堆中创建一个新的 \"Java\" 对象，s3 指向它 String s3 = new String(\"Java\"); // s4 指向字符串常量池中的 \"Java\" 对象，和 s1 是同一个对象 String s4 = s3.intern(); // s1 和 s2 指向的是同一个常量池中的对象 System.out.println(s1 == s2); // true // s3 指向堆中的对象，s4 指向常量池中的对象，所以不同 System.out.println(s3 == s4); // false // s1 和 s4 都指向常量池中的同一个对象 System.out.println(s1 == s4); // true 8. String 类型的变量和常量做“+”运算时发生了什么？ 常量折叠： 对于 String str3 = \"str\" + \"ing\"; 编译器会给你优化成 String str3 = \"string\"; 。 并不是所有的常量都会进行折叠，只有编译器在程序编译期就可以确定值的常量才可以： 基本数据类型( byte、boolean、short、char、int、float、long、double)以及字符串常量。 final 修饰的基本数据类型和字符串变量 String str1 = \"str\"; String str2 = \"ing\"; String str3 = \"str\" + \"ing\";//发生了常量折叠，指向常量池的string String str4 = str1 + str2;//创建一个新的对象，实际上是通过 StringBuilder 调用 append() 方法实现的，拼接完成之后调用 toString() 得到一个 String 对象 。 String str5 = \"string\"; System.out.println(str3 == str4);//false System.out.println(str3 == str5);//true System.out.println(str4 == str5);//false final String str1 = \"str\"; final String str2 = \"ing\"; // 下面两个表达式其实是等价的 String c = \"str\" + \"ing\";// 常量池中的对象 String d = str1 + str2; // 常量池中的对象 System.out.println(c == d);// true 异常 1.基础知识 2. Exception 和 Error 有什么区别？ Exception :程序本身可以处理的异常，可以通过 catch 来进行捕获。Exception 又可以分为 Checked Exception (受检查异常，必须处理) 和 Unchecked Exception (不受检查异常，可以不处理)。 Error：Error 属于程序无法处理的错误 ，不建议通过catch捕获 。例如 Java 虚拟机运行错误（Virtual MachineError）、虚拟机内存不够错误(OutOfMemoryError)、类定义错误（NoClassDefFoundError）等 。这些异常发生时，Java 虚拟机（JVM）一般会选择线程终止。 3. Checked Exception 和 Unchecked Exception 有什么区别？ Checked Exception 即 受检查异常 ，Java 代码在编译过程中，如果受检查异常没有被 catch或者throws 关键字处理的话，就没办法通过编译。 除了RuntimeException及其子类以外，其他的Exception类及其子类都属于受检查异常 。常见的受检查异常有：IO 相关的异常、ClassNotFoundException、SQLException...。 Unchecked Exception 即 不受检查异常 ，Java 代码在编译过程中 ，我们即使不处理不受检查异常也可以正常通过编译。 RuntimeException 及其子类都统称为非受检查异常，常见的有（建议记下来，日常开发中会经常用到）： NullPointerException(空指针错误) IllegalArgumentException(参数错误比如方法入参类型错误) NumberFormatException（字符串转换为数字格式错误，IllegalArgumentException的子类） ArrayIndexOutOfBoundsException（数组越界错误） ClassCastException（类型转换错误） ArithmeticException（算术错误） SecurityException （安全错误比如权限不够） UnsupportedOperationException(不支持的操作错误比如重复创建同一用户) 4. Throwable 类常用方法有哪些？ String getMessage(): 返回异常发生时的详细信息 String toString(): 返回异常发生时的简要描述 5. try-catch-finally 如何使用？ try块：用于捕获异常。其后可接零个或多个 catch 块，如果没有 catch 块，则必须跟一个 finally 块。 catch块：用于处理 try 捕获到的异常。 finally 块：无论是否捕获或处理异常，finally 块里的语句都会被执行。当在 try 块或 catch 块中遇到 return 语句时，finally 语句块将在方法返回之前被执行。 不要在 finally 语句块中使用 return! 当 try 语句和 finally 语句中都有 return 语句时，try 语句块中的 return 语句会被忽略。 6. finally 中的代码一定会执行吗？ 就比如说 finally 之前虚拟机被终止运行的话，finally 中的代码就不会被执行。 7. 异常使用有哪些需要注意的地方？ 不要把异常定义为静态变量，因为这样会导致异常栈信息错乱。每次手动抛出异常，我们都需要手动 new 一个异常对象抛出。 抛出的异常信息一定要有意义。 建议抛出更加具体的异常比如字符串转换为数字格式错误的时候应该抛出NumberFormatException而不是其父类IllegalArgumentException。 泛型 1.什么是泛型？有什么作用？ 泛型可以让代码更加灵活和可重用 2. 泛型的使用方式有哪几种？ 泛型类、泛型接口、泛型方法。 见JavaGuide。 3. 项目中哪里用到了泛型？ 自定义接口通用返回结果 CommonResult 通过参数 T 可根据具体的返回类型动态指定结果的数据类型 反射 1. 何谓反射？ Java 反射机制是在运行状态中，对于任意一个类，都能够知道这个类中的所有属性和方法和注解， 对于任意一个对象，都能够调用它的任意一个方法（包括私有方法）和属性，包括修改； 这种动态获取的信息以及动态调用对象的方法的功能称为 Java 语言的反射机制。 2. 反射的优缺点？ 反射让我们在运行时有了分析操作类的能力的同时，也增加了安全问题，比如可以无视泛型参数的安全检查（泛型参数的安全检查发生在编译时）。另外，反射的性能也要稍差点，不过，对于框架来说实际是影响不大的。 3. 反射的应用场景？ 这些框架中也大量使用了动态代理，而动态代理的实现也依赖反射。 为什么你使用 Spring 的时候 ，一个@Component注解就声明了一个类为 Spring Bean 呢？为什么你通过一个 @Value注解就读取到配置文件中的值呢？究竟是怎么起作用的呢？ 这涉及到，Spring 使用反射机制调用类的构造方法创建 Bean 的实例。 我们的项目底层数据库有时是用mysql，有时用oracle，需要动态地根据实际情况加载驱动类，这个时候反射就有用了，假设 com.mikechen.java.myqlConnection，com.mikechen.java.oracleConnection这两个类我们要用。 这时候我们在使用 JDBC 连接数据库时使用 Class.forName()通过反射加载数据库的驱动程序，如果是mysql则传入mysql的驱动类，而如果是oracle则传入的参数就变成另一个了。 // DriverManager.registerDriver(new com.mysql.cj.jdbc.Driver()); Class.forName(\"com.mysql.cj.jdbc.Driver\"); 4. 获取 Class 对象的四种方式 1. 知道具体类的情况下可以使用： Class alunbarClass = TargetObject.class; 2. 通过 Class.forName()传入类的全路径获取： Class alunbarClass1 = Class.forName(\"cn.javaguide.TargetObject\"); 3. 通过对象实例instance.getClass()获取： TargetObject o = new TargetObject(); Class alunbarClass2 = o.getClass(); 4. 通过类加载器xxxClassLoader.loadClass()传入类路径获取: 通过类加载器获取 Class 对象不会进行初始化，意味着不进行包括初始化等一系列步骤，静态代码块和静态对象不会得到执行 ClassLoader.getSystemClassLoader().loadClass(\"cn.javaguide.TargetObject\"); 5. 反射的一些基本操作 见javaguide。 注解 1. 能讲一讲Java注解的原理吗？ 注解本质是一个继承了Annotation的特殊接口，其具体实现类是Java运行时生成的动态代理类。 我们通过反射获取注解时，返回的是Java运行时生成的动态代理对象。通过代理对象调用自定义注解的方法，会最终调用注解处理器AnnotationInvocationHandler的invoke方法。该方法会从memberValues这个Map中索引出对应的值。而memberValues的来源是Java常量池。 2. 注解的解析方法有哪几种？ 编译期直接扫描：编译器在编译 Java 代码的时候扫描对应的注解并处理，比如某个方法使用@Override 注解，编译器在编译的时候就会检测当前的方法是否重写了父类对应的方法。 运行期通过反射处理：像框架中自带的注解(比如 Spring 框架的 @Value、@Component)都是通过反射来进行处理的。 序列化和反序列化 1. 什么是序列化?什么是反序列化? 如果我们需要持久化 Java 对象比如将 Java 对象保存在文件中，或者在网络传输 Java 对象，这些场景都需要用到序列化。 序列化：将数据结构或对象转换成可以存储或传输的形式，通常是二进制字节流，也可以是 JSON, XML 等文本格式 反序列化：将在序列化过程中所生成的数据转换为原始数据结构或者对象的过程 序列化和反序列化常见应用场景： 对象在进行网络传输（比如远程方法调用 RPC 的时候）之前需要先被序列化，接收到序列化的对象之后需要再进行反序列化； 将对象存储到文件之前需要进行序列化，将对象从文件中读取出来需要进行反序列化； 将对象存储到数据库（如 Redis）之前需要用到序列化，将对象从缓存数据库中读取出来需要反序列化； 将对象存储到内存之前需要进行序列化，从内存中读取出来之后需要进行反序列化 序列化协议属于 TCP/IP 协议应用层的一部分。 2. 常见序列化协议有哪些？ Hessian、Kryo、Protobuf、ProtoStuff，这些都是基于二进制的序列化协议。 像 JSON 和 XML 这种属于文本类序列化方式。虽然可读性比较好，但是性能较差，一般不会选择。 不推荐用JDK自带的序列化。 JDK 自带的序列化，只需实现 java.io.Serializable接口即可。 不支持跨语言调用 : 如果调用的是其他语言开发的服务的时候就不支持了。 性能差：相比于其他序列化框架性能更低，主要原因是序列化之后的字节数组体积较大，导致传输成本加大。 存在安全问题：序列化和反序列化本身并不存在问题。但当输入的反序列化的数据可被用户控制，那么攻击者即可通过构造恶意输入，让反序列化产生非预期的对象，在此过程中执行构造的任意代码。相关阅读： 代理模式 1. 代理模式 代理模式的主要作用是扩展目标对象的功能，比如说在目标对象的某个方法执行前后你可以增加一些自定义的操作。 2. 静态代理 定义一个接口及其实现类； 创建一个代理类同样实现这个接口 将目标对象注入进代理类，然后在代理类的对应方法调用目标类中的对应方法。这样的话，我们就可以通过代理类屏蔽对目标对象的访问，并且可以在目标方法执行前后做一些自己想做的事情。 非常不灵活（比如接口一旦新增加方法，目标对象和代理对象都要进行修改）且麻烦(需要对每个目标类都单独写一个代理类）。 实际应用场景非常非常少，日常开发几乎看不到使用静态代理的场景。 3. 动态代理 Spring AOP、RPC 框架应该是两个不得不提的，它们的实现都依赖了动态代理。 动态代理在我们日常开发中使用的相对较少，但是在框架中的几乎是必用的一门技术。学会了动态代理之后，对于我们理解和学习各种框架的原理也非常有帮助。 3.1 JDK 动态代理机制 在 Java 动态代理机制中 InvocationHandler 接口和 Proxy 类是核心。 Proxy 类中使用频率最高的方法是：newProxyInstance() ，这个方法主要用来生成一个代理对象。 定义一个接口及其实现类； 自定义类实现InvocationHandler并重写invoke方法，在 invoke 方法中我们会调用原生方法（被代理类的方法）并自定义一些处理逻辑； 通过 Proxy.newProxyInstance(ClassLoader loader,Class[] interfaces,InvocationHandler h) 方法创建代理对象； 3.2. CGLIB 动态代理机制 JDK 动态代理有一个最致命的问题是其只能代理实现了接口的类。 定义一个类； 自定义 MethodInterceptor 并重写 intercept 方法，intercept 用于拦截增强被代理类的方法，和 JDK 动态代理中的 invoke 方法类似； 通过 Enhancer 类的 create()创建代理类； IO 1. Java IO 流了解吗？ 数据输入到计算机内存的过程即输入，反之输出到外部存储（比如数据库，文件，远程主机）的过程即输出。 InputStream/Reader: 所有的输入流的基类，前者是字节输入流，后者是字符输入流。 OutputStream/Writer: 所有输出流的基类，前者是字节输出流，后者是字符输出流。 2. I/O 流为什么要分为字节流和字符流呢? 不管是文件读写还是网络发送接收，信息的最小存储单元都是字节，那为什么 I/O 流操作要分为字节流操作和字符流操作呢？ 字符流是由 Java 虚拟机将字节转换得到的，这个过程还算是比较耗时； 如果我们不知道编码类型的话，使用字节流的过程中很容易出现乱码问题。 字节流：适用于处理任何类型的文件，特别是二进制文件，像图片、音频、视频等。常见的字节流类有 FileInputStream、FileOutputStream、BufferedInputStream 和 BufferedOutputStream 等。 字符流：主要用于处理文本文件，如 .txt、.java、.xml 等。常见的字符流类有 FileReader、FileWriter、BufferedReader 和 BufferedWriter 等。 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-02-18 13:15:56 "},"Chapter3/Java并发.html":{"url":"Chapter3/Java并发.html","title":"Java并发","keywords":"","body":"线程 1. 什么是线程和进程? 进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。 线程与进程相似，但线程是一个比进程更小的执行单位，线程是任务调度的基本单位。具体的见操作系统。 一个 Java 程序的运行是 main 线程和多个其他线程同时运行。 2. Java 线程和操作系统的线程有啥区别？ 现在的 Java 线程的本质其实就是操作系统的线程，一个 Java 线程对应一个系统内核线程。 3. 请简要描述线程与进程的关系,区别及优缺点？ 见操作系统和Javaguide。 程序计数器为什么是私有的? 为了线程切换后能恢复到正确的执行位置。 虚拟机栈和本地方法栈为什么是私有的? 为了保证线程中的局部变量不被别的线程访问到，虚拟机栈和本地方法栈是线程私有的 堆和方法区是所有线程共享的资源。 其中堆是进程中最大的一块内存，主要用于存放新创建的对象 (几乎所有对象都在这里分配内存)。 方法区主要用于存放已被加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 4. 如何创建线程？ Java创建线程有很多种方式啊，像实现Runnable、Callable接口、继承Thread类、创建线程池等等，不过这些方式并没有真正创建出线程，严格来说，Java就只有一种方式可以创建线程，那就是通过new Thread().start()创建。 而所谓的Runnable、Callable……对象，这仅仅只是线程体，也就是提供给线程执行的任务，并不属于真正的Java线程，它们的执行，最终还是需要依赖于new Thread()…… 继承Thread类，重写run方法： public class ExtendsThread extends Thread { @Override public void run() { System.out.println(\"1......\"); } public static void main(String[] args) { new ExtendsThread().start(); } } 实现Runnable接口并重写run方法 public class ImplementsRunnable implements Runnable { @Override public void run() { System.out.println(\"2......\"); } public static void main(String[] args) { ImplementsRunnable runnable = new ImplementsRunnable(); new Thread(runnable).start(); } } 实现Callable接口并重写call方法，如下： public class ImplementsCallable implements Callable { @Override public String call() throws Exception { System.out.println(\"3......\"); return \"zhuZi\"; } public static void main(String[] args) throws Exception { ImplementsCallable callable = new ImplementsCallable(); FutureTask futureTask = new FutureTask<>(callable); new Thread(futureTask).start(); System.out.println(futureTask.get()); } } 可以通过Executors创建线程池 public class UseExecutorService { public static void main(String[] args) { ExecutorService poolA = Executors.newFixedThreadPool(2); poolA.execute(()->{ System.out.println(\"4A......\"); }); poolA.shutdown(); // 又或者自定义线程池 ThreadPoolExecutor poolB = new ThreadPoolExecutor(2, 3, 0, TimeUnit.SECONDS, new LinkedBlockingQueue(3), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy()); poolB.submit(()->{ System.out.println(\"4B......\"); }); poolB.shutdown(); } } 5. 说说线程的生命周期和状态? Java 线程在运行的生命周期中的指定时刻只可能处于下面 6 种不同状态的其中一个状态： NEW: 初始状态，线程被创建出来但没有被调用 start() 。 RUNNABLE: 运行状态，线程被调用了 start()等待运行的状态。 BLOCKED：阻塞状态，需要等待锁释放。 WAITING：等待状态，表示该线程需要等待其他线程做出一些特定动作（通知或中断）。 TIME_WAITING：超时等待状态，可以在指定的时间后自行返回而不是像 WAITING 那样一直等待。 TERMINATED：终止状态，表示该线程已经运行完毕。 在操作系统层面，线程有 READY 和 RUNNING 状态；而在 JVM 层面，只能看到 RUNNABLE 状态 6. 什么是线程上下文切换? 线程切换意味着需要保存当前线程的上下文，留待线程下次占用 CPU 的时候恢复现场。 主动让出 CPU，比如调用了 sleep(), wait() 等。 时间片用完，因为操作系统要防止一个线程或者进程长时间占用 CPU 导致其他线程或者进程饿死。 调用了阻塞类型的系统中断，比如请求 IO，线程被阻塞。 7. Thread#sleep() 方法和 Object#wait() 方法对比 共同点：两者都可以暂停线程的执行。 区别： sleep() 方法没有释放锁，而 wait() 方法释放了锁 。 wait() 通常被用于线程间交互/通信，sleep()通常被用于暂停执行。 wait() 方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的 notify()或者 notifyAll() 方法。sleep()方法执行完成后，线程会自动苏醒，或者也可以使用 wait(long timeout) 超时后线程会自动苏醒。 sleep() 是 Thread 类的静态本地方法，wait() 则是 Object 类的本地方法。为什么这样设计呢？下一个问题就会聊到。 为什么 wait() 方法不定义在 Thread 中？ 每个对象（Object）都拥有对象锁，既然要释放当前线程占有的对象锁并让其进入 WAITING 状态，自然是要操作对应的对象（Object）而非当前的线程（Thread）。 为什么 sleep() 方法定义在 Thread 中？ 因为 sleep() 是让当前线程暂停执行，不涉及到对象类，也不需要获得对象锁。 8. 可以直接调用 Thread 类的 run 方法吗？ 调用 start() 方法方可启动线程并使线程进入就绪状态，直接执行 run() 方法的话会当成一个 main 线程下的普通方法去执行，不会以多线程的方式执行。 多线程 1. 并发与并行的区别 并发：两个及两个以上的作业在同一 时间段 内执行。 并行：两个及两个以上的作业在同一 时刻 执行。 2. 同步和异步的区别 同步：发出一个调用之后，在没有得到结果之前， 该调用就不可以返回，一直等待。 异步：调用在发出之后，不用等待返回结果，该调用直接返回。 3. 为什么要使用多线程? 从计算机底层来说： 线程可以比作是轻量级的进程，是程序执行的最小单位,线程间的切换和调度的成本远远小于进程。另外，多核 CPU 时代意味着多个线程可以同时运行，这减少了线程上下文切换的开销。 从当代互联网发展趋势来说： 现在的系统动不动就要求百万级甚至千万级的并发量，而多线程并发编程正是开发高并发系统的基础，利用好多线程机制可以大大提高系统整体的并发能力以及性能。 4. 单核 CPU 支持 Java 多线程吗？ 单核 CPU 是支持 Java 多线程的。操作系统通过时间片轮转的方式，将 CPU 的时间分配给不同的线程。尽管单核 CPU 一次只能执行一个任务，但通过快速在多个线程之间切换，可以让用户感觉多个任务是同时进行的。 5. 单核 CPU 上运行多个线程效率一定会高吗？ 对于单核 CPU 来说，如果任务是 CPU 密集型的，那么开很多线程会频繁的线程切换，影响效率；如果任务是 IO 密集型的，那么开很多线程会提高效率。 6. 使用多线程可能带来什么问题? 内存泄漏、死锁、线程不安全（数据的正确性和一致性）等等。 JMM https://javaguide.cn/java/concurrent/jmm.html 1. 基础知识 CPU 可以通过制定缓存一致协议（比如 MESI 协议）来解决内存缓存不一致性问题。 为了提升执行速度/性能，计算机在执行程序代码的时候，会对指令进行重排序。 简单来说就是系统在执行代码的时候并不一定是按照你写的代码的顺序依次执行。指令重排序可以保证串行语义一致，但是没有义务保证多线程间的语义也一致 ，所以在多线程下，指令重排序可能会导致一些问题。 你可以把 JMM 看作是 Java 定义的并发编程相关的一组规范，除了抽象了线程和主内存之间的关系之外，其还规定了从 Java 源代码到 CPU 可执行指令的这个转化过程要遵守哪些和并发相关的原则和规范，其主要目的是为了简化多线程编程，增强程序可移植性的。 2. Java内存模型 从上图来看，线程 1 与线程 2 之间如果要进行通信的话，必须要经历下面 2 个步骤： 线程 1 把本地内存中修改过的共享变量副本的值同步到主内存中去。 线程 2 到主存中读取对应的共享变量的值。 也就是说，JMM 为共享变量提供了可见性的保障。 不过，多线程下，对主内存中的一个共享变量进行操作有可能诱发线程安全问题。举个例子： 线程 1 和线程 2 分别对同一个共享变量进行操作，一个执行修改，一个执行读取。 线程 2 读取到的是线程 1 修改之前的值还是修改后的值并不确定，都有可能，因为线程 1 和线程 2 都是先将共享变量从主内存拷贝到对应线程的工作内存中。 3. 并发编程三个重要特性 3.1 原子性 一次操作或者多次操作，要么所有的操作全部都得到执行并且不会受到任何因素的干扰而中断，要么都不执行。 在 Java 中，可以借助synchronized、各种 Lock 以及各种原子类实现原子性。 synchronized 和各种 Lock 可以指令重排序可以保证串行语义一致，但是没有义务保证多线程间的语义也一致 ，所以在多线程下，指令重排序可能会导致一些问题。 各种原子类是利用 CAS (compare and swap) 操作（可能也会用到 volatile或者final关键字）来保证原子操作。 3.2 可见性 当一个线程对共享变量进行了修改，那么另外的线程都是立即可以看到修改后的最新值。 在 Java 中，可以借助synchronized、volatile 以及各种 Lock 实现可见性。 锁确保线程在释放锁时，会将本地内存中的共享变量刷新到主内存； 在获取锁时，会从主内存中读取共享变量的最新值。 如果我们将变量声明为 volatile ，这就指示 JVM，这个变量是共享且不稳定的，每次使用它都到主存中进行读取。 3.3 有序性 由于指令重排序问题，代码的执行顺序未必就是编写代码时候的顺序。 我们上面讲重排序的时候也提到过： 指令重排序可以保证串行语义一致，但是没有义务保证多线程间的语义也一致 ，所以在多线程下，指令重排序可能会导致一些问题。 在 Java 中，volatile 关键字可以禁止指令进行重排序优化。 4. volatile 关键字 保证变量可见性 禁止指令重排序 在 Java 内存模型（JMM）中，每个线程都有自己的工作内存，线程对变量的操作都是在工作内存中进行的，而变量的实际存储位置是主内存。当一个线程修改了主内存中变量的值后，其他线程的工作内存中该变量的副本可能还是旧值，这就会导致数据不一致的问题。 当一个变量被声明为 volatile 时，线程对该变量的写操作会立即刷新到主内存中，而读操作会直接从主内存中读取。这样一来，一个线程对 volatile 变量的修改能够及时被其他线程看到。 在 Java 中，为了提高程序的执行效率，编译器和处理器会对指令进行重排序。指令重排序可以保证串行语义一致，但是没有义务保证多线程间的语义也一致 ，所以在多线程下，指令重排序可能会导致一些问题。volatile 关键字可以禁止指令重排序。 锁 1. 什么是悲观锁？ 认为共享资源每次被访问的时候就会出现问题，因此共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程。 像 Java 中synchronized和ReentrantLock等独占锁就是悲观锁思想的实现。 public void performSynchronisedTask() { synchronized (this) { // 需要同步的操作 } } private Lock lock = new ReentrantLock(); lock.lock(); try { // 需要同步的操作 } finally { lock.unlock(); } 高并发的场景下，激烈的锁竞争会造成线程阻塞，大量阻塞线程会导致系统的上下文切换，增加系统的性能开销。并且，悲观锁还可能会存在死锁问题，影响代码的正常运行。 2. 什么是乐观锁？ 乐观锁总是假设最好的情况，认为共享资源每次被访问的时候不会出现问题，线程可以不停地执行，无需加锁也无需等待，只是在提交修改的时候去验证对应的资源（也就是数据）是否被其它线程修改了（具体方法可以使用版本号机制或 CAS 算法）。 在 Java 中java.util.concurrent.atomic包下面的原子变量类（比如AtomicInteger、LongAdder）就是使用了乐观锁的一种实现方式 CAS 实现的。 但是，如果冲突频繁发生（写占比非常多的情况），会频繁失败和重试，这样同样会非常影响性能，导致 CPU 飙升。 3. 如何选择乐观锁和悲观锁？ 悲观锁通常多用于写比较多的情况（多写场景，竞争激烈），这样可以避免频繁失败和重试影响性能，悲观锁的开销是固定的。不过，如果乐观锁解决了频繁失败和重试这个问题的话（比如LongAdder），也是可以考虑使用乐观锁的，要视实际情况而定。 乐观锁通常多用于写比较少的情况（多读场景，竞争较少），这样可以避免频繁加锁影响性能。不过，乐观锁主要针对的对象是单个共享变量（参考java.util.concurrent.atomic包下面的原子变量类）。 4. 如何实现乐观锁？ 版本号机制：一般是在数据表中加上一个数据版本号 version 字段，表示数据被修改的次数。当数据被修改时，version 值会加一。当线程 A 要更新数据值时，在读取数据的同时也会读取 version 值，在提交更新时，若刚才读取到的 version 值为当前数据库中的 version 值相等时才更新，否则重试更新操作，直到更新成功。 CAS：Compare And Swap（比较与交换），就是用一个预期值和要更新的变量值进行比较，两值相等才会进行更新 CAS 是一个原子操作，底层依赖于一条 CPU 的原子指令。 CAS 涉及到三个操作数： V：要更新的变量值(Var) E：预期值(Expected) N：拟写入的新值(New) 当且仅当 V 的值等于 E 时，CAS 通过原子方式用新值 N 来更新 V 的值。如果不等，说明已经有其它线程更新了 V，则当前线程放弃更新。 举一个简单的例子：线程 A 要修改变量 i 的值为 6，i 原值为 1（V = 1，E=1，N=6，假设不存在 ABA 问题）。 i 与 1 进行比较，如果相等， 则说明没被其他线程修改，可以被设置为 6 。 i 与 1 进行比较，如果不相等，则说明被其他线程修改，当前线程放弃更新，CAS 操作失败。 当多个线程同时使用 CAS 操作一个变量时，只有一个会胜出，并成功更新，其余均会失败，但失败的线程并不会被挂起，仅是被告知失败，并且允许再次尝试，当然也允许失败的线程放弃操作。 5. Java 中 CAS 是如何实现的？ 在 Java 中，实现 CAS（Compare-And-Swap, 比较并交换）操作的一个关键类是Unsafe Unsafe类中的 CAS 方法是native方法，确保比较和交换操作是不可分割的，即在一个CPU周期内完成。 无锁：避免线程阻塞，提升并发性能。 原子性：确保操作的不可分割性。 6. CAS 算法存在哪些问题？ ABA 问题 如果一个变量 V 初次读取的时候是 A 值，并且在准备赋值的时候检查到它仍然是 A 值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回 A，那 CAS 操作就会误认为它从来没有被修改过。这个问题被称为 CAS 操作的 \"ABA\"问题。 ABA 问题的解决思路是在变量前面追加上版本号或者时间戳。 循环时间长开销大 CAS 经常会用到自旋操作来进行重试，也就是不成功就一直循环执行直到成功。如果长时间不成功，会给 CPU 带来非常大的执行开销。 synchronized 关键字 1. synchronized 是什么？有什么用？ 主要解决的是多个线程之间访问资源的同步性，可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行。 2. 如何使用 synchronized？ 修饰实例方法、修饰静态方法、修饰代码块。 见JAVAguide。 3. JDK1.6 之后的 synchronized 底层做了哪些优化？ 在 Java 6 之后， synchronized 引入了大量的优化如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销，这些优化让 synchronized 锁的效率提升了很多（JDK18 中，偏向锁已经被彻底废弃，前面已经提到过了）。 4. synchronized 和 volatile 有什么区别？ synchronized 关键字和 volatile 关键字是两个互补的存在，而不是对立的存在！ volatile 关键字能保证数据的可见性，但不能保证数据的原子性。synchronized 关键字两者都能保证。可见性：锁确保线程在释放锁时，会将本地内存中的共享变量刷新到主内存；在获取锁时，会从主内存中读取共享变量的最新值。 volatile关键字主要用于解决变量在多个线程之间的可见性，而 synchronized 关键字解决的是多个线程之间访问资源的同步性。 volatile 关键字只能用于变量而 synchronized 关键字可以修饰方法以及代码块 。 5. 原理 Monitor 机制，每个 Java 对象都与一个 Monitor 相关联。 Monitor 的核心组件 Owner：当前持有锁的线程。 EntrySet：等待获取锁的线程队列。 WaitSet：调用了 wait() 方法而进入等待状态的线程队列。 锁的获取 当线程尝试进入 synchronized 代码块时，JVM 会检查 Monitor 的 Owner 是否为空。 如果 Owner 为空，当前线程成为 Owner，并进入临界区执行代码。 如果 Owner 不为空，当前线程进入 EntrySet，进入阻塞状态，等待锁的释放。 锁的释放 当线程执行完 synchronized 代码块时，会释放 Monitor，并将 Owner 设置为空。 JVM 会从 EntrySet 中唤醒一个线程，使其成为新的 Owner。 wait() 和 notify() 当线程调用 wait() 时，它会释放锁并进入 WaitSet。 当其他线程调用 notify() 或 notifyAll() 时，WaitSet 中的线程会被唤醒，重新进入 EntrySet，竞争锁。 6. 锁的升级过程 JVM 对 synchronized 进行了优化，引入了 锁升级机制： 无锁状态 初始状态，没有线程竞争。 偏向锁 当只有一个线程访问同步代码时，JVM 会将锁标记为偏向锁，避免 CAS 操作。 偏向锁的目的是减少无竞争情况下的开销。 轻量级锁 当有多个线程轻度竞争时，JVM 会将偏向锁升级为轻量级锁。 轻量级锁通过 CAS 操作尝试获取锁，避免线程阻塞。 重量级锁 当竞争激烈时，轻量级锁会升级为重量级锁。 重量级锁依赖于操作系统的互斥量（Mutex），会导致线程阻塞和上下文切换。 ReentrantLock 1. ReentrantLock 是什么？ ReentrantLock 实现了 Lock 接口，是一个可重入且独占式的锁，和 synchronized 关键字类似。不过，ReentrantLock 更灵活、更强大，增加了轮询、超时、中断、公平锁和非公平锁等高级功能。 2. 公平锁和非公平锁有什么区别？ 公平锁 : 锁被释放之后，先申请的线程先得到锁。性能较差一些，因为公平锁为了保证时间上的绝对顺序，上下文切换更频繁。 非公平锁：锁被释放之后，后申请的线程可能会先获取到锁，是随机或者按照其他优先级排序的。性能更好，但可能会导致某些线程永远无法获取到锁。 3. synchronized 和 ReentrantLock 有什么区别？ 二者都是可重入锁。 ReentrantLock 就属于是可中断锁，synchronized 就属于是不可中断锁。 可中断锁：获取锁的过程中可以被中断，不需要一直等到获取锁之后 才能进行其他逻辑处理。ReentrantLock 就属于是可中断锁。 不可中断锁：一旦线程申请了锁，就只能等到拿到锁以后才能进行其他的逻辑处理。 synchronized 就属于是不可中断锁。 可重入锁 也叫递归锁，指的是线程可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果是不可重入锁的话，就会造成死锁。 public class SynchronizedDemo { public synchronized void method1() { System.out.println(\"方法1\"); method2(); } public synchronized void method2() { System.out.println(\"方法2\"); } } 由于 synchronized锁是可重入的，同一个线程在调用method1() 时可以直接获得当前对象的锁，执行 method2() 的时候可以再次获取这个对象的锁，不会产生死锁问题。假如synchronized是不可重入锁的话，由于该对象的锁已被当前线程所持有且无法释放，这就导致线程在执行 method2()时获取锁失败，会出现死锁问题。 synchronized 依赖于 JVM 而 ReentrantLock 依赖于 API ReentrantLock 比 synchronized 增加了一些高级功能 可实现公平锁 : ReentrantLock可以指定是公平锁还是非公平锁。而synchronized只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。ReentrantLock默认情况是非公平的，可以通过 ReentrantLock类的ReentrantLock(boolean fair)构造方法来指定是否是公平的。 支持超时 ：ReentrantLock 提供了 tryLock(timeout) 的方法，可以指定等待获取锁的最长等待时间，如果超过了等待时间，就会获取锁失败，不会一直等待。 等待可中断 : ReentrantLock提供了一种能够中断等待锁的线程的机制，通过 lock.lockInterruptibly() 来实现这个机制。也就是说当前线程在等待获取锁的过程中，如果其他线程中断当前线程「 interrupt() 」，当前线程就会抛出 InterruptedException 异常，可以捕捉该异常进行相应处理。 4. 可中断锁和不可中断锁有什么区别？ 可中断锁：获取锁的过程中可以被中断，不需要一直等到获取锁之后才能进行其他逻辑处理。ReentrantLock 就属于是可中断锁。 不可中断锁：一旦线程申请了锁，就只能等到拿到锁以后才能进行其他的逻辑处理。 synchronized 就属于是不可中断锁。 5. 原理 AQS（AbstractQueuedSynchronizer），AQS 是一个用于构建锁和同步器的框架，它通过一个 FIFO 队列 来管理等待锁的线程。 AQS 的核心组件 state 状态变量 用于表示锁的状态。 对于 ReentrantLock，state 表示锁的持有次数： state = 0：锁未被持有。 state > 0：锁被持有，且值为重入次数。 FIFO 等待队列 用于存储等待锁的线程。 每个线程会被封装成一个 Node 对象，加入队列中。 CAS 操作 通过 CAS（Compare-And-Swap）操作来修改 state 的值，确保线程安全。 非公平锁的获取过程 线程尝试通过 CAS 操作将 state 从 0 改为 1，如果成功，则获取锁。 如果失败，检查当前线程是否是锁的持有者（重入锁），如果是，则增加 state 的值。 如果以上都失败，线程会被封装成 Node 对象，加入等待队列，并进入阻塞状态。 公平锁的获取过程 检查等待队列中是否有其他线程在等待锁。 如果有，当前线程直接加入等待队列。 如果没有，尝试通过 CAS 操作获取锁。 锁的释放过程 将 state 的值减 1。 如果 state 变为 0，表示锁完全释放，唤醒等待队列中的下一个线程。 ThreadLocal 1. ThreadLocal 有什么用？用法？ ThreadLocal 类允许每个线程绑定自己的值，用于存储私有数据，确保不同线程之间的数据互不干扰。 public class MyDemo { private static ThreadLocal tl = new ThreadLocal<>(); private String content; private String getContent() { return tl.get(); } private void setContent(String content) { tl.set(content); } } 2. ThreadLocal 原理了解吗？ ThreadLocalMap 理解为ThreadLocal 类实现的定制化的 HashMap。 最终的变量是放在了当前线程的 ThreadLocalMap 中，并不是存在 ThreadLocal 上，ThreadLocal 可以理解为只是ThreadLocalMap的封装，传递了变量值。 ThrealLocal 类中可以通过Thread.currentThread()获取到当前线程对象后，直接通过getMap(Thread t)可以访问到该线程的ThreadLocalMap对象。 public void set(T value) { //获取当前请求的线程 Thread t = Thread.currentThread(); //取出 Thread 类内部的 threadLocals 变量(哈希表结构) ThreadLocalMap map = getMap(t); if (map != null) // 将需要存储的值放入到这个哈希表中 map.set(this, value);//this就是ThreadLocal的对象 else createMap(t, value); } ThreadLocalMap getMap(Thread t) { return t.threadLocals; } 每个Thread中都具备一个ThreadLocalMap，而ThreadLocalMap可以存储以ThreadLocal为 key ，Object 对象为 value 的键值对。 比如我们在同一个线程中声明了两个 ThreadLocal 对象的话， Thread内部都是使用仅有的那个ThreadLocalMap 存放数据的，ThreadLocalMap的 key 就是 ThreadLocal对象，value 就是 ThreadLocal 对象调用set方法设置的值。 3. ThreadLocal 内存泄露问题是怎么导致的？如何避免？ ThreadLocal原理： key 是弱引用：ThreadLocalMap 中的 key 是 ThreadLocal 的弱引用 (WeakReference>)。 这意味着，如果 ThreadLocal 实例不再被任何强引用指向，垃圾回收器会在下次 GC 时回收该实例，导致 ThreadLocalMap 中对应的 key 变为 null。 value 是强引用：ThreadLocalMap 中的 value 是强引用。 即使 key 被回收（变为 null），value 仍然存在于 ThreadLocalMap 中，被强引用，不会被回收。 归根到底，由于ThreadLocalMap的生命周期跟Thread一样长，如果没有手动删除对应key就会导致内存泄漏，与key是弱引用无关，反而弱引用多了一层保障。 具体的说：ThreadLocal 实例不再被强引用，由于是弱引用，它会被垃圾回收，导致键变为null，但值Value是强引用，不会被回收；并且，线程持续存活，导致 ThreadLocalMap 长期存在。此外虽然 ThreadLocalMap 在 get(), set() 和 remove() 操作时会尝试清理 key 为 null 的 entry，但这种清理机制是被动的，并不完全可靠。 如何避免？ 在使用完 ThreadLocal 后，务必调用 remove() 方法。 这是最安全和最推荐的做法。 remove() 方法会从 ThreadLocalMap 中显式地移除对应的 entry，彻底解决内存泄漏的风险。 即使将 ThreadLocal 定义为 static final，也强烈建议在每次使用后调用 remove()。 在线程池等线程复用的场景下，使用 try-finally 块可以确保即使发生异常，remove() 方法也一定会被执行。 4.为什么使用弱引用？ ​ 事实上，在ThreadLocalMap中的set/getEntry方法中，会对key为null（也即是ThreadLocal为null）进行判断，如果为null的话，那么是会对value置为null的。 ​ 这就意味着使用完ThreadLocal，CurrentThread依然运行的前提下，就算忘记调用remove方法，弱引用比强引用可以多一层保障：弱引用的ThreadLocal会被回收，对应的value在下一次ThreadLocalMap调用set,get,remove中的任一方法的时候会被清除，从而避免内存泄漏。 5. 如何跨线程传递 ThreadLocal 的值？ InheritableThreadLocal ：InheritableThreadLocal 是 JDK1.2 提供的工具，继承自 ThreadLocal 。使用 InheritableThreadLocal 时，会在创建子线程时，令子线程继承父线程中的 ThreadLocal 值，但是无法支持线程池场景下的 ThreadLocal 值传递。 TransmittableThreadLocal ： TransmittableThreadLocal （简称 TTL） 是阿里巴巴开源的工具类，继承并加强了InheritableThreadLocal类，可以在线程池的场景下支持 ThreadLocal 值传递。项目地址：https://github.com/alibaba/transmittable-thread-local。 线程池 1. 什么是线程池? 线程池就是管理一系列线程的资源池。当有任务要处理时，直接从线程池中获取线程来处理，处理完之后线程并不会立即被销毁，而是等待下一个任务。 2. 为什么要用线程池？ 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控 3. 如何创建线程池？常用方法 方式一：通过ThreadPoolExecutor构造函数来创建（推荐）。 方式二：通过 Executor 框架的工具类 Executors 来创建。极其不推荐 FixedThreadPool：固定线程数量的线程池。该线程池中的线程数量始终不变。当有一个新的任务提交时，线程池中若有空闲线程，则立即执行。若没有，则新的任务会被暂存在一个任务队列中，待有线程空闲时，便处理在任务队列中的任务。 SingleThreadExecutor： 只有一个线程的线程池。若多余一个任务被提交到该线程池，任务会被保存在一个任务队列中，待线程空闲，按先入先出的顺序执行队列中的任务。 CachedThreadPool： 可根据实际情况调整线程数量的线程池。线程池的线程数量不确定，但若有空闲线程可以复用，则会优先使用可复用的线程。若所有线程均在工作，又有新的任务提交，则会创建新的线程处理任务。所有线程在当前任务执行完毕后，将返回线程池进行复用。 ScheduledThreadPool：给定的延迟后运行任务或者定期执行任务的线程池。 如果你只需要提交不需要返回结果的任务，并且不关心任务执行过程中的异常情况，可以使用 execute 方法； 如果需要获取任务的执行结果，或者需要处理任务执行过程中的异常，建议使用 submit 方法。 4. 为什么不推荐使用内置线程池？ 阿里巴巴不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 构造函数的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险 Executors 返回线程池对象的弊端如下： FixedThreadPool 和 SingleThreadExecutor:使用的是有界阻塞队列是 LinkedBlockingQueue ，其任务队列的最大长度为 Integer.MAX_VALUE ，可能堆积大量的请求，从而导致 OOM。 CachedThreadPool:使用的是同步队列 SynchronousQueue, 允许创建的线程数量为 Integer.MAX_VALUE ，如果任务数量过多且执行速度较慢，可能会创建大量的线程，从而导致 OOM。 ScheduledThreadPool 和 SingleThreadScheduledExecutor :使用的无界的延迟阻塞队列 DelayedWorkQueue ，任务队列最大长度为 Integer.MAX_VALUE ，可能堆积大量的请求，从而导致 OOM。 5. 线程池常见参数有哪些？如何解释？ 七个。 核心的是：核心线程池、最大线程数量、任务队列。 6. 线程池的核心线程会被回收吗？ ThreadPoolExecutor 默认不会回收核心线程，即使它们已经空闲了。 7. 线程池的拒绝策略有哪些？ 8. 如果不允许丢弃任务任务，应该选择哪个拒绝策略？ CallerRunsPolicy 9. CallerRunsPolicy 拒绝策略有什么风险？如何解决？ 可能会导致主线程阻塞，影响程序的正常运行。 10. 线程池常用的阻塞队列有哪些？ 容量为 Integer.MAX_VALUE 的 LinkedBlockingQueue（有界阻塞队列）：FixedThreadPool 和 SingleThreadExecutor 。FixedThreadPool最多只能创建核心线程数的线程（核心线程数和最大线程数相等），SingleThreadExecutor只能创建一个线程（核心线程数和最大线程数都是 1），二者的任务队列永远不会被放满。 SynchronousQueue（同步队列）：CachedThreadPool 。SynchronousQueue 没有容量，不存储元素，目的是保证对于提交的任务，如果有空闲线程，则使用空闲线程来处理；否则新建一个线程来处理任务。也就是说，CachedThreadPool 的最大线程数是 Integer.MAX_VALUE ，可以理解为线程数是可以无限扩展的，可能会创建大量线程，从而导致 OOM。 DelayedWorkQueue（延迟队列）：ScheduledThreadPool 和 SingleThreadScheduledExecutor 。DelayedWorkQueue 的内部元素并不是按照放入的时间排序，而是会按照延迟的时间长短对任务进行排序，内部采用的是“堆”的数据结构，可以保证每次出队的任务都是当前队列中执行时间最靠前的。DelayedWorkQueue 添加元素满了之后会自动扩容，增加原来容量的 50%，即永远不会阻塞，最大扩容可达 Integer.MAX_VALUE，所以最多只能创建核心线程数的线程。 ArrayBlockingQueue（有界阻塞队列）：底层由数组实现，容量一旦创建，就不能修改 11. 线程池处理任务的流程了解吗？ 线程池在提交任务前，可以提前创建线程吗？ 答案是可以的！ThreadPoolExecutor 提供了两个方法帮助我们在提交任务之前，完成核心线程的创建，从而实现线程池预热的效果： prestartCoreThread():启动一个线程，等待任务，如果已达到核心线程数，这个方法返回 false，否则返回 true； prestartAllCoreThreads():启动所有的核心线程，并返回启动成功的核心线程数。 12. 线程池中线程异常后，销毁还是复用？ 简单来说： 使用execute()时，未捕获异常导致线程终止，线程池创建新线程替代； 使用submit()时，异常被封装在Future中，线程继续复用。 13. 如何设定线程池的大小？ CPU 密集型任务(N+1)： 这种任务消耗的主要是 CPU 资源，可以将线程数设置为 N（CPU 核心数）+1。比 CPU 核心数多出来的一个线程是为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用 CPU 的空闲时间。 I/O 密集型任务(2N)： 这种任务应用起来，系统会用大部分的时间来处理 I/O 交互，而线程在处理 I/O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用。因此在 I/O 密集型任务的应用中，我们可以多配置一些线程，具体的计算方法是 2N。 CPU 密集型简单理解就是利用 CPU 计算能力的任务比如你在内存中对大量数据进行排序。但凡涉及到网络读取，文件读取这类都是 IO 密集型，这类任务的特点是 CPU 计算耗费时间相比于等待 IO 操作完成的时间来说很少，大部分时间都花在了等待 IO 操作完成上。 14. 如何设计一个能够根据任务的优先级来执行的线程池？ 使用 PriorityBlockingQueue （优先级阻塞队列）作为任务队列。 15. 如何知道是否要销毁线程？ 线程进入空闲状态： 当线程执行完任务后，会尝试从任务队列中获取新任务。 如果任务队列为空，线程会调用BlockingQueue.poll(keepAliveTime, TimeUnit)方法等待新任务。 线程超时： 如果线程在keepAliveTime时间内没有获取到新任务，poll方法会返回null。 线程池会将该线程标记为可销毁。 线程销毁： 线程池会检查当前线程数是否大于corePoolSize。 如果大于corePoolSize，线程池会调用线程的interrupt()方法，终止线程的执行；否则，线程会继续等待任务。 核心线程是线程池中始终存在的线程，即使它们处于空闲状态也不会被销毁（除非设置了allowCoreThreadTimeOut(true)） 16. 线程池区分临时线程和核心线程吗，销毁的一定是后来创建的临时线程吗? 销毁的线程 不一定是后来创建的临时线程，而是销毁线程在keepAliveTime时间内没有在任务队列获取到新任务的线程。 线程池在内部 并不显式区分核心线程和临时线程，临时线程的数量由maximumPoolSize - corePoolSize决定。 Future 1. Future 类有什么用？ 当我们执行某一耗时的任务时，可以将这个耗时任务交给一个子线程去异步执行，同时我们可以干点其他事情，不用傻傻等待耗时任务执行完成。 // V 代表了Future执行的任务返回值的类型 public interface Future { // 取消任务执行，成功取消返回 true，否则返回 false boolean cancel(boolean mayInterruptIfRunning); // 判断任务是否被取消 boolean isCancelled(); // 判断任务是否已经执行完成 boolean isDone(); // 获取任务执行结果 V get() throws InterruptedException, ExecutionException; // 指定时间内没有返回计算结果就抛出 TimeOutException 异常 V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutExceptio } 2. Callable 和 Future 有什么关系？ FutureTask 提供了 Future 接口的基本实现，ExecutorService.submit() 方法返回的其实就是 Future 的实现类 FutureTask 。 FutureTask 有两个构造函数，可传入 Callable 或者 Runnable 对象。实际上，传入 Runnable 对象也会在方法内部转换为Callable 对象。 3. 一个任务需要依赖另外两个任务执行完之后再执行，怎么设计？ 1:使用 CountDownLatch import java.util.concurrent.CountDownLatch; public class CountDownLatchExample { public static void main(String[] args) throws InterruptedException { // 创建一个 CountDownLatch，计数为 2 CountDownLatch latch = new CountDownLatch(2); // 第一个依赖任务 Thread task1 = new Thread(() -> { try { System.out.println(\"任务 1 开始执行\"); Thread.sleep(2000); // 模拟耗时操作 System.out.println(\"任务 1 执行完毕\"); } catch (InterruptedException e) { e.printStackTrace(); } finally { // 任务完成，计数减 1 latch.countDown(); } }); // 第二个依赖任务 Thread task2 = new Thread(() -> { try { System.out.println(\"任务 2 开始执行\"); Thread.sleep(3000); // 模拟耗时操作 System.out.println(\"任务 2 执行完毕\"); } catch (InterruptedException e) { e.printStackTrace(); } finally { // 任务完成，计数减 1 latch.countDown(); } }); // 启动两个依赖任务 task1.start(); task2.start(); // 主线程等待两个任务完成 latch.await(); // 执行依赖于前两个任务的任务 System.out.println(\"开始执行依赖任务\"); System.out.println(\"依赖任务执行完毕\"); } } 2.CompletableFuture import java.util.concurrent.CompletableFuture; import java.util.concurrent.ExecutionException; public class CompletableFutureExample { public static void main(String[] args) throws ExecutionException, InterruptedException { // 第一个依赖任务 CompletableFuture task1 = CompletableFuture.runAsync(() -> { try { System.out.println(\"任务 1 开始执行\"); Thread.sleep(2000); // 模拟耗时操作 System.out.println(\"任务 1 执行完毕\"); } catch (InterruptedException e) { e.printStackTrace(); } }); // 第二个依赖任务 CompletableFuture task2 = CompletableFuture.runAsync(() -> { try { System.out.println(\"任务 2 开始执行\"); Thread.sleep(3000); // 模拟耗时操作 System.out.println(\"任务 2 执行完毕\"); } catch (InterruptedException e) { e.printStackTrace(); } }); // 等待两个任务完成 CompletableFuture allTasks = CompletableFuture.allOf(task1, task2); // 执行依赖于前两个任务的任务 CompletableFuture dependentTask = allTasks.thenRun(() -> { System.out.println(\"开始执行依赖任务\"); System.out.println(\"依赖任务执行完毕\"); }); // 等待依赖任务完成 dependentTask.get(); } } 3.使用线程池和 Future import java.util.concurrent.*; public class FutureExample { public static void main(String[] args) { // 创建一个固定大小的线程池 ExecutorService executor = Executors.newFixedThreadPool(2); // 第一个依赖任务 Future task1 = executor.submit(() -> { try { System.out.println(\"任务 1 开始执行\"); Thread.sleep(2000); // 模拟耗时操作 System.out.println(\"任务 1 执行完毕\"); } catch (InterruptedException e) { e.printStackTrace(); } }); // 第二个依赖任务 Future task2 = executor.submit(() -> { try { System.out.println(\"任务 2 开始执行\"); Thread.sleep(3000); // 模拟耗时操作 System.out.println(\"任务 2 执行完毕\"); } catch (InterruptedException e) { e.printStackTrace(); } }); try { // 等待两个任务完成 task1.get(); task2.get(); // 执行依赖于前两个任务的任务 System.out.println(\"开始执行依赖任务\"); System.out.println(\"依赖任务执行完毕\"); } catch (InterruptedException | ExecutionException e) { e.printStackTrace(); } finally { // 关闭线程池 executor.shutdown(); } } } Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-03-02 00:07:36 "},"Chapter3/Java集合.html":{"url":"Chapter3/Java集合.html","title":"Java集合","keywords":"","body":"集合概述 1. 集合框架图 只有Vector和HashTable和Stack是线程安全的，其他的都是线程不安全。 2. 集合框架底层数据结构总结 2.1 List ArrayList：Object[] 数组。详细可以查看：ArrayList 源码分析。 Vector：Object[] 数组。 LinkedList：双向链表(JDK1.6 之前为循环链表，JDK1.7 取消了循环)。详细可以查看：LinkedList 源码分析。 2.2 Set HashSet(无序，唯一): 基于 HashMap 实现的，底层采用 HashMap 来保存元素，基于 HashMap 实现。它将元素存储在 HashMap 的键中，而 HashMap 的值则统一使用一个静态的 PRESENT 对象。 LinkedHashSet: LinkedHashSet 是 HashSet 的子类，并且其内部是通过 LinkedHashMap 来实现的，它在 HashMap 的基础上维护了一个双向链表，用于记录元素的插入顺序。 TreeSet(有序，唯一): 红黑树(自平衡的排序二叉树)。 2.3 Queue PriorityQueue: Object[] 数组来实现小顶堆。详细可以查看：PriorityQueue 源码分析。 DelayQueue:PriorityQueue。详细可以查看：DelayQueue 源码分析。 ArrayDeque: 可扩容动态双向数组。 2.4 Map HashMap：JDK1.8 之前 HashMap 由数组+链表组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）。JDK1.8 以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。详细可以查看：HashMap 源码分析。 LinkedHashMap：LinkedHashMap 继承自 HashMap，所以它的底层仍然是基于拉链式散列结构即由数组和链表或红黑树组成。另外，LinkedHashMap 在上面结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。详细可以查看：LinkedHashMap 源码分析 Hashtable：数组+链表组成的，数组是 Hashtable 的主体，链表则是主要为了解决哈希冲突而存在的。 TreeMap：红黑树（自平衡的排序二叉树）。 List 1. ArrayList 和 Array（数组）的区别？ ArrayList 内部基于动态数组实现，比 Array（静态数组） 使用起来更加灵活： ArrayList会根据实际存储的元素动态地扩容或缩容，而 Array 被创建之后就不能改变它的长度了。 ArrayList 允许你使用泛型来确保类型安全，Array 则不可以。 ArrayList 中只能存储对象。对于基本类型数据，需要使用其对应的包装类（如 Integer、Double 等）。Array 可以直接存储基本类型数据，也可以存储对象。 ArrayList 支持插入、删除、遍历等常见操作，并且提供了丰富的 API 操作方法，比如 add()、remove()等。Array 只是一个固定长度的数组，只能按照下标访问其中的元素，不具备动态添加、删除元素的能力。 ArrayList创建时不需要指定大小，而Array创建时必须指定大小。 2. ArrayList 和 Vector 的区别?（了解即可） ArrayList 是 List 的主要实现类，底层使用 Object[]存储，适用于频繁的查找工作，线程不安全 。 Vector 是 List 的古老实现类，底层使用Object[] 存储，线程安全。 3. Vector 和 Stack 的区别?（了解即可） Vector 和 Stack 两者都是线程安全的，都是使用 synchronized 关键字进行同步处理。 Stack 继承自 Vector，是一个后进先出的栈，而 Vector 是一个列表。 随着 Java 并发编程的发展，Vector 和 Stack 已经被淘汰，推荐使用并发集合类（例如 ConcurrentHashMap、CopyOnWriteArrayList 等）或者手动实现线程安全的方法来提供安全的多线程操作支持。 4. ArrayList 可以添加 null 值吗？ ArrayList 中可以存储任何类型的对象，包括 null 值。不过，不建议向ArrayList 中添加 null 值， null 值无意义，会让代码难以维护比如忘记做判空处理就会导致空指针异常。 5. ArrayList 插入和删除元素的时间复杂度？ 对于插入： 头部插入：由于需要将所有元素都依次向后移动一个位置，因此时间复杂度是 O(n)。 尾部插入：当 ArrayList 的容量未达到极限时，往列表末尾插入元素的时间复杂度是 O(1)，因为它只需要在数组末尾添加一个元素即可；当容量已达到极限并且需要扩容时，则需要执行一次 O(n) 的操作将原数组复制到新的更大的数组中，然后再执行 O(1) 的操作添加元素。 指定位置插入：需要将目标位置之后的所有元素都向后移动一个位置，然后再把新元素放入指定位置。这个过程需要移动平均 n/2 个元素，因此时间复杂度为 O(n)。 对于删除： 头部删除：由于需要将所有元素依次向前移动一个位置，因此时间复杂度是 O(n)。 尾部删除：当删除的元素位于列表末尾时，时间复杂度为 O(1)。 指定位置删除：需要将目标元素之后的所有元素向前移动一个位置以填补被删除的空白位置，因此需要移动平均 n/2 个元素，时间复杂度为 O(n)。 6. LinkedList 插入和删除元素的时间复杂度？ 头部插入/删除：只需要修改头结点的指针即可完成插入/删除操作，因此时间复杂度为 O(1)。 尾部插入/删除：只需要修改尾结点的指针即可完成插入/删除操作，因此时间复杂度为 O(1)。 指定位置插入/删除：需要先移动到指定位置，再修改指定节点的指针完成插入/删除，不过由于有头尾指针，可以从较近的指针出发，因此需要遍历平均 n/4 个元素，时间复杂度为 O(n)。 7. LinkedList 为什么不能实现 RandomAccess 接口？ 由于 LinkedList 底层数据结构是链表，内存地址不连续，只能通过指针来定位，不支持随机快速访问，所以不能实现 RandomAccess 接口。 注意：RandomAccess 接口只是标识（没定义功能），并不是说 ArrayList 实现 RandomAccess 接口才具有快速随机访问功能的！ 8. ArrayList 与 LinkedList 区别? 是否保证线程安全： ArrayList 和 LinkedList 都是不同步的，也就是不保证线程安全； 底层数据结构：ArrayList 底层使用的是 Object 数组；LinkedList 底层使用的是 双向链表 数据结构（JDK1.6 之前为循环链表，JDK1.7 取消了循环。注意双向链表和双向循环链表的区别，下面有介绍到！） 插入和删除是否受元素位置的影响： ArrayList 采用数组存储，所以插入和删除元素的时间复杂度受元素位置的影响。 比如：执行add(E e)方法的时候， ArrayList 会默认在将指定的元素追加到此列表的末尾，这种情况时间复杂度就是 O(1)。但是如果要在指定位置 i 插入和删除元素的话（add(int index, E element)），时间复杂度就为 O(n)。因为在进行上述操作的时候集合中第 i 和第 i 个元素之后的(n-i)个元素都要执行向后位/向前移一位的操作。 LinkedList 采用链表存储，所以在头尾插入或者删除元素不受元素位置的影响（add(E e)、addFirst(E e)、addLast(E e)、removeFirst()、 removeLast()），时间复杂度为 O(1)，如果是要在指定位置 i 插入和删除元素的话（add(int index, E element)，remove(Object o),remove(int index)）， 时间复杂度为 O(n) ，因为需要先移动到指定位置再插入和删除。 是否支持快速随机访问：LinkedList 不支持高效的随机元素访问，而 ArrayList（实现了 RandomAccess 接口） 支持。快速随机访问就是通过元素的序号快速获取元素对象(对应于get(int index)方法)。 内存空间占用：ArrayList 的空间浪费主要体现在在 list 列表的结尾会预留一定的容量空间，而 LinkedList 的空间花费则体现在它的每一个元素都需要消耗比 ArrayList 更多的空间（因为要存放直接后继和直接前驱以及数据）。 我们在项目中一般是不会使用到 LinkedList 的，需要用到 LinkedList 的场景几乎都可以使用 ArrayList 来代替，并且，性能通常会更好！就连 LinkedList 的作者约书亚 · 布洛克（Josh Bloch）自己都说从来不会使用 LinkedList 。 9. 说一说 ArrayList 的扩容机制吧 ArrayList在添加元素时，如果当前元素个数已经达到了内部数组的容量上限，就会触发扩容操作。 计算新的容量：一般情况下，新的容量会扩大为原容量的1.5倍（在JDK 10之后，扩容策略做了调整），然后检查是否超过了最大容量限制。 创建新的数组：根据计算得到的新容量，创建一个新的更大的数组。 将元素复制：将原来数组中的元素逐个复制到新数组中。 更新引用：将ArrayList内部指向原数组的引用指向新数组。 完成扩容：扩容完成后，可以继续添加新元素。 ArrayList的扩容操作涉及到数组的复制和内存的重新分配，所以在频繁添加大量元素时，扩容操作可能会影响性能。为了减少扩容带来的性能损耗，可以在初始化ArrayList时预分配足够大的容量，避免频繁触发扩容操作。 之所以扩容是 1.5 倍，是因为 1.5 可以充分利用移位操作，减少浮点数或者运算时间和运算次数。 int newCapacity = oldCapacity + (oldCapacity >> 1),所以 ArrayList 每次扩容之后容量都会变为原来的 1.5 倍左右（oldCapacity 为偶数就是 1.5 倍，否则是 1.5 倍左右）！ 奇偶不同，比如：10+10/2 = 15, 33+33/2=49。如果是奇数的话会丢掉小数. 以无参数构造方法创建 ArrayList 时，实际上初始化赋值的是一个空数组。当真正对数组进行添加元素操作时，才真正分配容量。即向数组中添加第一个元素时，数组容量扩为 10。 下面在我们分析 ArrayList 扩容时会讲到这一点内容！ 10. 为什么ArrayList不是线程安全的，具体来说是哪里不安全？ 在高并发添加数据下，ArrayList会暴露三个问题; 部分值为null：当线程1走到了扩容那里发现当前size是9，而数组容量是10，所以不用扩容，这时候cpu让出执行权，线程2也进来了，发现size是9，而数组容量是10，所以不用扩容，这时候线程1继续执行，将数组下标索引为9的位置set值了，还没有来得及执行size++，这时候线程2也来执行了，又把数组下标索引为9的位置set了一遍，这时候两个先后进行size++，导致下标索引10的地方就为null了。 索引越界异常：线程1走到扩容那里发现当前size是9，数组容量是10不用扩容，cpu让出执行权，线程2也发现不用扩容，这时候数组的容量就是10，而线程1 set完之后size++，这时候线程2再进来size就是10，数组的大小只有10，而你要设置下标索引为10的就会越界（数组的下标索引从0开始）； size与我们add的数量不符：这个基本上每次都会发生，这个理解起来也很简单，因为size++本身就不是原子操作，可以分为三步：获取size的值，将size的值加1，将新的size值覆盖掉原来的，线程1和线程2拿到一样的size值加完了同时覆盖，就会导致一次没有加上，所以肯定不会与我们add的数量保持一致的； Set 1. 无序性和不可重复性的含义是什么 无序性不等于随机性 ，无序性是指存储的数据在底层数组中并非按照数组索引的顺序添加 ，而是根据数据的哈希值决定的。 不可重复性是指添加的元素按照 equals() 判断时 ，返回 false，需要同时重写 equals() 方法和 hashCode() 方法。 2. 比较 HashSet、LinkedHashSet 和 TreeSet 三者的异同 HashSet、LinkedHashSet 和 TreeSet 都是 Set 接口的实现类，都能保证元素唯一，并且都不是线程安全的。 HashSet、LinkedHashSet 和 TreeSet 的主要区别在于底层数据结构不同。HashSet 的底层数据结构是哈希表（基于 HashMap 实现）。LinkedHashSet 的底层数据结构是链表和哈希表，元素的插入和取出顺序满足 FIFO。TreeSet 底层数据结构是红黑树，元素是有序的，排序的方式有自然排序和定制排序。 底层数据结构不同又导致这三者的应用场景不同。HashSet 用于不需要保证元素插入和取出顺序的场景，LinkedHashSet 用于保证元素的插入和取出顺序满足 FIFO 的场景，TreeSet 用于支持对元素自定义排序规则的场景。 HashSet、LinkedHashSet 和 TreeSet 插入删除复杂度分别为 O (1)、 O (1)、 O (log n) 3.HashSet 如何检查重复? 当你把对象加入HashSet时，HashSet 会先计算对象的hashcode值来判断对象加入的位置，同时也会与其他加入的对象的 hashcode 值作比较，如果没有相符的 hashcode，HashSet 会假设对象没有重复出现。但是如果发现有相同 hashcode 值的对象，这时会调用equals()方法来检查 hashcode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让加入操作成功。 也就是说，在 JDK1.8 中，实际上无论HashSet中是否已经存在了某元素，HashSet都会直接插入，只是会在add()方法的返回值处告诉我们插入前是否存在相同元素。 Queue 1. Queue 与 Deque 的区别 2. ArrayDeque 与 LinkedList 的区别 ArrayDeque 和 LinkedList 都实现了 Deque 接口，两者都具有队列的功能，但两者有什么区别呢？ ArrayDeque 是基于可变长的数组和双指针来实现，而 LinkedList 则通过链表来实现。 ArrayDeque 不支持存储 NULL 数据，但 LinkedList 支持。 ArrayDeque 是在 JDK1.6 才被引入的，而LinkedList 早在 JDK1.2 时就已经存在。 ArrayDeque 插入时可能存在扩容过程, 不过均摊后的插入操作依然为 O(1)。虽然 LinkedList 不需要扩容，但是每次插入数据时均需要申请新的堆空间，均摊性能相比更慢。 从性能的角度上，选用 ArrayDeque 来实现队列要比 LinkedList 更好。此外，ArrayDeque 也可以用于实现栈。 3. 说一说 PriorityQueue PriorityQueue 利用了二叉堆的数据结构来实现的，底层使用可变长的数组来存储数据 PriorityQueue 通过堆元素的上浮和下沉，实现了在 O(logn) 的时间复杂度内插入元素和删除堆顶元素。 PriorityQueue 是非线程安全的，且不支持存储 NULL 和 non-comparable 的对象。 PriorityQueue 默认是小顶堆，但可以接收一个 Comparator 作为构造参数，从而来自定义元素优先级的先后。 4. 什么是 BlockingQueue？ BlockingQueue （阻塞队列）是一个接口，继承自 Queue。BlockingQueue阻塞的原因是其支持当队列没有元素时一直阻塞，直到有元素；还支持如果队列已满，一直等到队列可以放入新元素时再放入。 Map 1. HashMap 和 Hashtable 的区别 线程是否安全： HashMap 是非线程安全的，Hashtable 是线程安全的,因为 Hashtable 内部的方法基本都经过synchronized 修饰。（如果你要保证线程安全的话就使用 ConcurrentHashMap 吧！）； 效率： 因为线程安全的问题，HashMap 要比 Hashtable 效率高一点。另外，Hashtable 基本被淘汰，不要在代码中使用它； 对 Null key 和 Null value 的支持： HashMap 可以存储 null 的 key 和 value，但 null 作为键只能有一个，null 作为值可以有多个；Hashtable 不允许有 null 键和 null 值，否则会抛出 NullPointerException。 初始容量大小和每次扩充容量大小的不同： ① 创建时如果不指定容量初始值，Hashtable 默认的初始大小为 11，之后每次扩充，容量变为原来的 2n+1。HashMap 默认的初始化大小为 16。之后每次扩充，容量变为原来的 2 倍。② 创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 HashMap 会将其扩充为 2 的幂次方大小（HashMap 中的tableSizeFor()方法保证，下面给出了源代码）。也就是说 HashMap 总是使用 2 的幂作为哈希表的大小,后面会介绍到为什么是 2 的幂次方。 底层数据结构： JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）时，将链表转化为红黑树（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树），以减少搜索时间（后文中我会结合源码对这一过程进行分析）。Hashtable 没有这样的机制。 哈希函数的实现：HashMap 对哈希值进行了高位和低位的混合扰动处理以减少冲突，而 Hashtable 直接使用键的 hashCode() 值。 2. HashMap 和 HashSet 区别 HashSet 底层就是基于 HashMap 实现的。（HashSet 的源码非常非常少，因为除了 clone()、writeObject()、readObject()是 HashSet 自己不得不实现之外，其他方法都是直接调用 HashMap 中的方法。 3. HashMap 和 TreeMap 区别 哈希表、红黑树 无序、有序 O(1)、O(logn) 4. HashMap 的底层实现 HashMap 通过 key 的 hashcode 经过扰动函数处理过后得到 hash 值，然后通过 (n - 1) & hash 判断当前元素存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。 HashMap 中的扰动函数（hash 方法）是用来优化哈希值的分布。通过对原始的 hashCode() 进行额外处理，扰动函数可以减小由于糟糕的 hashCode() 实现导致的碰撞，从而提高数据的分布均匀性。 JDK1.8之前： JDK1.8之后 5. HashMap 的长度为什么是 2 的幂次方 得到hash值之后，用之前还要先做对数组的长度取模运算，得到的余数才能用来要存放的位置也就是对应的数组下标。 位运算效率更高：位运算(&)比取余运算(%)更高效。当长度为 2 的幂次方时，hash % length 等价于 hash & (length - 1)。 可以更好地保证哈希值的均匀分布：扩容之后，在旧数组元素 hash 值比较均匀的情况下，新数组元素也会被分配的比较均匀，最好的情况是会有一半在新数组的前半部分，一半在新数组后半部分。 扩容机制变得简单和高效：扩容后只需检查哈希值高位的变化来决定元素的新位置，要么位置不变（高位为 0），要么就是移动到新位置（高位为 1，原索引位置+原容量）。 假设有一个元素的哈希值为 10101100 旧数组元素位置计算： hash = 10101100 length - 1 = 00000111 & ----------------- index = 00000100 (4) 新数组元素位置计算： hash = 10101100 length - 1 = 00001111 & ----------------- index = 00001100 (12) 看第四位（从右数）： 1.高位为 0：位置不变。 2.高位为 1：移动到新位置（原索引位置+原容量）。 6. 列举HashMap在多线程下可能会出现的问题？ JDK1.7中的 HashMap 使用头插法插入元素，在多线程的环境下，扩容的时候有可能导致环形链表的出现，形成死循环。因此，JDK1.8使用尾插法插入元素，在扩容时会保持链表元素原本的顺序，不会出现环形链表的问题。 多线程同时执行 put 操作，如果计算出来的索引位置是相同的（比如发生了哈希冲突），那会造成前一个 key 被后一个 key 覆盖，从而导致元素的丢失。此问题在JDK 1.7和 JDK 1.8 中都存在 7. ConcurrentHashMap 和 Hashtable 的区别 HashTable底层是数组+链表，它对整个 Hashtable 对象进行加锁，效率十分低。 ConcurrentHashMap，在 JDK1.7 的时候， 采用分段的数组+链表，但是每一把锁只锁容器其中一部分数据（下面有示意图），多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。 到了 JDK1.8 的时候，直接用 Node 数组+链表+红黑树的数据结构来实现，并发控制使用 synchronized 和 CAS 来操作，像是优化过且线程安全的 HashMap。 8. ConcurrentHashMap 线程安全的具体实现方式/底层具体实现 JDK1.8 之前 一个 ConcurrentHashMap 里包含一个 Segment 数组，Segment 的个数一旦初始化就不能改变。 Segment 数组的大小默认是 16，也就是说默认可以同时支持 16 个线程并发写。 也就是说，对同一 Segment 的并发写入会被阻塞，不同 Segment 的写入是可以并发执行的。 JDK1.8 之后 Java 8 中，锁粒度更细，synchronized 只锁定当前链表或红黑二叉树的首节点，这样只要 hash 不冲突，就不会产生并发，就不会影响其他 Node 的读写，效率大幅提升。 9. ConcurrentHashMap 能保证复合操作的原子性吗？ 如果线程 A 和 B 的执行顺序是这样： 线程 A 判断 map 中不存在 key 线程 B 判断 map 中不存在 key 线程 B 将 (key, anotherValue) 插入 map 线程 A 将 (key, value) 插入 map 那么最终的结果是 (key, value)，而不是预期的 (key, anotherValue)。这就是复合操作的非原子性导致的问题。 提供了一些原子性的复合操作，如 putIfAbsent、compute、computeIfAbsent 、computeIfPresent、merge等。这些方法都可以接受一个函数作为参数，根据给定的 key 和 value 来计算一个新的 value，并且将其更新到 map 中。 // 线程 A map.putIfAbsent(key, value); // 线程 B map.putIfAbsent(key, anotherValue); 10. ConcurrentHashMap已经用了synchronized，为什么还要用CAS（乐观锁）呢？ ConcurrentHashMap使用这两种手段来保证线程安全主要是一种权衡的考虑，在某些操作中使用synchronized，还是使用CAS，主要是根据锁竞争程度来判断的。 比如：在putVal中，如果计算出来的hash槽没有存放元素，那么就可以直接使用CAS来进行设置值，这是因为在设置元素的时候，因为hash值经过了各种扰动后，造成hash碰撞的几率较低，那么我们可以预测使用较少的自旋来完成具体的hash落槽操作。 当发生了hash碰撞的时候说明容量不够用了或者已经有大量线程访问了，因此这时候使用synchronized来处理hash碰撞比CAS效率要高，因为发生了hash碰撞大概率来说是线程竞争比较强烈。 11. 为什么HashMap要用红黑树而不是平衡二叉树？ 与平衡树不同的是，红黑树在插入、删除等操作，不会像平衡树那样，频繁着破坏红黑树的规则，所以不需要频繁着调整，这也是我们为什么大多数情况下使用红黑树的原因。 12. HashMap的扩容机制介绍一下 hashMap默认的负载因子是0.75，即如果hashmap中的元素个数超过了总容量75%，则会触发扩容，扩容分为两个步骤： 第1步是对哈希表长度的扩展（2倍） 第2步是将旧哈希表中的数据放到新的哈希表中。 13. 往hashmap存20个元素，会扩容几次？ 当插入 20 个元素时，HashMap 的扩容过程如下： 初始容量：16 插入第 1 到第 12 个元素时，不需要扩容。 插入第 13 个元素时，达到负载因子限制，需要扩容。此时，HashMap 的容量从 16 扩容到 32。 扩容后的容量：32 插入第 14 到第 24 个元素时，不需要扩容。 因此，总共会进行一次扩容。 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-02-06 14:57:59 "},"Chapter3/JVM.html":{"url":"Chapter3/JVM.html","title":"JVM","keywords":"","body":"Java内存模型 1. 基础知识 线程私有的： 程序计数器 虚拟机栈 本地方法栈 线程共享的： 堆 方法区 直接内存 (非运行时数据区的一部分) Java 虚拟机规范对于运行时数据区域的规定是相当宽松的。以堆为例：堆可以是连续空间，也可以不连续。堆的大小可以固定，也可以在运行时按需扩展 。虚拟机实现者可以使用任何垃圾回收算法管理堆，甚至完全不进行垃圾收集也是可以的。 2. 程序计数器 程序计数器是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完成。 另外，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 程序计数器是唯一一个不会出现 OutOfMemoryError 的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 3. Java 虚拟机栈 栈绝对算的上是 JVM 运行时数据区域的一个核心，除了一些 Native 方法调用是通过本地方法栈实现的(后面会提到)，其他所有的 Java 方法调用都是通过栈来实现的（也需要和其他运行时数据区域比如程序计数器配合）。 栈帧随着方法调用而创建，随着方法结束而销毁。无论方法正常完成还是异常完成都算作方法结束。 栈空间虽然不是无限的，但一般正常调用的情况下是不会出现问题的。不过，如果函数调用陷入无限循环的话，就会导致栈中被压入太多栈帧而占用太多空间，导致栈空间过深。那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度的时候，就抛出 StackOverFlowError 错误。 局部变量表 主要存放了编译期可知的各种数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference 类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）。 操作数栈 主要作为方法调用的中转站使用，用于存放方法执行过程中产生的中间计算结果。另外，计算过程中产生的临时变量也会放在操作数栈中。 动态链接 主要服务一个方法需要调用其他方法的场景。Class 文件的常量池里保存有大量的符号引用比如方法引用的符号引用。当一个方法要调用其他方法，需要将常量池中指向方法的符号引用转化为其在内存地址中的直接引用。动态链接的作用就是为了将符号引用转换为调用方法的直接引用，这个过程也被称为 动态连接 。 4. 本地方法栈 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 本地方法被执行的时候，在本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。 5. 堆 Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。 此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。 Java 堆是垃圾收集器管理的主要区域，因此也被称作 GC 堆（Garbage Collected Heap）。 6. 方法区 方法区属于是 JVM 运行时数据区域的一块逻辑区域，是各个线程共享的内存区域。 当虚拟机要使用一个类时，它需要读取并解析 Class 文件获取相关信息，再将信息存入到方法区。 方法区会存储已被虚拟机加载的 类信息、字段信息、方法信息、常量、静态变量、即时编译器编译后的代码缓存等数据。 7. 运行时常量池（方法区中） 用于存储编译期生成的字面量和符号引用。 字面量： 符号引用： 8. 字符串常量池（在堆中） 字符串常量池 是 JVM 为了提升性能和减少内存消耗针对字符串（String 类）专门开辟的一块区域，主要目的是为了避免字符串的重复创建。 Java 程序中通常会有大量的被创建的字符串等待回收，将字符串常量池放到堆中，能够更高效及时地回收字符串内存。 9. 直接内存 直接内存是一种特殊的内存缓冲区，并不在 Java 堆或方法区中分配的，而是通过 JNI 的方式在本地内存上分配的。 10. 堆和栈的区别 HotSpot 虚拟机对象探秘 1. 对象的创建 Step1:类加载检查 虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 Step2:分配内存 在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。 指针碰撞（Bump the Pointer）：如果Java堆内存是规整的（即已使用的内存和未使用的内存之间有明确的分界指针），那么分配内存仅仅是将指针向未使用的内存区域移动一段与对象大小相等的距离。 空闲列表（Free List）：如果Java堆内存不是规整的，JVM会维护一个空闲列表，记录哪些内存块是可用的，然后从列表中找到一个足够大的空间分配给对象。 内存分配并发问题（补充内容，需要掌握） 在创建对象的时候有一个很重要的问题，就是线程安全，因为在实际开发过程中，创建对象是很频繁的事情，作为虚拟机来说，必须要保证线程是安全的，通常来讲，虚拟机采用两种方式来保证线程安全： CAS+失败重试： CAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性。 TLAB： 为每一个线程预先在 Eden 区分配一块儿内存，JVM 在给线程中的对象分配内存时，首先在 TLAB 分配，当对象大于 TLAB 中的剩余内存或 TLAB 的内存已用尽时，再采用上述的 CAS 进行内存分配 Step3:初始化零值 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，可以直接使用默认值（如 0、false、null 等），程序能访问到这些字段的数据类型所对应的零值。 Step4:设置对象头 初始化零值完成之后，虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。 Step5:执行 init 方法 在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始， 方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。 2. 对象的内存布局 对象在内存中的布局可以分为 3 块区域：对象头（Header）、实例数据（Instance Data）\\和*对齐填充（Padding）*。 对象头包括两部分信息： 标记字段（Mark Word）：用于存储对象自身的运行时数据， 如哈希码（HashCode）、GC 分代年龄、锁状态标志、线程持有的锁、偏向线程 ID、偏向时间戳等等。 类型指针（Klass pointer）：对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。 实例数据部分是对象真正存储的有效信息，也是在程序中所定义的各种类型的字段内容。 对齐填充部分不是必然存在的，也没有什么特别的含义，仅仅起占位作用。 因为 Hotspot 虚拟机的自动内存管理系统要求对象起始地址必须是 8 字节的整数倍，换句话说就是对象的大小必须是 8 字节的整数倍。而对象头部分正好是 8 字节的倍数（1 倍或 2 倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。 3. 对象的访问定位 见javaguide。句柄和没有句柄。 内存分配和回收原则 Eden 区与两个 Survivor 区：空间大小比例是 8:1:1, 大因为大部分新创建的对象在短时间内就会变为垃圾，因此可以将更多的空间分配给 Eden 区，以减少 Minor GC 的频率。 新生代与老年代：默认情况下，新生代和老年代的比例为 1:2。原因：新创建的对象大部分都是短生命周期的。 1. 对象优先在 Eden 区分配 大多数情况下，对象在新生代中 Eden 区分配。当 Eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC。 Minor GC 是指对新生代（Young Generation）的垃圾回收。 它的主要目标是清理 Eden 区 和 Survivor 区 中的无用对象，并将存活的对象移动到另一个 Survivor 区或老年代（分配担保机制 把新生代的对象提前转移到老年代中去）。 Minor GC 通常比 Major GC（针对老年代的垃圾回收）更频繁，因为新生代中的对象生命周期较短，大部分对象在 Minor GC 时就会被回收。 2. 大对象直接进入老年代 大对象就是需要大量连续内存空间的对象（比如：字符串、数组）。 大对象直接进入老年代的行为是由虚拟机动态决定的，它与具体使用的垃圾回收器和相关参数有关。 大对象直接进入老年代是一种优化策略，旨在避免将大对象放入新生代，从而减少新生代的垃圾回收频率和成本。 3. 长期存活的对象将进入老年代 虚拟机给每个对象一个对象年龄（Age）计数器。 对象都会首先在 Eden 区域分配。如果对象在 Eden 出生并经过第一次 Minor GC 后仍然能够存活，并且能被 Survivor 容纳的话，将被移动到 Survivor 空间（s0 或者 s1）中（轮转），并将对象年龄设为 1(Eden 区->Survivor 区后对象的初始年龄变为 1)。 对象在 Survivor 中每熬过一次 MinorGC,年龄就增加 1 岁，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 4. 进行 gc 的区域和时机 新生代收集（Minor GC / Young GC）：只对新生代进行垃圾收集；Eden 区空间不足触发Minor GC。 老年代收集（Major GC / Old GC）：只对老年代进行垃圾收集。需要注意的是 Major GC 在有的语境中也用于指代整堆收集；老年代空间不足触发。 混合收集（Mixed GC /full gc）：对整个新生代和部分老年代进行垃圾收集。 5. 空间分配担保 在 Minor GC 之前，JVM 会检查老年代是否有足够的连续空间来存放所有可能从新生代晋升的对象。如果没有足够的空间，JVM 会先触发一次 Full GC，清理老年代的空间，然后再进行 Minor GC。这个过程就是空间分配担保。 如果 Full GC 后仍然没有足够的空间，JVM 会抛出 OutOfMemoryError。 死亡对象判断方法 1. 引用计数法 给对象中添加一个引用计数器： 每当有一个地方引用它，计数器就加 1； 当引用失效，计数器就减 1； 任何时候计数器为 0 的对象就是不可能再被使用的。 这个方法实现简单，效率高，但是目前主流的虚拟机中并没有选择这个算法来管理内存，其最主要的原因是它很难解决对象之间循环引用的问题。 2. 可达性分析算法 这个算法的基本思想就是通过一系列的称为 “GC Roots” 的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的，需要被回收。 下图中的 Object 6 ~ Object 10 之间虽有引用关系，但它们到 GC Roots 不可达，因此为需要被回收的对象。 哪些对象可以作为 GC Roots 呢？ 虚拟机栈中的局部变量 方法区中静态变量 方法区中常量 所有被同步锁持有的对象 虚拟机内部引用：类加载器、Class对象 对象可以被回收，就代表一定会被回收吗？ 即使在可达性分析法中不可达的对象，也并非是“非死不可”的，这时候它们暂时处于“缓刑阶段”，要真正宣告一个对象死亡，至少要经历 两次标记过程；可达性分析法中不可达的对象被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行 finalize 方法。当对象没有覆盖 finalize 方法，或 finalize 方法已经被虚拟机调用过时，虚拟机将这两种情况视为没有必要执行。 被判定为需要执行的对象将会被放在一个队列中进行第二次标记，除非这个对象与引用链上的任何一个对象建立关联，否则就会被真的回收。 3. 引用类型总结 在程序设计中一般很少使用弱引用与虚引用，使用软引用的情况较多，这是因为软引用可以加速 JVM 对垃圾内存的回收速度，可以维护系统的运行安全，防止内存溢出（OutOfMemory）等问题的产生。 3.1 强引用 如果一个对象具有强引用，那就类似于必不可少的生活用品，垃圾回收器绝不会回收它。当内存空间不足，Java 虚拟机宁愿抛出 OutOfMemoryError 错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题 3.2 软引用 如果一个对象只具有软引用，那就类似于可有可无的生活用品。如果内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。 3.3 弱引用 如果一个对象只具有弱引用，那就类似于可有可无的生活用品。 弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。 弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java 虚拟机就会把这个弱引用加入到与之关联的引用队列中。 3.4 虚引用 \"虚引用\"顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。 4. 如何判断一个常量是废弃常量？ 假如在字符串常量池中存在字符串 \"abc\"，如果当前没有任何 String 对象引用该字符串常量的话，就说明常量 \"abc\" 就是废弃常量，如果这时发生内存回收的话而且有必要的话，\"abc\" 就会被系统清理出常量池了。 5. 如何判断一个类是无用的类？ 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 垃圾收集算法 1. 标记-清除算法 分为两个过程：标记和清除。 效率问题：标记和清除两个过程效率都不高。 空间问题：标记清除后会产生大量不连续的内存碎片。 2. 复制算法 可用内存变小：可用内存缩小为原来的一半。 不适合老年代：如果存活对象数量比较大，复制性能会变得很差。 3. 标记-整理算法 由于多了整理这一步，因此效率也不高，适合老年代这种垃圾回收频率不是很高的场景。 4. 分代收集算法( HotSpot 为什么要分为新生代和老年代？) 当前虚拟机的垃圾收集都采用分代收集算法，这种算法没有什么新的思想，只是根据对象存活周期的不同将内存分为几块。 一般将 Java 堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。 比如在新生代中，每次收集都会有大量对象死去，所以可以选择“复制”算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。 而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。 垃圾收集器 如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现 1. 基础知识 如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。 直到现在为止还没有最好的垃圾收集器出现，更加没有万能的垃圾收集器，我们能做的就是根据具体应用场景选择适合自己的垃圾收集器。 JDK 8: Parallel Scavenge（新生代）+ Parallel Old（老年代） JDK 9 ~ JDK22: G1 其他知识见javaguide。 2. Serial 收集器 Serial（串行）收集器是最基本、历史最悠久的垃圾收集器了。 它的 “单线程” 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ \"Stop The World\" ），直到它收集结束。 新生代采用标记-复制算法，老年代采用标记-整理算法。 3. Serial Old 收集器 Serial 收集器的老年代版本，它同样是一个单线程收集器。它主要有两大用途：一种用途是在 JDK1.5 以及以前的版本中与 Parallel Scavenge 收集器搭配使用，另一种用途是作为 CMS 收集器的后备方案。 4. ParNew 收集器 ParNew 收集器其实就是 Serial 收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和 Serial 收集器完全一样。 但是会暂停所有用户线程。 新生代采用标记-复制算法，老年代采用标记-整理算法。 5. Parallel Scavenge 收集器 它看上去几乎和 ParNew 都一样。所谓吞吐量就是 CPU 中用于运行用户代码的时间与 CPU 总消耗时间的比值。 Parallel Scavenge 收集器关注点是吞吐量（高效率的利用 CPU）。 CMS 等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。 Parallel Scavenge 收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解，手工优化存在困难的时候，使用 Parallel Scavenge 收集器配合自适应调节策略，把内存管理优化交给虚拟机去完成也是一个不错的选择。 6. Parallel Old 收集器 Parallel Scavenge 收集器的老年代版本。使用多线程和“标记-整理”算法。在注重吞吐量以及 CPU 资源的场合，都可以优先考虑 Parallel Scavenge 收集器和 Parallel Old 收集器。 7. CMS 收集器 CMS（Concurrent Mark Sweep）收集器是 HotSpot 虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。 CMS 收集器是一种 “标记-清除”算法实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤： 初始标记： 短暂停顿，标记直接与 root 相连的对象（根对象）； 并发标记： 同时开启 GC 和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以 GC 线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。 重新标记： 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短 并发清除： 开启用户线程，同时 GC 线程开始对未标记的区域做清扫。 并发收集、低停顿 对 CPU 资源敏感； 无法处理浮动垃圾； 它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。 CMS 垃圾回收器在 Java 9 中已经被标记为过时(deprecated)，并在 Java 14 中被移除。 8. G1 收集器 G1 (Garbage-First) 是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足 GC 停顿时间要求的同时,还具备高吞吐量性能特征。 从 JDK9 开始，G1 垃圾收集器成为了默认的垃圾收集器。 优点： G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region(这也就是它的名字 Garbage-First 的由来) ，保证了 G1 收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。 并行与并发：G1 能充分利用 CPU、多核环境下的硬件优势，使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间。部分其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行。 空间整合：与 CMS 的“标记-清除”算法不同，G1 从整体来看是基于“标记-整理”算法实现的收集器；从局部上来看是基于“标记-复制”算法实现的，产生的内存碎片少 可预测的停顿：这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在垃圾收集上的时间不得超过 N 毫秒。 分代收集：虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。 G1 收集器的运作大致分为以下几个步骤： 初始标记： 短暂停顿（Stop-The-World，STW），标记从 GC Roots 可直接引用的对象，即标记所有直接可达的活跃对象 并发标记：与应用并发运行，标记所有可达对象。 这一阶段可能持续较长时间，取决于堆的大小和对象的数量。 最终标记： 短暂停顿（STW），处理并发标记阶段结束后残留的少量未处理的引用变更。 筛选回收：根据标记结果，选择回收价值高的区域，复制存活对象到新区域，回收旧区域内存。这一阶段包含一个或多个停顿（STW），具体取决于回收的复杂度。 类 1. 类文件结构 1.1 魔数（Magic Number） 每个 Class 文件的头 4 个字节称为魔数（Magic Number）,它的唯一作用是确定这个文件是否为一个能被虚拟机接收的 Class 文件。 Java 规范规定魔数为固定值：0xCAFEBABE。如果读取的文件不是以这个魔数开头，Java 虚拟机将拒绝加载它。 1.2 Class 文件版本号 第 5 和第 6 个字节是次版本号，第 7 和第 8 个字节是主版本号。 每当 Java 发布大版本（比如 Java 8，Java9）的时候，主版本号都会加 1。你可以使用 javap -v 命令来快速查看 Class 文件的版本号信息。 高版本的 Java 虚拟机可以执行低版本编译器生成的 Class 文件，但是低版本的 Java 虚拟机不能执行高版本编译器生成的 Class 文件。所以，我们在实际开发的时候要确保开发的的 JDK 版本和生产环境的 JDK 版本保持一致。 1.3 常量池（Constant Pool） 字面量和符号引用。字面量比较接近于 Java 语言层面的的常量概念，如文本字符串、声明为 final 的常量值等。而符号引用则属于编译原理方面的概念。包括下面三类常量： 类和接口的全限定名 字段的名称和描述符 方法的名称和描述符 1.4 访问标志(Access Flags) 这个 Class 是类还是接口，是否为 public 或者 abstract 类型，如果是类的话是否声明为 final 等等。 1.5 当前类（This Class）、父类（Super Class）、接口（Interfaces）索引集合 类索引用于确定这个类的全限定名，父类索引用于确定这个类的父类的全限定名。 接口索引集合用来描述这个类实现了哪些接口，这些被实现的接口将按 implements (如果这个类本身是接口的话则是extends) 后的接口顺序从左到右排列在接口索引集合中。 1.6 字段表集合（Fields） 字段表（field info）用于描述接口或类中声明的变量。字段包括类级变量以及实例变量，但不包括在方法内部声明的局部变量。 1.7 方法表集合（Methods） 1.8 属性表集合（Attributes） 属性表集合的限制稍微宽松一些，不再要求各个属性表具有严格的顺序，并且只要不与已有的属性名重复，任何人实现的编译器都可以向属性表中写 入自己定义的属性信息，Java 虚拟机运行时会忽略掉它不认识的属性。 2. 类加载过程 2.1 加载 由类加载器完成，具体是哪个类加载器加载由 双亲委派模型 决定。 通过全类名获取定义此类的二进制字节流。 将字节流所代表的静态存储结构转换为方法区的运行时数据结构。 在内存中生成一个代表该类的 Class 对象，作为方法区这些数据的访问入口。 2.2 验证 确保 Class 文件的字节流中包含的信息符合《Java 虚拟机规范》的全部约束要求，保证这些信息被当作代码运行后不会危害虚拟机自身的安全。 2.3 准备 准备阶段是正式为类变量（静态变量）分配内存并设置类变量初始值的阶段，这些内存都将在方法区中分配。 2.4 解析 将常量池内的符号引用替换为直接引用的过程。 2.5 初始化 执行类的初始化代码，包括静态变量赋值（static int i= 3）和静态代码块。 触发条件： 创建类的实例（new）。 访问类的静态变量或静态方法。 使用反射（Class.forName()）。 初始化子类时，父类会被初始化。 JVM 启动时指定的主类（包含 main 方法的类）。 2.6 类卸载 该类的 Class 对象被 GC。 该类的所有的实例对象都已被 GC，也就是说堆不存在该类的实例对象。 该类没有在其他任何地方被引用 该类的类加载器的实例已被 GC 3. 类加载器 3.1 类加载器介绍 类加载器的主要作用就是加载 Java 类的字节码（ .class 文件）到 JVM 中，在内存中生成一个代表该类的 Class 对象。 类加载器是一个负责加载类的对象，用于实现类加载过程中的加载这一步。 每个 Java 类都有一个引用指向加载它的 ClassLoader。 数组类不是通过 ClassLoader 创建的（数组类没有对应的二进制字节流），是由 JVM 直接生成的。 3.2 类加载器加载规则 JVM 启动的时候，并不会一次性加载所有的类，而是根据需要去动态加载。也就是说，大部分类在具体用到的时候才会去加载，这样对内存更加友好。 对于已经加载的类会被放在 ClassLoader 中。在类加载的时候，系统会首先判断当前类是否被加载过。已经被加载的类会直接返回，否则才会尝试加载。也就是说，对于一个类加载器来说，相同二进制名称的类只会被加载一次。 3.3 类加载器总结 JVM 中内置了三个重要的 ClassLoader： BootstrapClassLoader(启动类加载器)：最顶层的加载类，由 C++实现，通常表示为 null，并且没有父级，主要用来加载 JDK 内部的核心类库（ %JAVA_HOME%/lib目录下的 rt.jar、resources.jar、charsets.jar等 jar 包和类）以及被 -Xbootclasspath参数指定的路径下的所有类。 ExtensionClassLoader(扩展类加载器)：主要负责加载 %JRE_HOME%/lib/ext 目录下的 jar 包和类以及被 java.ext.dirs 系统变量所指定的路径下的所有类。 AppClassLoader(应用程序类加载器)：面向我们用户的加载器，负责加载当前应用 classpath 下的所有 jar 包和类。 除了这三种类加载器之外，用户还可以加入自定义的类加载器来进行拓展，以满足自己的特殊需求。 除了 BootstrapClassLoader 是 JVM 自身的一部分之外，其他所有的类加载器都是在 JVM 外部实现的，并且全都继承自 ClassLoader抽象类。这样做的好处是用户可以自定义类加载器，以便让应用程序自己决定如何去获取所需的类。 每个 ClassLoader 可以通过getParent()获取其父 ClassLoader，如果获取到 ClassLoader 为null的话，那么该类是通过 BootstrapClassLoader 加载的。 4. 双亲委派模型 4.1 双亲委派模型介绍 类加载器有很多种，当我们想要加载一个类的时候，具体是哪个类加载器加载呢？这就需要提到双亲委派模型了。 ClassLoader 类使用委托模型来搜索类和资源。 双亲委派模型要求除了顶层的启动类加载器外，其余的类加载器都应有自己的父类加载器。 ClassLoader 实例会在试图亲自查找类或资源之前，将搜索类或资源的任务委托给其父类加载器。 4.2 双亲委派模型的执行流程 向上委派：如果当前类已经被类加载器加载过（缓存中有），就直接返回；否则委派给父类加载器完成，直到最顶层的BootstrapClassLoader 中。 向下加载：不同的类加载器有不同的加载路径，如果最顶层的路径中没有，就让子加载器去加载，直到回到最初的加载器，如果此时还没有，就返回ClassNotFoundException 异常。 4.3 双亲委派模型的好处 安全性：如果没有双亲委派模型，用户可以自定义一个 java.lang.String 类，并替换 JVM 核心类库中的 String 类，导致安全问题。双亲委派模型确保核心类库（如 java.lang.*）由启动类加载器加载（BootstrapClassLoader），用户无法通过自定义类加载器替换核心类库。 避免类的重复加载：如果没有双亲委派模型，不同的类加载器可能会加载同一个类，导致类的重复加载和内存浪费。双亲委派模型通过委派机制，确保类只会被加载一次。父类加载器加载的类，子类加载器不会重复加载。 4.4 JVM判定两个 Java 类是否相同的具体规则 JVM 不仅要看类的全名是否相同，还要看加载此类的类加载器是否一样。只有两者都相同的情况，才认为两个类是相同的。 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-02-18 14:35:59 "},"Chapter3/MySQL.html":{"url":"Chapter3/MySQL.html","title":"MySQL","keywords":"","body":"MySQL 基础 1. 什么是关系型数据库？ 关系型数据库是一种基于关系模型的数据库，使用表格（即“关系”）来存储和管理数据。 关系模型表明了数据库中所存储的数据之间的联系（一对一、一对多、多对多）。 2. 什么是 SQL？ 结构化查询语言(Structured Query Language)，专门用来与数据库打交道，目的是提供一种从数据库中读写数据的简单有效的方法。 3.什么是 MySQL？ MySQL 是一种关系型数据库，MySQL 的默认端口号是3306。 4. MySQL 有什么优点？ 成熟稳定，功能完善。 开源免费。 社区活跃，生态完善。 事务支持优秀， InnoDB 存储引擎默认使用 REPEATABLE-READ 并不会有任何性能损失，并且，InnoDB 实现的 REPEATABLE-READ 隔离级别其实是可以解决幻读问题发生的。 支持分库分表、读写分离、高可用。 MySQL 字段类型 1. 有哪些字段？ 数值类型：整型（TINYINT、SMALLINT、MEDIUMINT、INT 和 BIGINT）、浮点型（FLOAT 和 DOUBLE）、定点型（DECIMAL） 字符串类型：CHAR、VARCHAR、TINYTEXT、TEXT、MEDIUMTEXT、LONGTEXT、TINYBLOB、BLOB、MEDIUMBLOB 和 LONGBLOB 等，最常用的是 CHAR 和 VARCHAR。 日期时间类型：YEAR、TIME、DATE、DATETIME 和 TIMESTAMP 等。 2. 整数类型的 UNSIGNED 属性有什么用？ 使用 UNSIGNED 属性可以将正整数的上限提高一倍，因为它不需要存储负数值。 3. CHAR 和 VARCHAR 的区别是什么？ CHAR 是定长字符串; CHAR 在存储时会在右边填充空格以达到指定的长度，检索时会去掉空格； CHAR 更适合存储长度较短或者长度都差不多的字符串，例如 Bcrypt 算法、MD5 算法加密后的密码、身份证号码。 VARCHAR 是变长字符串。 VARCHAR 在存储时需要使用 1 或 2 个额外字节记录字符串的长度，检索时不需要处理。 VARCHAR 类型适合存储长度不确定或者差异较大的字符串，例如用户昵称、文章标题等。 4. VARCHAR(100)和 VARCHAR(10)的区别是什么？ VARCHAR(100)和 VARCHAR(10)都是变长类型，表示能存储最多 100 个字符和 10 个字符。 二者存储相同的字符串，所占用磁盘的存储空间其实是一样的。 不过，VARCHAR(100) 会消耗更多的内存。在进行排序的时候，VARCHAR(100)是按照 100 这个长度来进行的，也就会消耗更多内存。 5. DECIMAL 和 FLOAT/DOUBLE 的区别是什么？ DECIMAL 是定点数，FLOAT/DOUBLE 是浮点数。 DECIMAL 可以存储精确的小数值，FLOAT/DOUBLE 只能存储近似的小数值。 MySQL 的 DECIMAL 类型对应的是 Java 类 java.math.BigDecimal 6. 为什么不推荐使用 TEXT 和 BLOB？ 不能有默认值。 检索效率较低。 不能直接创建索引，需要指定前缀长度。 7. DATETIME 和 TIMESTAMP 的区别是什么？ DATETIME 类型没有时区信息，TIMESTAMP 和时区有关。 TIMESTAMP 只需要使用 4 个字节的存储空间，但是 DATETIME 需要耗费 8 个字节的存储空间。 8. NULL 和 '' 的区别是什么？ ''的长度是 0，是不占用空间的，而NULL 是需要占用空间的。 NULL 会影响聚合函数的结果。例如，SUM、AVG、MIN、MAX 等聚合函数会忽略 NULL 值。 查询 NULL 值时，必须使用 IS NULL 或 IS NOT NULLl 来判断，而不能使用 =、!=、 之类的比较运算符。而''是可以使用这些比较运算符的。 NULL 代表一个不确定的值，需要增加判断。 9. Boolean 类型如何表示？ MySQL 中没有专门的布尔类型，而是用 TINYINT(1) 类型来表示布尔值。TINYINT(1) 类型可以存储 0 或 1，分别对应 false 或 true。 MySQL 基础架构 1. 结构 连接器： 身份认证和权限相关(登录 MySQL 的时候)。 查询缓存： 执行查询语句的时候，会先查询缓存（MySQL 8.0 版本后移除，因为这个功能不太实用）。 分析器： 没有命中缓存的话，SQL 语句就会经过分析器，分析器说白了就是要先看你的 SQL 语句要干嘛，再检查你的 SQL 语句语法是否正确。 优化器： 按照 MySQL 认为最优的方案去执行。 执行器： 执行语句，然后从存储引擎返回数据。 执行语句之前会先判断是否有权限，如果没有权限的话，就会报错。 插件式存储引擎：主要负责数据的存储和读取，采用的是插件式架构，支持 InnoDB、MyISAM、Memory 等多种存储引擎。InnoDB 是 MySQL 的默认存储引擎，绝大部分场景使用 InnoDB 就是最好的选择。 2. SQL语句在MySQL中的执行过程 查询语句的执行流程如下：权限校验（如果命中缓存）--->查询缓存--->分析器--->优化器--->权限校验--->执行器--->引擎 更新语句执行流程如下：分析器---->权限校验---->执行器--->引擎---redo log(prepare 状态)--->binlog--->redo log(commit 状态) MySQL 存储引擎 1. MySQL 支持哪些存储引擎？默认使用哪个？ 默认是InnoDB。MySQL 5.5.5 之前，MyISAM 是 MySQL 的默认存储引擎。5.5.5 版本之后，InnoDB 是 MySQL 的默认存储引擎。 InnoDB ，支持事务，支持外键，支持行级锁。 2. MySQL 存储引擎架构了解吗？ MySQL 存储引擎采用的是 插件式架构 ，支持多种存储引擎，存储引擎是基于表的，而不是数据库。 3. MyISAM 和 InnoDB 有什么区别？如何选择？ InnoDB 支持行级别的锁粒度，MyISAM 不支持，只支持表级别的锁粒度。 MyISAM 不提供事务支持。InnoDB 提供事务支持，实现了 SQL 标准定义了四个隔离级别。 MyISAM 不支持外键，而 InnoDB 支持。 MyISAM 不支持 MVCC，而 InnoDB 支持。 虽然 MyISAM 引擎和 InnoDB 引擎都是使用 B+Tree 作为索引结构，但是两者的实现方式不太一样。 MyISAM 不支持数据库异常崩溃后的安全恢复，而 InnoDB 支持。 InnoDB 的性能比 MyISAM 更强大。 读密集的情况下，使用 MyISAM 也是合适的。对于咱们日常开发的业务系统来说，你几乎找不到什么理由使用 MyISAM 了，老老实实用默认的 InnoDB 就可以了！ MySQL 索引 1. 索引介绍 索引是一种用于快速查询和检索数据的数据结构，其本质可以看成是一种排序好的数据结构。 常见的索引结构有: B 树， B+树 和 Hash、红黑树。在 MySQL 中，无论是 Innodb 还是 MyIsam，都使用了 B+树作为索引结构。 2. 索引的优缺点 优点： 使用索引可以大大加快数据的检索速度（大大减少检索的数据量）, 减少 IO 次数，这也是创建索引的最主要的原因。 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。 缺点： 创建索引和维护索引需要耗费许多时间。当对表中的数据进行增删改的时候，如果数据有索引，那么索引也需要动态的修改，会降低 SQL 执行效率。 索引需要使用物理文件存储，也会耗费一定空间。 大多数情况下，索引查询都是比全表扫描要快的。但是如果数据库的数据量不大，那么使用索引也不一定能够带来很大提升。 百万行以上的数据，索引可以带来显著的性能提示。 3. 索引底层数据结构选型 见MySQL进阶篇。 3.1 B+和B-树区别？ 多路平衡查找树 B+Tree，所有的数据都会出现在叶子节点，非叶子节点仅仅起到索引数据作用。 B+Tree，叶子节点形成一个单向链表。 MySQL优化后的形成了双向循环链表。 3.2 为什么MySQL选用B+，不是B-、二叉树、哈希？ 相对于二叉树，层级更少，搜索效率高； 对于B-tree，无论是叶子节点还是非叶子节点，都会保存数据，这样导致一页中存储的键值减少，指针跟着减少，要同样保存大量数据，只能增加树的高度，导致性能降低； 相对Hash索引，B+tree支持范围匹配及排序操作； 4. 索引分类 4.1 按照类型 唯一索引的主要目的不是为了加快查询，而是为了不重复。 4.2 按照底层存储方式角度 如果存在主键，主键索引就是聚集索引。 如果不存在主键，将使用第一个唯一（UNIQUE）索引作为聚集索引。 如果表没有主键，或没有合适的唯一索引，则InnoDB会自动生成一个rowid作为隐藏的聚集索引。 回表查询： 这种先到二级索引中查找数据，找到主键值，然后再到聚集索引中根据主键值，获取数据的方式，就称之为回表查询。 4.3 覆盖索引 如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称之为 覆盖索引（Covering Index） 。 覆盖索引即需要查询的字段正好是索引的字段，那么直接根据该索引，就可以查到数据了，而无需回表查询。 4.4 前缀索引 当字段类型为字符串（varchar，text，longtext等）时，有时候需要索引很长的字符串，这会让索引变得很大，查询时，浪费大量的磁盘IO， 影响查询效率。此时可以只将字符串的一部分前缀，建立索引，这样可以大大节约索引空间，从而提高索引效率 4.5 联合索引 使用表中的多个字段创建索引，就是 联合索引，也叫 组合索引 或 复合索引。 5. 最左前缀法则 如果索引了多列（联合索引），最左前缀法则指的是查询从索引的最左列开始，并且不跳过索引中的列。如果跳跃某一列，索引将会部分失效(后面的字段索引失效)。 最左匹配原则会一直向右匹配，直到遇到范围查询（如 >、。对于 >=、 最左前缀法则中指的最左边的列，是指在查询时，联合索引的最左边的字段(即是第一个字段)必须存在，与我们编写SQL时，条件编写的先后顺序无关。 6. 索引失效 不要在索引列上进行运算操作， 索引将失效。 字符串不加引号：如果字符串不加单引号，对于查询结果，没什么影响，但是数据库存在隐式类型转换，索引将失效 模糊查询：在like模糊查询中，在关键字后面加%，索引可以生效。而如果在关键字前面加了%，索引将会失效。 or连接条件：如果or前的条件中的列有索引，而后面的列中没有索引，那么涉及的索引都不会被用到。 ​ 如果MySQL评估使用索引比全表更慢，则不使用索引。 7. 正确使用索引的一些建议 不为 NULL 的字段：索引字段的数据应该尽量不为 NULL，因为对于数据为 NULL 的字段，数据库较难优化。如果字段频繁被查询，但又避免不了为 NULL，建议使用 0,1,true,false 这样语义较为清晰的短值或短字符作为替代。 被频繁查询的字段：我们创建索引的字段应该是查询操作非常频繁的字段。 被作为条件查询的字段：被作为 WHERE 条件查询的字段，应该被考虑建立索引。 频繁需要排序的字段：索引已经排序，这样查询可以利用索引的排序，加快排序查询时间。 选择区分度高的列作为索引，尽量建立唯一索引，区分度越高，使用索引的效率越高。 尽量使用联合索引，减少单列索引，查询时，联合索引很多时候可以覆盖索引，节省存储空间，避免回表，提高查询效率。 被频繁更新的字段应该慎重建立索引 限制每张表上的索引数量 被经常频繁用于连接的字段：经常用于连接的字段可能是一些外键列，对于外键列并不一定要建立外键，只是说该列涉及到表与表的关系。对于频繁被连接查询的字段，可以考虑建立索引，提高多表连接查询的效率。 日志 1. redo log 重做日志缓冲（redo log buffer）以及重做日志文件（redo logfile）,前者是在内存中，后者在磁盘中。 当事务提交之后会把所有修改信息都存到该日志文件中, 用于在刷新脏页到磁盘,发生错误时, 进行数据恢复使用。 redo log（重做日志）让 InnoDB 存储引擎拥有了崩溃恢复能力。 我们知道，在InnoDB引擎中的内存结构中，主要的内存区域就是缓冲池，在缓冲池中缓存了很多的数据页。 当我们在一个事务中，执行多个增删改的操作时，InnoDB引擎会先操作缓冲池中的数据，如果缓冲区没有对应的数据，会通过后台线程将磁盘中的数据加载出来，存放在缓冲区中，然后将缓冲池中的数据修改，修改后的数据页我们称为脏页。 而脏页则会在一定的时机，通过后台线程刷新到磁盘 中，从而保证缓冲区与磁盘的数据一致。 而缓冲区的脏页数据并不是实时刷新的，而是一段时间之后将缓冲区的数据刷新到磁盘中，假如刷新到磁盘的过程出错了，而提示给用户事务提交成功，而数据却没有持久化下来，这就出现问题了，没有保证事务的持久性。 有了redolog之后，当对缓冲区的数据进行增删改之后，会首先将操作的数据页的变化，记录在redolog buffer中。 在事务提交时，会将redo log buffer中的数据刷新到redo log磁盘文件中。 过一段时间之后，如果刷新缓冲区的脏页到磁盘时，发生错误，此时就可以借助于redo log进行数据恢复，这样就保证了事务的持久性。 而如果脏页成功刷新到磁盘 或 或者涉及到的数据已经落盘，此时redolog就没有作用了，就可以删除了，所以存在的两个redolog文件是循环写的。 那为什么每一次提交事务，要刷新redo log 到磁盘中呢，而不是直接将buffer pool中的脏页刷新到磁盘呢 ? 我们操作数据一般都是随机读写磁盘的，而不是顺序读写磁盘。 而redo log在往磁盘文件中写入数据，由于是日志文件，所以都是顺序写的。顺序写的效率，要远大于随机写。 2. undo log 回滚日志，用于记录数据被修改前的信息 , 作用包含两个 : 提供回滚(保证事务的原子性) 和MVCC(多版本并发控制) 。 undo log和redo log记录物理日志不一样，它是逻辑日志。可以认为当delete一条记录时，undolog中会记录一条对应的insert记录，反之亦然，当update一条记录时，它记录一条对应相反的update记录。当执行rollback时，就可以从undo log中的逻辑记录读取到相应的内容并进行回滚。 3. binlog 4. 总结 InnoDB 引擎使用 redo log(重做日志) 保证事务的持久性。 undo log(回滚日志) 来保证事务的原子性。 锁和MVCC保证了隔离性。 MVCC 1. 当前读、快照读、MVCC 当前读：读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁。对于我们日常的操作，如：select ... lock in share mode(共享锁)，select ...for update、update、insert、delete(排他锁)都是一种当前读。 快照读：简单的select（不加锁）就是快照读，快照读，读取的是记录数据的可见版本，有可能是历史数据，不加锁，是非阻塞读。 在测试中,我们看到即使事务B提交了数据,事务A中也查询不到。 原因就是因为普通的select是快照读，而在当前默认的RR隔离级别下，开启事务后第一个select语句才是快照读的地方，后面执行相同的select语句都是从快照中获取数据，可能不是当前的最新数据，这样也就保证了可重复读。 MVCC：Multi-Version Concurrency Control，多版本并发控制指维护一个数据的多个版本，使得读写操作没有冲突。 依赖于： 数据库记录中的三个隐式字段 undo log日志； readView。 2. 三个隐藏字段 3. undoLog 回滚日志，在insert、update、delete的时候产生的便于数据回滚的日志。 当insert的时候，产生的undo log日志只在回滚时需要，在事务提交后，可被立即删除。 而update、delete的时候，产生的undo log日志不仅在回滚时需要，在快照读时也需要，不会立即被删除。 4. readview ReadView（读视图）是 快照读 SQL执行时MVCC提取数据的依据，记录并维护系统当前活跃的事务（未提交的）id。 5. RC 和 RR 隔离级别下 MVCC 的差异 在 RC（读已提交） 隔离级别下的 每次select 查询前都生成一个Read View (m_ids 列表) 在 RR （可重复读）隔离级别下只在事务开始后 第一次select 数据前生成一个Read View（m_ids 列表） 在 RC 隔离级别下，事务在每次查询开始时都会生成并设置新的 Read View，所以导致不可重复读。 RR 隔离级别只会在事务开启后的第一次查询生成 Read View ，并使用至事务提交。所以在生成 Read View 之后其它事务所做的更新、插入记录版本对当前事务并不可见，实现了可重复读和防止快照读下的 “幻读”。 6. MVCC➕Next-key-Lock 防止幻读 mysql的RR是可以防止幻读的，具体分为是快照读还是当前读。 1、执行普通 select，此时会以 MVCC 快照读的方式读取数据 在快照读的情况下，RR 隔离级别只会在事务开启后的第一次查询生成 Read View ，并使用至事务提交。所以在生成 Read View 之后其它事务所做的更新、插入记录版本对当前事务并不可见，实现了可重复读和防止快照读下的 “幻读” 2、执行 select...for update/lock in share mode、insert、update、delete 等当前读 在当前读下，读取的都是最新的数据，如果其它事务有插入新的记录，并且刚好在当前事务查询范围内，就会产生幻读！InnoDB 使用 Next-key Lock 来防止这种情况。当执行当前读时，会锁定读取到的记录的同时，锁定它们的间隙，防止其它事务在查询范围内插入数据。只要我不让你插入，就不会发生幻读 事务 1. 何谓事务？ACID四个特性？ 事务是逻辑上的一组操作，要么都执行，要么都不执行。 原子性（Atomicity）：事务是不可分割的最小操作单元，要么全部成功，要么全部失败。 一致性（Consistency）：事务完成时，数据都保持一致。例如转账业务中，无论事务是否成功，转账者和收款人的总额应该是不变的； 隔离性（Isolation）：数据库系统提供的隔离机制，保证事务在不受外部并发操作影响的独立环境下运行。 持久性（Durability）：事务一旦提交或回滚，它对数据库中的数据的改变就是永久的，即使数据库发生故障也不应该对其有任何影响。 只有保证了事务的持久性、原子性、隔离性之后，一致性才能得到保障，也就是说 A、I、D 是手段，C 是目的！ 2. 四个特性有由什么来保持？ 3. 并发事务带来了哪些问题? 脏读：一个事务读取了另一个未提交事务的中间数据，如果未提交事务回滚，读取的数据将无效，导致不一致。 不可重复读：同一事务内多次读取同一数据，结果不一致，原因是：其他事务在读取间隙修改了数据并提交。 幻读：同一事务内多次查询，结果集不一致（多了或少了数据），他事务在查询间隙插入或删除了数据。 幻读其实可以看作是不可重复读的一种特殊情况。 4. 并发事务的控制方式有哪些？ MySQL 中并发事务的控制方式无非就两种：锁 和 MVCC。 锁可以看作是悲观控制的模式，多版本并发控制（MVCC，Multiversion concurrency control）可以看作是乐观控制的模式。 锁： 共享锁（S 锁）：又称读锁，事务在读取记录的时候获取共享锁，允许多个事务同时获取（锁兼容）。 排他锁（X 锁）：又称写锁/独占锁，事务在修改记录的时候获取排他锁，不允许多个事务同时获取。如果一个记录已经被加了排他锁，那其他事务不能再对这条记录加任何类型的锁（锁不兼容）。 MVCC： MVCC 是多版本并发控制方法，即对一份数据会存储多个版本，通过事务的可见性来保证事务能看到自己应该看到的版本。通常会有一个全局的版本分配器来为每一行数据设置版本号，版本号是唯一的。 5. SQL 标准定义了哪些事务隔离级别? READ-UNCOMMITTED(读取未提交) ：最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 READ-COMMITTED(读取已提交) ：允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 REPEATABLE-READ(可重复读) ：对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE(可串行化) ：最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。 默认的是可重复读。 从上面对 SQL 标准定义了四个隔离级别的介绍可以看出，标准的 SQL 隔离级别定义里，REPEATABLE-READ(可重复读)是不可以防止幻读的。但是！InnoDB 实现的 REPEATABLE-READ 隔离级别其实是可以解决幻读问题发生的，主要有下面两种情况： 快照读：由 MVCC 机制来保证不出现幻读。 当前读：使用 Next-Key Lock 进行加锁来保证不出现幻读，Next-Key Lock 是行锁（Record Lock）和间隙锁（Gap Lock）的结合，行锁只能锁住已经存在的行，为了避免插入新行，需要依赖间隙锁。 6. MySQL 的隔离级别是基于锁实现的吗？ 基于锁和 MVCC 机制共同实现的。 SERIALIZABLE 隔离级别是通过锁来实现的，READ-COMMITTED 和 REPEATABLE-READ 隔离级别是基于 MVCC 实现的。不过， SERIALIZABLE 之外的其他隔离级别可能也需要用到锁机制，就比如 REPEATABLE-READ 在当前读情况下需要使用加锁读来保证不会出现幻读。 锁 1. 表级锁和行级锁了解吗？有什么区别？ 表级锁： MySQL 中锁定粒度最大的一种锁（全局锁除外），是针对非索引字段加的锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。不过，触发锁冲突的概率最高，高并发下效率极低。表级锁和存储引擎无关，MyISAM 和 InnoDB 引擎都支持表级锁。 行级锁： MySQL 中锁定粒度最小的一种锁，是 针对索引字段加的锁 ，只针对当前操作的行记录进行加锁。 行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。行级锁和存储引擎有关，是在存储引擎层面实现的 2. 行级锁的使用有什么注意事项？ 在 InnoDB 默认的隔离级别 REPEATABLE-READ 下，行锁默认使用的是 Next-Key Lock。 行级锁只会对查询用到的索引加锁，如果查询未使用索引或者索引失效的话（where中），MySQL 会退化为表锁。 如果你查询的条件中用到了 唯一索引或主键索引，那么 MySQL 会精准地锁定符合条件的行，使用记录锁，特别的如果是给不存在的记录加锁（where条件中写：=不存在的数据），使用间隙锁。 如果使用的是 普通索引 或 非唯一索引，则可能锁定范围更大，甚至包括多个不相关的行，使用临键锁。 3. InnoDB 有哪几类行锁？ 在 InnoDB 默认的隔离级别 REPEATABLE-READ 下，行锁默认使用的是 Next-Key Lock。 记录锁（Record Lock）：属于单个行记录上的锁。 间隙锁（Gap Lock）：锁定一个范围，不包括记录本身，可以防止在这个间隙插入数据，产生幻读。 临键锁（Next-Key Lock）：Record Lock+Gap Lock，锁定一个范围，包含记录本身，主要目的是为了解决幻读问题（MySQL 事务部分提到过） 4. 共享锁和排他锁呢？ 不论是表级锁还是行级锁，都存在共享锁（Share Lock，S 锁）和排他锁（Exclusive Lock，X 锁）这两类： 共享锁（S 锁）：又称读锁，事务在读取记录的时候获取共享锁，允许多个事务同时获取（锁兼容）。 排他锁（X 锁）：又称写锁/独占锁，事务在修改记录的时候获取排他锁，不允许多个事务同时获取。如果一个记录已经被加了排他锁，那其他事务不能再对这条事务加任何类型的锁（锁不兼容）。 排他锁与任何的锁都不兼容，共享锁仅和共享锁兼容。 5. 意向锁有什么作用？ 如果需要用到表锁的话，如何判断表中的记录没有行锁呢，一行一行遍历肯定是不行，性能太差。我们需要用到一个叫做意向锁的东东来快速判断是否可以对某个表使用表锁。 意向锁是由数据引擎自己维护的，用户无法手动操作意向锁，在执行DML操作时，会对涉及的行加行锁，同时也会对该表加上意向锁。而其他客户端，在对这张表加表锁的时候，会根据该表上所加的意向锁来判定是否可以成功加表锁，而不用逐行判断行锁情况了 6. 当前读和快照读有什么区别？ 快照读（一致性非锁定读）就是单纯的 SELECT 语句，但不包括下面这两类 SELECT 语句： SELECT ... FOR UPDATE # 共享锁 可以在 MySQL 5.7 和 MySQL 8.0 中使用 SELECT ... LOCK IN SHARE MODE; # 共享锁 可以在 MySQL 8.0 中使用 SELECT ... FOR SHARE; 快照读的情况下，如果读取的记录正在执行 UPDATE/DELETE 操作，读取操作不会因此去等待记录上 X 锁的释放，而是会去读取行的一个快照（历史版本）。 快照读比较适合对于数据一致性要求不是特别高且追求极致性能的业务场景。 当前读 （一致性锁定读）就是给行记录加 X 锁或 S 锁。 # 对读的记录加一个X锁 SELECT...FOR UPDATE # 对读的记录加一个S锁 SELECT...LOCK IN SHARE MODE # 对读的记录加一个S锁 SELECT...FOR SHARE # 对修改的记录加一个X锁 INSERT... UPDATE... DELETE... 三大日志 InnoDB 见MySQL高级篇。 redo log 它是物理日志，记录内容是“在某个数据页上做了什么修改”，属于 InnoDB 存储引擎。 刷盘时机： 事务提交：当事务提交时，log buffer 里的 redo log 会被刷新到磁盘（可以通过innodb_flush_log_at_trx_commit参数控制，后文会提到）。 log buffer 空间不足时：log buffer 中缓存的 redo log 已经占满了 log buffer 总容量的大约一半左右，就需要把这些日志刷新到磁盘上。 正常关闭服务器：MySQL 关闭的时候，redo log 都会刷入到磁盘里去。 InnoDB 存储引擎有一个后台线程，每隔1 秒，就会把 redo log buffer 中的内容写到文件系统缓存（page cache），然后调用 fsync 刷盘。 刷盘策略： 0：设置为 0 的时候，表示每次事务提交时不进行刷盘操作。这种方式性能最高，但是也最不安全，因为如果 MySQL 挂了或宕机了，可能会丢失最近 1 秒内的事务。 1：默认设置，设置为 1 的时候，表示每次事务提交时都将进行刷盘操作。这种方式性能最低，但是也最安全，因为只要事务提交成功，redo log 记录就一定在磁盘里，不会有任何数据丢失。 2：设置为 2 的时候，表示每次事务提交时都只把 log buffer 里的 redo log 内容写入 page cache（文件系统缓存）。page cache 是专门用来缓存文件的，这里被缓存的文件就是 redo log 文件。这种方式的性能和安全性都介于前两者中间。 刷盘策略innodb_flush_log_at_trx_commit 的默认值为 1，设置为 1 的时候才不会丢失任何数据。为了保证事务的持久性，我们必须将其设置为 1。也就是说，一个没有提交事务的 redo log 记录，也可能会刷盘。 binlog 而 binlog 是逻辑日志，记录内容是语句的原始逻辑，类似于“给 ID=2 这一行的 c 字段加 1”，属于MySQL Server 层。 不管用什么存储引擎，只要发生了表数据更新，都会产生 binlog 日志。 binlog 会记录所有涉及更新数据的逻辑操作，并且是顺序写。 用于 主从复制（Replication） 和 时间点恢复（Point-in-Time Recovery）。 两阶段提交 redo log（重做日志）让 InnoDB 存储引擎拥有了崩溃恢复能力。 binlog（归档日志）保证了 MySQL 集群架构的数据一致性。 redo log 在事务执行过程中可以不断写入，而 binlog 只有在提交事务时才写入，所以 redo log 与 binlog 的写入时机不一样。 我们以update语句为例，假设id=2的记录，字段c值是0，把字段c值更新成1，SQL语句为update T set c=1 where id=2。 假设执行过程中写完 redo log 日志后，binlog 日志写期间发生了异常，会出现什么情况呢？ 由于 binlog 没写完就异常，这时候 binlog 里面没有对应的修改记录。因此，之后用 binlog 日志恢复数据时，就会少这一次更新，恢复出来的这一行c值是0，而原库因为 redo log 日志恢复，这一行c值是1，最终数据不一致。 为了解决两份日志之间的逻辑一致问题，InnoDB 存储引擎使用两阶段提交方案。 原理很简单，将 redo log 的写入拆成了两个步骤prepare和commit，这就是两阶段提交。 使用两阶段提交后，写入 binlog 时发生异常也不会有影响，因为 MySQL 根据 redo log 日志恢复数据时，发现 redo log 还处于prepare阶段，并且没有对应 binlog 日志，就会回滚该事务。 核心逻辑。 undo log undo log 属于逻辑日志，记录的是 SQL 语句，比如说事务执行一条 DELETE 语句，那 undo log 就会记录一条相对应的 INSERT 语句。 保证了原子性、还有MVCC需要用。 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-02-11 21:51:56 "},"Chapter3/RabbitMQ.html":{"url":"Chapter3/RabbitMQ.html","title":"RabbitMQ","keywords":"","body":"RabbitMQ中的AMQP AMQP，Advanced Message Queuing Protocol，高级消息队列协议. 它是一个提供统一消息服务的应用层二进制协议，为面向消息的中间件设计。它是基于TCP/IP协议构造的协议 基于此协议的客户端与消息中间件可传递消息，可跨平台传输。 为什么要用RabbitMQ？有什么好处？场景是什么？ 结合黑马点评来说。 异步处理：对于一些不需要立即生效的操作，可以拆分出来，异步执行，使用消息队列实现，性能高。 流量消峰：在高并发场景下，消息队列可以作为一个缓冲区，暂时存储大量请求，避免系统因瞬时高流量而崩溃。比如秒杀中的订单。 应用解耦：消息队列允许生产者和消费者之间通过消息进行通信，而不需要直接调用对方的接口。 RabbitMQ 中有哪些重要的角色？ 生产者：消息的创建者，负责创建和推送数据到消息服务器； 消费者：消息的接收方，用于处理数据和确认消息； 代理：就是 RabbitMQ 消息队列本身，用于扮演“快递”的角色，本身不生产消息，只是扮演“快递”的角色。 RabbitMQ对应的架构如图： 其中包含几个概念： publisher：生产者，也就是发送消息的一方 consumer：消费者，也就是消费消息的一方 queue：队列，存储消息。生产者投递的消息会暂存在消息队列中，等待消费者处理 exchange：交换机，负责消息路由。生产者发送的消息由交换机决定投递到哪个队列。 virtual host：虚拟主机，起到数据隔离的作用。每个虚拟主机相互独立，有各自的exchange、queue 还有其他什么消息队列？为什么使用RabbitMQ，优缺点？ 还有RocketMQ，Kafka 几种常见MQ的对比： RabbitMQ ActiveMQ RocketMQ Kafka 公司/社区 Rabbit Apache 阿里 Apache 开发语言 Erlang Java Java Scala&Java 协议支持 AMQP，XMPP，SMTP，STOMP OpenWire,STOMP，REST,XMPP,AMQP 自定义协议 自定义协议 可用性 高 一般 高 高 单机吞吐量 一般 差 高 非常高 消息延迟 微秒级 毫秒级 毫秒级 毫秒以内 消息可靠性 高 一般 高 一般 RabbitMQ可靠性比较高：生产者确认、消费者确认、消息持久化。 灵活的路由机制：支持多种交换机类型，可以满足复杂的消息路由需求。 RabbitMQ 拥有友好的管理界面和丰富的文档资源，易于上手和维护。 缺点： 使用erlang实现，不利于二次开发和维护； 在高并发场景下，单机性能可能不如 Kafka ， 交换机的类型 Fanout：广播，将消息交给所有绑定到交换机的队列。我们最早在控制台使用的正是Fanout交换机 Direct：订阅，基于RoutingKey（路由key）发送给订阅了消息的队列 Topic：通配符订阅，与Direct类似，只不过RoutingKey可以使用通配符 headers：不依赖于routing key与binding key的匹配规则，而是根据发送消息内容中的headers属性进行匹配；除此之外 headers 交换器和 direct 交换器完全一致，但性能差很多（目前几乎用不到了） 消息基于什么传输？为什么使用信道？ RabbitMQ 使用信道的方式来传输数据。 由于TCP连接的创建和销毁开销较大。 TCP并发数受系统资源限制，会造成性能瓶颈。 信道是建立在真实的TCP连接内的虚拟连接，且每条TCP连接上的信道数量没有限制 消息属性和有效载荷(消息主体) Content type: 内容类型 Content encoding: 内容编码 Routing Key: 路由键 Delivery mode: 投递方式(持久化 or 非持久化) Message priority: 消息优先权 Message publishing timestamp: 消息发布的时间戳 Expiration period: 消息的有效期 Publisher application id: 发布应用的id 怎么设置消息的过期时间？ 在生产端发送消息时，给消息设置过期时间，单位毫秒(ms) // 设置\"tyson\"消息时间为3000毫秒 Message msg = new Message(\"tyson\".getBytes(), mp); msg.getMessageProperties().setExpiration(\"3000\"); 在消息队列创建队列时，指定队列的TTL，从消息入队列开始计算，超过该时间的消息将会被移除。 如何确保消息不丢失？ 生产者连接重试 就是生产者发送消息时，出现了网络故障，导致与MQ的连接中断。 为了解决这个问题，SpringAMQP提供的消息发送时的重试机制。即：当RabbitTemplate与MQ连接超时后，多次重试。 spring: rabbitmq: connection-timeout: 1s # 设置MQ的连接超时时间 template: retry: enabled: true # 开启超时重试机制 initial-interval: 1000ms # 失败后的初始等待时间 multiplier: 1 # 失败后下次的等待时长倍数，下次等待时长 = initial-interval * multiplier max-attempts: 3 # 最大重试次数 不过SpringAMQP提供的重试机制是阻塞式的重试，也就是说多次重试等待的过程中，当前线程是被阻塞的。 如果对于业务性能有要求，建议禁用重试机制。如果一定要使用，请合理配置等待时长和重试次数。 生产者确认 一般情况下，只要生产者与MQ之间的网路连接顺畅，基本不会出现发送消息丢失的情况，因此大多数情况下我们无需考虑这种问题。 开启生产者确认比较消耗MQ性能，一般不建议开启。而且大多都由编程错误导致（RoutingKey填错了，名称写错了，忘记bind） 默认两种机制都是关闭状态，需要通过配置文件来开启。 Publisher Return：当消息投递到MQ，但是路由失败时，通过Publisher Return返回异常信息，同时返回ack的确认信息，代表投递成功 Publisher Confirm： 临时消息投递到了MQ，并且入队成功，返回ACK，告知投递成功 持久消息投递到了MQ，并且入队完成持久化，返回ACK ，告知投递成功 其它情况都会返回NACK，告知投递失败 MQ的可靠性（持久化） RabbitMQ 提供了持久化的机制，将内存中的消息持久化到硬盘上，即使重启RabbitMQ，消息也不会丢失。 为了保证数据的可靠性，必须配置数据持久化，包括：交换机持久化、队列持久化、消息持久化 当发布一条消息到交换机上时，RabbitMQ 会先把消息写入持久化日志文件，然后才向生产者发送响应。 一旦消费者从持久队列中消费了一条持久化消息并且做了确认，RabbitMQ会在持久化日志中把这条消息标记为等待垃圾收集，从而移除这条消息。 如果持久化消息在被消费之前RabbitMQ重启，服务器会自动重建交换机和队列（以及绑定），并重新加载持久化日志中的消息到相应的队列或者交换机上，保证消息不会丢失。 消费者确认机制 即：当消费者处理消息结束后，应该向RabbitMQ发送一个回执，告知RabbitMQ自己消息处理状态。回执有三种可选值： ack：成功处理消息，RabbitMQ从队列中删除该消息 nack：消息处理失败，RabbitMQ需要再次投递消息 reject：消息处理失败并拒绝该消息，RabbitMQ从队列中删除该消息 一般reject方式用的较少，除非是消息格式有问题，那就是开发问题了。因此大多数情况下我们需要将消息处理的代码通过try catch机制捕获，消息处理成功时返回ack，处理失败时返回nack. SpringAMQP帮我们实现了消息确认。并允许我们通过配置文件设置ACK处理方式，有三种模式： **none**：不处理。即消息投递给消费者后立刻ack，消息会立刻从MQ删除。非常不安全，不建议使用 **manual**：手动模式。需要自己在业务代码中调用api，发送ack或reject，存在业务入侵，但更灵活 **auto**：自动模式。SpringAMQP利用AOP对我们的消息处理逻辑做了环绕增强，当业务正常执行时则自动返回ack. 当业务出现异常时，根据异常判断返回不同结果： 如果是业务异常，会自动返回nack； 如果是消息处理或校验异常，自动返回reject; 消费者重试 当消费者出现异常后，消息会不断requeue（重入队）到队列，再重新发送给消费者。 如果消费者再次执行依然出错，消息会再次requeue到队列，再次投递，直到消息处理成功为止。 极端情况就是消费者一直无法执行成功，那么消息requeue就会无限循环，导致mq的消息处理飙升，带来不必要的压力： Spring又提供了消费者失败重试机制：在消费者出现异常时利用本地重试，而不是无限制的requeue到mq队列。 消费者在失败后消息没有重新回到MQ无限重新投递，而是在本地重试了3次 本地重试3次以后，抛出了AmqpRejectAndDontRequeueException异常。查看RabbitMQ控制台，发现消息被删除了，说明最后SpringAMQP返回的是reject 但是这在某些对于消息可靠性要求较高的业务场景下，显然不太合适了。 因此Spring允许我们自定义重试次数耗尽后的消息处理策略，这个策略是由MessageRecovery接口来定义的，它有3个不同实现： RejectAndDontRequeueRecoverer：重试耗尽后，直接reject，丢弃消息。默认就是这种方式 ImmediateRequeueMessageRecoverer：重试耗尽后，返回nack，消息重新入队 RepublishMessageRecoverer：重试耗尽后，将失败消息投递到指定的交换机 比较优雅的一种处理方案是RepublishMessageRecoverer，失败后将消息投递到一个指定的，专门存放异常消息的队列，后续由人工集中处理。 总结 首先，支付服务会正在用户支付成功以后利用MQ消息通知交易服务，完成订单状态同步。 其次，为了保证MQ消息的可靠性，我们采用了生产者确认机制、消费者确认、消费者失败重试等策略，确保消息投递的可靠性 最后，我们还在交易服务设置了定时任务，定期查询订单支付状态。这样即便MQ通知失败，还可以利用定时任务作为兜底方案，确保订单支付状态的最终一致性。 如何解决消息积压的问题 要么是发送变快了。要么是消费变慢了。 可以通过 扩容消费端的实例数来提升总体的消费能力。 如果短时间内没有足够的服务器资源进行扩容，那么就将系统降级，通过关闭一些不重要的业务，减少发送方发送的数据量，最低限度让系统还能正常运转，服务一些重要业务。 如何处理消息堆积情况?几千万条数据在MQ里积压了七八个小时 一般这个时候，只能操作临时紧急扩容了，具体操作步骤和思路如下： 然后写一个临时的分发数据的consumer程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的10倍数量的queue。 临时将queue资源和consumer资源扩大10倍，以正常的10倍速度来消费数据。 等快速消费完积压数据之后，得恢复原先部署架构，重新用原先的consumer机器来消费消息 由于消息积压导致过期被清理了怎么办 不是说数据会大量积压在mq里，而是大量的数据会直接搞丢。 等过了高峰期以后，这个时候我们就开始写程序，将丢失的那批数据，写个临时程序，一点一点的查出来，然后重新灌入mq里面去，把白天丢的数据给他补回来。 假设1万个订单积压在mq里面，没有处理，其中1000个订单都丢了，你只能手动写程序把那1000个订单给查出来，手动发到mq里去再补一次。 消费端怎么进行限流？ 当 RabbitMQ 服务器积压大量消息时，队列里的消息会大量涌入消费端，可能导致消费端服务器崩溃。这种情况下需要对消费端限流。 Spring RabbitMQ 提供参数 prefetch 参数指定了消费者在处理完当前消息之前，可以从队列中预取的消息数量。 例如，如果 prefetch=10，消费者会一次性从队列中拉取 10 条消息到本地缓存，然后逐条处理。 如果消费者同时处理的消息到达最大值的时候，则该消费者会阻塞，不会消费新的消息，直到有消息 ack 才会消费新的消息。 默认是不设置prefetch，RabbitMQ 默认会尽可能多地将消息推送给消费者（不是轮寻），可能会导致以下问题：如果某个消费者处理速度较慢，它可能会堆积大量未处理的消息，而其他消费者可能处于空闲状态。 重复消费（业务幂等性） 在程序开发中，则是指同一个业务，执行一次或多次对业务状态的影响是一致的。 查询和删除是幂等的，但数据的更新往往不是幂等的，如果重复执行可能造成不一样的后果。 取消订单，恢复库存的业务。如果多次恢复就会出现库存重复增加的情况 退款业务。重复退款对商家而言会有经济损失。 然而在实际业务场景中，由于意外经常会出现业务被重复执行的情况，例如： 假如用户刚刚支付完成，并且投递消息到交易服务，交易服务更改订单为已支付状态。 由于某种原因，例如网络故障导致生产者没有得到确认，隔了一段时间后重新投递给交易服务。 但是，在新投递的消息被消费之前，用户选择了退款，将订单状态改为了已退款状态。 退款完成后，新投递的消息才被消费，那么订单状态会被再次改为已支付。业务异常。 唯一消息ID解决： 每一条消息都生成一个唯一的id，与消息一起投递给消费者。 消费者接收到消息后处理自己的业务，业务处理成功后将消息ID保存到数据库 如果下次又收到相同消息，去数据库查询判断是否存在，存在则为重复消息放弃处理。 @Bean public MessageConverter messageConverter(){ // 1.定义消息转换器 Jackson2JsonMessageConverter jjmc = new Jackson2JsonMessageConverter(); // 2.配置自动创建消息id，用于识别不同消息，也可以在业务中基于ID判断是否是重复消息 jjmc.setCreateMessageIds(true); return jjmc; } 业务判断： 例如我们当前案例中，处理消息的业务逻辑是把订单状态从未支付修改为已支付。 因此我们就可以在执行业务时判断订单状态是否是未支付，如果不是则证明订单已经被处理过，无需重复处理。 相比较而言，消息ID的方案需要改造原有的数据库，所以我更推荐使用业务判断的方案。 比如修改订单状态的时候，只有原来是未支付，我才修改为已支付。 如何保证消息的有序性？ RabbitMQ 本身并不完全保证消息的全局有序性。 只能通过合理的配置和设计来实现消息的有序性。 单队列单消费者：如果只有一个队列和一个消费者，RabbitMQ 会严格按照消息进入队列的顺序将消息传递给消费者。 重试机制：比如有一个微博业务场景的操作，发微博、写评论、删除微博，这三个异步操作，如果一个消费者先执行了写评论的操作，但是这时微博都还没发，写评论一定是失败的，等一段时间。等另一个消费者，先执行发微博的操作后，再执行，就可以成功。 延迟消息 例如，订单支付超时时间为30分钟，则我们应该在用户下单后的第30分钟检查订单支付状态，如果发现未支付，应该立刻取消订单，释放库存。 法1：死信交换机+TTL 当一个队列中的消息满足下列情况之一时，可以成为死信（dead letter）： 消费者使用basic.reject或 basic.nack声明消费失败，并且消息的requeue参数设置为false 消息是一个过期消息，超时无人消费 要投递的队列消息满了，无法投递 如果一个队列中的消息已经成为死信，并且这个队列通过dead-letter-exchange属性指定了一个交换机，那么队列中的死信就会投递到这个交换机中，而这个交换机就称为死信交换机（Dead Letter Exchange）。 而此时加入有队列与死信交换机绑定，则最终死信就会被投递到这个队列中。 死信交换机有什么作用呢？ 收集那些因处理失败而被拒绝的消息 收集那些因队列满了而被拒绝的消息 收集因TTL（有效期）到期的消息 如何实现？ ttl.queue根本没有消费者，最终消息会过期，然后被放到死信交换机，到达延迟队列。 注意这里的ttl.fanout不需要RoutingKey，但是当消息变为死信并投递到死信交换机时，会沿用之前的RoutingKey，这样hmall.direct才能正确路由消息。 法2：DelayExchange插件 声明延迟交换机 发送延迟消息时：必须通过x-delay属性设定延迟时间： 延迟消息插件内部会维护一个本地数据库表，同时使用Elang Timers功能实现计时。 如果消息的延迟时间设置较长，可能会导致堆积的延迟消息非常多，会带来较大的CPU开销，同时延迟消息的时间会存在误差。 因此，不建议设置延迟时间过长的延迟消息。 假如订单超时支付时间为30分钟，理论上说我们应该在下单时发送一条延迟消息，延迟时间为30分钟。这样就可以在接收到消息时检验订单支付状态，关闭未支付订单。 但是大多数情况下用户支付都会在1分钟内完成，我们发送的消息却要在MQ中停留30分钟，额外消耗了MQ的资源。因此，我们最好多检测几次订单支付状态，而不是在最后第30分钟才检测。 例如：我们在用户下单后的第10秒、20秒、30秒、45秒、60秒、1分30秒、2分、...30分分别设置延迟消息，如果监视器提前发现订单已经支付，则后续的检测取消即可。这样就可以有效避免对MQ资源的浪费了。 你了解LazeQueue吗？ 在默认情况下，RabbitMQ会将接收到的信息保存在内存中以降低消息收发的延迟。但在某些特殊情况下，这会导致消息积压，比如： 消费者宕机或出现网络故障 消息发送量激增，超过了消费者处理速度 消费者处理业务发生阻塞 一旦出现消息堆积问题，RabbitMQ的内存占用就会越来越高，直到触发内存预警上限。 此时RabbitMQ会将内存消息刷到磁盘上，这个行为成为PageOut. PageOut会耗费一段时间，并且会阻塞队列进程。因此在这个过程中RabbitMQ不会再处理新的消息，生产者的所有请求都会被阻塞。 为了解决这个问题，在3.12版本之后，LazyQueue已经成为所有队列的默认格式 接收到消息后直接存入磁盘而非内存。 消费者要消费消息时才会从磁盘中读取并加载到内存（也就是懒加载） 支持数百万条的消息存储 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-03-03 13:56:28 "},"Chapter3/Redis.html":{"url":"Chapter3/Redis.html","title":"Redis","keywords":"","body":"Redis基础 1. 什么是 Redis？ 基于 C 语言开发的开源 NoSQL 数据库，保存在内存中的（内存数据库，支持持久化），因此读写速度非常快，被广泛应用于分布式缓存方向。Redis 存储的是 KV 键值对数据。 2. Redis 为什么这么快？ Redis 基于内存，内存的访问速度比磁盘快很多； Redis 内置了多种优化过后的数据类型/结构实现，性能非常高。 Redis 基于 Reactor 模式设计开发了一套高效的事件处理模型，主要是单线程事件循环和 IO 多路复用（Redis 线程模式后面会详细介绍到）； Redis 通信协议实现简单且解析高效 3. 除了 Redis，你还知道其他分布式缓存方案吗？ Memcached 是分布式缓存最开始兴起的那会，比较常用的。后来，随着 Redis 的发展，大家慢慢都转而使用更加强大的 Redis 了。 分布式缓存首选 Redis ，毕竟经过这么多年的生考验，生态也这么优秀，资料也很全面！ 4. 为什么要用 Redis？ 1、访问速度更快 传统数据库数据保存在磁盘，而 Redis 基于内存，内存的访问速度比磁盘快很多。引入 Redis 之后，我们可以把一些高频访问的数据放到 Redis 中，这样下次就可以直接从内存中读取，速度可以提升几十倍甚至上百倍。 2、高并发 一般像 MySQL 这类的数据库的 QPS 大概都在 4k 左右（4 核 8g） ，但是使用 Redis 缓存之后很容易达到 5w+，甚至能达到 10w+（就单机 Redis 的情况，Redis 集群的话会更高）。 3、功能全面 Redis 除了可以用作缓存之外，还可以用于分布式锁、限流、消息队列、延时队列等场景，功能强大！ 5. 3种常用的缓存读写策略详解 5.1 Cache Aside Pattern（旁路缓存模式） 由缓存调用者，在更新数据库的同时更新缓存。 在写数据的过程中，可以先删除 cache ，后更新 db 么？ 不行，会出现数据不一致，A删除缓存->B查询缓存未命中，查询数据库中的旧数据，把旧数据写入缓存->A更新数据库为新数据。 反过来呢？先更新db在删除cache。 出现数据不一致的概率很低，因为缓存的写入速度是比数据库的写入速度快很多，需要在很短的时间内更新数据库才会出现缓存不一致。 缺点： 缺陷 1：首次请求数据一定不在 cache 的问题 解决办法：可以将热点数据可以提前放入 cache 中。 缺陷 2：写操作比较频繁的话导致 cache 中的数据会被频繁被删除，这样会影响缓存命中率 。 数据库和缓存数据强一致场景：更新 db 的时候同样更新 cache，不过我们需要加一个锁/分布式锁来保证更新 cache 的时候不存在线程安全问题。 可以短暂地允许数据库和缓存数据不一致的场景：更新 db 的时候同样更新 cache，但是给缓存加一个比较短的过期时间，这样的话就可以保证即使数据不一致的话影响也比较小。 5.2 Read/Write Through Pattern（读写穿透） 缓存与数据库整合为一个服务，由服务来维护一致性。调用者调用该服务，无需关心缓存一致性问题。 服务端把 cache 视为主要数据存储，从中读取数据并将数据写入其中。 cache 服务负责将此数据读取和写入 db，从而减轻了应用程序的职责。 5.3 Write Behind Pattern（异步缓存写入） 调用者只操作缓存，由其它线程异步的将缓存数据持久化到数据库，保证最终一致。 这种策略在我们平时开发过程中也非常非常少见，但是不代表它的应用场景少，比如MySQL 的 Innodb Buffer Pool 机制用到了这种策略。 Redis 应用 1. Redis 除了做缓存，还能做什么？ 分布式锁：通过 Redis 来做分布式锁是一种比较常见的方式。通常情况下，我们都是基于 Redisson 来实现分布式锁。 消息队列：Redis 自带的 List 数据结构可以作为一个简单的队列使用。Redis 5.0 中增加的 Stream 类型的数据结构更加适合用来做消息队列。它比较类似于 Kafka，有主题和消费组的概念，支持消息持久化以及 ACK 机制。 分布式 Session ：利用 String 或者 Hash 数据类型保存 Session 数据，所有的服务器都可以访问。 2. 如何基于 Redis 实现分布式锁？ 基础的，用Lua脚本配合setnx命令，删除的时候先判断是不是自己的锁。 3. Redis 可以做消息队列么？ 可以是可以，但不建议使用 Redis 来做消息队列。和专业的消息队列相比，还是有很多欠缺的地方。 4. Redis 可以做搜索引擎么？ Redis 是可以实现全文搜索引擎功能的，需要借助 RediSearch ，这是一个基于 Redis 的搜索引擎模块。 5. 如何基于 Redis 实现延时任务？ 订单在 10 分钟后未支付就失效，如何用 Redis 实现？ Redis 数据类型 1. Redis 数据类型有哪些？ 1.0 基本概念 五种基本类型： String（字符串）、List（列表）、Set（集合）、Hash（散列）、Zset（有序集合）。 八种数据结构： 简单动态字符串（SDS）、LinkedList（双向链表）、Dict（哈希表/字典）、SkipList（跳跃表）、Intset（整数集合）、ZipList（压缩列表）、QuickList（快速列表）。 1.1 底层实现 数据结构 查询性能 插入/删除性能 内存占用 适用场景 ZipList O(N) O(N) 低（连续存储） 小规模数据，节省内存 SkipList O(log N) O(log N) 高（维护索引） 大规模数据，高效查询 ZipList（压缩列表） ZipList 是一种紧凑的、连续内存存储的数据结构，适用于存储较小的哈希。当哈希中的元素较少且每个元素的键和值都比较小时，Redis 会使用 ZipList 来存储哈希。 内存紧凑：ZipList 将所有元素连续存储在一块内存中，减少了内存碎片和指针的开销。 元素限制：ZipList 适用于元素数量较少且元素大小较小的场景。当元素数量或大小超过一定阈值时，Redis 会将 ZipList 转换为 Dict。 时间复杂度：由于 ZipList 是线性结构，查找、插入和删除操作的时间复杂度为 O(n)，但由于元素数量较少，实际性能影响不大。 Dict： Dict 是 Redis 中最常用的数据结构之一，它是一个基于哈希表的键值对存储结构。当哈希中的元素较多时，Redis 会使用 Dict 来存储哈希。 哈希表：Dict 使用哈希表来实现，哈希表的每个桶（bucket）存储一个键值对。Redis 使用链地址法来解决哈希冲突。 动态扩容：当哈希表中的元素数量超过一定阈值时，Redis 会自动对哈希表进行扩容，以减少哈希冲突，保证查询效率。 时间复杂度：在平均情况下，Dict 的插入、删除和查找操作的时间复杂度都是 O(1)。 IntSet（整数集合） 当集合中的元素都是整数且元素数量较少时，Redis 会使用 IntSet 来存储集合。 有序数组：IntSet 是一个有序的整数数组，元素按照从小到大的顺序存储。这种结构在存储整数时非常高效。 内存紧凑：IntSet 将所有整数连续存储在一块内存中，减少了内存碎片和指针的开销。 元素限制：IntSet 适用于元素数量较少且元素为整数的场景。当元素数量超过一定阈值或元素类型不是整数时，Redis 会将 IntSet 转换为 Dict。 时间复杂度：由于 IntSet 是有序数组，查找操作可以使用二分查找，时间复杂度为 O(log n)。插入和删除操作的时间复杂度为 O(n)，但由于元素数量较少，实际性能影响不大。 QuickList 在早期版本的 Redis 中，列表数据结构有两种实现方式： 双向链表：支持高效的插入和删除操作，但每个节点都需要存储前后指针，内存开销较大。 压缩列表（ZipList）：内存紧凑，适合存储较小的列表，但当列表元素较多或较大时，性能会下降。 为了克服这两种结构的局限性，Redis 引入了 QuickList，它将多个 ZipList 通过双向链表连接起来，既保留了 ZipList 的内存紧凑性，又通过双向链表支持高效的插入和删除操作 QuickList 的整体结构是一个双向链表，其中每个节点都是一个 ZipList。 通过这种方式，QuickList 可以动态地调整每个 ZipList 的大小，以适应不同的使用场景。 1.2 String（字符串） 虽然 Redis 是用 C 语言写的，但是 Redis 并没有使用 C 的字符串表示，而是自己构建了一种 简单动态字符串（Simple Dynamic String，SDS）。 相比于 C 的原生字符串，Redis 的 SDS 不光可以保存文本数据还可以保存二进制数据。 并且获取字符串长度复杂度为 O(1)（C 字符串为 O(N)）,除此之外，Redis 的 SDS API 是安全的，不会造成缓冲区溢出。 应用场景： 需要存储常规数据的场景 举例：缓存 Session、Token、图片地址、序列化后的对象(相比较于 Hash 存储更节省内存)。 相关命令：SET、GET。 需要计数的场景 举例：用户单位时间的请求数（简单限流可以用到）、页面单位时间的访问数。 相关命令：SET、GET、 INCR、DECR 。 分布式锁 利用 SETNX key value 命令可以实现一个最简易的分布式锁（存在一些缺陷，通常不建议这样实现分布式锁）。 1.3 List（列表） Redis 的 List 的实现为一个 双向链表。 应用场景： 最新文章、最新动态。 相关命令：LPUSH、LRANGE。 1.4 Hash（哈希） Hash 类似于 JDK1.8 前的 HashMap，内部实现也差不多(数组 + 链表)。不过，Redis 的 Hash 做了更多优化。 应用场景： 对象数据存储场景：用户信息、商品信息、文章信息、购物车信息。 1.5 Set（集合） 类似于 Java 中的 HashSet。 你可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。这样的话，Set 可以非常方便的实现如共同关注、共同粉丝、共同喜好等功能。 应用场景： 文章的点赞信息、用户的关注信息 1.6 Sorted Set（有序集合） 类似于 Set，但和 Set 相比，Sorted Set 增加了一个权重参数 score，使得集合中的元素能够按 score 进行有序排列，还可以通过 score 的范围来获取元素的列表。 应用场景： 各种排行榜、点赞量排行榜、时间顺序排行。 1.7 特殊数据类型 HyperLogLog（基数统计）、Bitmap （位图）、Geospatial (地理位置) 2. String 还是 Hash 存储对象数据更好呢？ 在绝大多数情况下，String 更适合存储对象数据，尤其是当对象结构简单且整体读写是主要操作时。 如果你需要频繁操作对象的部分字段或节省内存，Hash 可能是更好的选择。因为Hash 是对对象的每个字段单独存储，可以获取部分字段的信息，也可以修改或者添加部分字段，节省网络流量 3. String 的底层实现是什么？ Redis 是基于 C 语言编写的，但 Redis 的 String 类型的底层实现并不是 C 语言中的字符串（即以空字符 \\0 结尾的字符数组），而是自己编写了 SDS（Simple Dynamic String，简单动态字符串） 来作为底层实现。 /* Note: sdshdr5 is never used, we just access the flags byte directly. * However is here to document the layout of type 5 SDS strings. */ struct __attribute__ ((__packed__)) sdshdr5 { unsigned char flags; /* 3 lsb of type, and 5 msb of string length */ char buf[]; }; struct __attribute__ ((__packed__)) sdshdr8 { uint8_t len; /* used */ uint8_t alloc; /* excluding the header and null terminator */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[]; }; struct __attribute__ ((__packed__)) sdshdr16 { uint16_t len; /* used */ uint16_t alloc; /* excluding the header and null terminator */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[]; }; struct __attribute__ ((__packed__)) sdshdr32 { uint32_t len; /* used */ uint32_t alloc; /* excluding the header and null terminator */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[]; }; struct __attribute__ ((__packed__)) sdshdr64 { uint64_t len; /* used */ uint64_t alloc; /* excluding the header and null terminator */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[]; }; SDS 共有五种实现方式 SDS_TYPE_5（并未用到）、SDS_TYPE_8、SDS_TYPE_16、SDS_TYPE_32、SDS_TYPE_64，其中只有后四种实际用到。Redis 会根据初始化的长度决定使用哪种类型，从而减少内存的使用。 SDS 相比于 C 语言中的字符串有如下提升： 可以避免缓冲区溢出：C 语言中的字符串被修改（比如拼接）时，一旦没有分配足够长度的内存空间，就会造成缓冲区溢出。SDS 被修改时，会先根据 len 属性检查空间大小是否满足要求，如果不满足，则先扩展至所需大小再进行修改操作。 获取字符串长度的复杂度较低：C 语言中的字符串的长度通常是经过遍历计数来实现的，时间复杂度为 O(n)。SDS 的长度获取直接读取 len 属性即可，时间复杂度为 O(1)。 减少内存分配次数：为了避免修改（增加/减少）字符串时，每次都需要重新分配内存（C 语言的字符串是这样的），SDS 实现了空间预分配和惰性空间释放两种优化策略。当 SDS 需要增加字符串时，Redis 会为 SDS 分配好内存，并且根据特定的算法分配多余的内存，这样可以减少连续执行字符串增长操作所需的内存重分配次数。当 SDS 需要减少字符串时，这部分内存不会立即被回收，会被记录下来，等待后续使用（支持手动释放，有对应的 API）。 二进制安全：C 语言中的字符串以空字符 \\0 作为字符串结束的标识，这存在一些问题，像一些二进制文件（比如图片、视频、音频）就可能包括空字符，C 字符串无法正确保存。SDS 使用 len 属性判断字符串是否结束，不存在这个问题。 4. 购物车信息用 String 还是 Hash 存储更好呢? 由于购物车中的商品频繁修改和变动，购物车信息建议使用 Hash 存储： 5. 使用 Redis 实现一个排行榜怎么做？ Redis 中有一个叫做 Sorted Set （有序集合）的数据类型经常被用在各种排行榜的场景，比如直播间送礼物的排行榜、朋友圈的微信步数排行榜、王者荣耀中的段位排行榜、话题热度排行榜等等。 相关的一些 Redis 命令: ZRANGE (从小到大排序)、 ZREVRANGE （从大到小排序）、ZREVRANK (指定元素排名)。 6. Redis 的有序集合底层为什么要用跳表，而不用平衡树、红黑树或者 B+树？ 跳表：多层链表，最底层存放全部元素，上面的是索引。 平衡树 vs 跳表：平衡树的插入、删除和查询的时间复杂度和跳表一样都是 O(log n)。对于范围查询来说，平衡树也可以通过中序遍历的方式达到和跳表一样的效果。但是它的每一次插入或者删除操作都需要保证整颗树左右节点的绝对平衡，只要不平衡就要通过旋转操作来保持平衡，这个过程是比较耗时的。跳表诞生的初衷就是为了克服平衡树的一些缺点。跳表使用概率平衡而不是严格强制的平衡，因此，跳表中的插入和删除算法比平衡树的等效算法简单得多，速度也快得多。 红黑树 vs 跳表：相比较于红黑树来说，跳表的实现也更简单一些，不需要通过旋转和染色（红黑变换）来保证黑平衡。并且，按照区间来查找数据这个操作，红黑树的效率没有跳表高。 B+树 vs 跳表：B+树更适合作为数据库和文件系统中常用的索引结构之一，它的核心思想是通过可能少的 IO 定位到尽可能多的索引来获得查询数据。对于 Redis 这种内存数据库来说，它对这些并不感冒，因为 Redis 作为内存数据库它不可能存储大量的数据，所以对于索引不需要通过 B+树这种方式进行维护，只需按照概率进行随机维护即可，节约内存。而且使用跳表实现 zset 时相较前者来说更简单一些，在进行插入时只需通过索引将数据插入到链表中合适的位置再随机维护一定高度的索引即可，也不需要像 B+树那样插入时发现失衡时还需要对节点分裂与合并。 7. Set 的应用场景是什么？ 存放的数据不能重复的场景：文章点赞、动态点赞等等。 需要获取多个数据源交集、并集和差集的场景：共同好友(交集)、共同粉丝(交集)、共同关注(交集)、好友推荐（差集）、音乐推荐（差集）、订阅号推荐（差集+交集） 等等。 需要随机获取数据源中的元素的场景：抽奖系统、随机点名等等。 8. 使用 Set 实现抽奖系统怎么做？ SADD key member1 member2 ...：向指定集合添加一个或多个元素。 SPOP key count：随机移除并获取指定集合中一个或多个元素，适合不允许重复中奖的场景。 SRANDMEMBER key count : 随机获取指定集合中指定数量的元素，适合允许重复中奖的场景。 9. 使用 Bitmap 统计活跃用户怎么做？ 见javaguide。 10. 使用 HyperLogLog 统计页面 UV 怎么做？ PFADD key element1 element2 ...：添加一个或多个元素到 HyperLogLog 中。 PFCOUNT key1 key2：获取一个或者多个 HyperLogLog 的唯一计数。 Redis持久化机制详解 见Redis高级和原理。 Redis 线程模型 1. Redis 单线程模型了解吗？ 在 Redis 6.0 之前，Redis 是典型的单线程模型，Redis 的核心操作（执行命令）（数据读写、过期键处理）是由一个主线程完成的。 从 Redis 6.0 开始，Redis 引入了多线程支持，但核心逻辑仍然是单线程的。多线程主要用于 网络 I/O 和 部分后台任务。 网络 I/O 多线程： Redis 6.0 引入了多线程来处理网络 I/O（读取请求和发送响应）。 主线程仍然负责命令的执行，但网络数据的读取和写入可以由多个 I/O 线程并行处理。 这样可以减轻主线程的负担，提升高并发场景下的性能。 后台任务多线程： 一些耗时的后台任务（如持久化、键过期删除等）可以使用多线程来执行。 例如，Redis 6.0 中，UNLINK 命令用于异步删除大键（DEL的异步版本），避免阻塞主线程。 既然是单线程，那怎么监听大量的客户端连接呢？ Redis 通过 IO 多路复用程序 来监听来自客户端的大量连接（或者说是监听多个 socket），它会将感兴趣的事件及类型（读、写）注册到内核中并监听每个事件是否发生。 I/O 多路复用技术的使用让 Redis 不需要额外创建多余的线程来监听客户端的大量连接，降低了资源的消耗。 2. Redis6.0 之前为什么不使用多线程？ 单线程编程容易并且更容易维护； Redis 的性能瓶颈不在 CPU ，主要在内存和网络； 多线程就会存在死锁、线程上下文切换等问题，甚至会影响性能。 3. Redis6.0 之后为何引入了多线程？ Redis6.0 引入多线程主要是为了提高网络 IO 读写性能，因为这个算是 Redis 中的一个性能瓶颈。 虽然，Redis6.0 引入了多线程，但是 Redis 的多线程只是在网络数据的读写这类耗时操作上使用了，执行命令仍然是单线程顺序执行。因此，你也不需要担心线程安全问题。 4. Redis 后台线程了解吗？ 通过 bio_close_file 后台线程来释放 AOF / RDB 等过程中产生的临时文件资源。 通过 bio_aof_fsync 后台线程调用 fsync 函数将系统内核缓冲区还未同步到到磁盘的数据强制刷到磁盘（ AOF 文件）。 通过 bio_lazy_free后台线程释放大对象（已删除）占用的内存空间. Redis 内存管理 1. Redis 给缓存数据设置过期时间有什么用？ 通过设置合理的过期时间，Redis 会自动删除暂时不需要的数据，为新的缓存数据腾出空间。 Redis 中除了字符串类型有自己独有设置过期时间的命令 setex 外，其他方法都需要依靠 expire 命令来设置过期时间 。另外， persist 命令可以移除一个键的过期时间。 2. Redis 是如何判断数据是否过期的呢？ Redis 通过一个叫做过期字典（可以看作是 hash 表）来保存数据过期的时间。过期字典的键指向 Redis 数据库中的某个 key(键)，过期字典的值是一个 long long 类型的整数，这个整数保存了 key 所指向的数据库键的过期时间（毫秒精度的 UNIX 时间戳）。 3. Redis 过期 key 删除策略了解么？ 如果假设你设置了一批 key 只能存活 1 分钟，那么 1 分钟后，Redis 是怎么对这批 key 进行删除的呢？ 常用的过期数据的删除策略就下面这几种： 惰性删除：只会在取出/查询 key 的时候才对数据进行过期检查。这种方式对 CPU 最友好，但是可能会造成太多过期 key 没有被删除。 定期删除：周期性地随机从设置了过期时间的 key 中抽查一批，然后逐个检查这些 key 是否过期，过期就删除 key。相比于惰性删除，定期删除对内存更友好，对 CPU 不太友好。 延迟队列：把设置过期时间的 key 放到一个延迟队列里，到期之后就删除 key。这种方式可以保证每个过期 key 都能被删除，但维护延迟队列太麻烦，队列本身也要占用资源。 定时删除：每个设置了过期时间的 key 都会在设置的时间到达时立即被删除。这种方法可以确保内存中不会有过期的键，但是它对 CPU 的压力最大，因为它需要为每个键都设置一个定时器。 Redis 采用的是 定期删除+惰性/懒汉式删除 结合的策略，这也是大部分缓存框架的选择。定期删除对内存更加友好，惰性删除对 CPU 更加友好。两者各有千秋，结合起来使用既能兼顾 CPU 友好，又能兼顾内存友好。 Redis 的定期删除过程是随机的（周期性地随机从设置了过期时间的 key 中抽查一批），所以并不保证所有过期键都会被立即删除。这也就解释了为什么有的 key 过期了，并没有被删除。并且，Redis 底层会通过限制删除操作执行的时长和频率来减少删除操作对 CPU 时间的影响。 另外，定期删除还会受到执行时间和过期 key 的比例的影响： 执行时间已经超过了阈值，那么就中断这一次定期删除循环，以避免使用过多的 CPU 时间。 如果这一批过期的 key 比例超过一个比例，就会重复执行此删除流程，以更积极地清理过期 key。相应地，如果过期的 key 比例低于这个比例，就会中断这一次定期删除循环，避免做过多的工作而获得很少的内存回收。 为什么定期删除不是把所有过期 key 都删除呢？ 这样会对性能造成太大的影响。如果我们 key 数量非常庞大的话，挨个遍历检查是非常耗时的，会严重影响性能。Redis 设计这种策略的目的是为了平衡内存和性能。 为什么 key 过期之后不立马把它删掉呢？这样不是会浪费很多内存空间吗？ 因为不太好办到，或者说这种删除方式的成本太高了。假如我们使用延迟队列作为删除策略，这样存在下面这些问题： 队列本身的开销可能很大：key 多的情况下，一个延迟队列可能无法容纳。 维护延迟队列太麻烦：修改 key 的过期时间就需要调整期在延迟队列中的位置，并且，还需要引入并发控制。 4. 大量 key 集中过期怎么办？ 请求延迟增加： Redis 在处理过期 key 时需要消耗 CPU 资源，如果过期 key 数量庞大，会导致 Redis 实例的 CPU 占用率升高，进而影响其他请求的处理速度，造成延迟增加。 内存占用过高： 过期的 key 虽然已经失效，但在 Redis 真正删除它们之前，仍然会占用内存空间。如果过期 key 没有及时清理，可能会导致内存占用过高，甚至引发内存溢出。 解决方法： 尽量避免 key 集中过期: 在设置键的过期时间时尽量随机一点。 开启 lazy free 机制: 修改 redis.conf 配置文件，将 lazyfree-lazy-expire 参数设置为 yes，即可开启 lazy free 机制。开启 lazy free 机制后，Redis 会在后台异步删除过期的 key，不会阻塞主线程的运行，从而降低对 Redis 性能的影响。 5. Redis 内存淘汰策略了解么？ MySQL 里有 2000w 数据，Redis 中只存 20w 的数据，如何保证 Redis 中的数据都是热点数据? volatile-lru（least recently used）：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰。 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰。 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰。 allkeys-lru（least recently used）：从数据集（server.db[i].dict）中移除最近最少使用的数据淘汰。 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰。 no-eviction（默认内存淘汰策略）：禁止驱逐数据，当内存不足以容纳新写入数据时，新写入操作会报错。 Redis性能优化 1. 使用批量操作减少网络传输 发送命令 命令排队 命令执行 返回结果 其中，第 1 步和第 4 步耗费时间之和称为 Round Trip Time (RTT,往返时间) ，也就是数据在网络上传输的时间。 使用批量操作可以减少网络传输次数，进而有效减小网络开销，大幅减少 RTT。 2. Redis bigkey（大 Key） 如果一个 key 对应的 value 所占用的内存比较大，那这个 key 就可以看作是 bigkey。 具体多大才算大呢？有一个不是特别精确的参考标准： String 类型的 value 超过 1MB 复合类型（List、Hash、Set、Sorted Set 等）的 value 包含的元素超过 5000 个（不过，对于复合类型的 value 来说，不一定包含的元素越多，占用的内存就越多）。 原因： 程序设计不当，比如直接使用 String 类型存储较大的文件对应的二进制数据。 对于业务的数据规模考虑不周到，比如使用集合类型的时候没有考虑到数据量的快速增长。 未及时清理垃圾数据，比如哈希中冗余了大量的无用键值对。 危害： 客户端超时阻塞：由于 Redis 执行命令是单线程处理，然后在操作大 key 时会比较耗时，那么就会阻塞 Redis，从客户端这一视角看，就是很久很久都没有响应。 网络阻塞：每次获取大 key 产生的网络流量较大，如果一个 key 的大小是 1 MB，每秒访问量为 1000，那么每秒会产生 1000MB 的流量，这对于普通千兆网卡的服务器来说是灾难性的。 工作线程阻塞：如果使用 del 删除大 key 时，会阻塞工作线程，这样就没办法处理后续的命令。 如何发现 bigkey？ 1、使用 Redis 自带的 --bigkeys 参数来查找。 2、使用 Redis 自带的 SCAN 命令 4、借助公有云的 Redis 分析服务。 3. Redis hotkey（热 Key） 如果一个 key 的访问次数比较多且明显多于其他 key 的话，那这个 key 就可以看作是 hotkey（热 Key）。 处理 hotkey 会占用大量的 CPU 和带宽，可能会影响 Redis 实例对其他请求的正常处理。 如何发现 hotkey？ 使用 Redis 自带的 --hotkeys 参数来查找。 根据业务情况提前预估。 借助公有云的 Redis 分析服务。 如何解决 hotkey？ 读写分离：主节点处理写请求，从节点处理读请求。 使用 Redis Cluster：将热点数据分散存储在多个 Redis 节点上。 二级缓存：hotkey 采用二级缓存的方式进行处理，将 hotkey 存放一份到 JVM 本地内存中（可以用 Caffeine）。 4. 慢查询命令 为什么会有慢查询命令？ Redis 中的大部分命令都是 O(1)时间复杂度，但也有少部分 O(n) 时间复杂度的命令，例如： KEYS *：会返回所有符合规则的 key。 HGETALL：会返回一个 Hash 中所有的键值对。 LRANGE：会返回 List 中指定范围内的元素。 SMEMBERS：返回 Set 中的所有元素。 SINTER/SUNION/SDIFF：计算多个 Set 的交集/并集/差集。 除了这些 O(n)时间复杂度的命令可能会导致慢查询之外， 还有一些时间复杂度可能在 O(N) 以上的命令，例如： ZRANGE/ZREVRANGE：返回指定 Sorted Set 中指定排名范围内的所有元素。时间复杂度为 O(log(n)+m)，n 为所有元素的数量， m 为返回的元素数量，当 m 和 n 相当大时，O(n) 的时间复杂度更小。 如何找到慢查询命令？ 在 redis.conf 文件中，我们可以使用 slowlog-log-slower-than 参数设置耗时命令的阈值，并使用 slowlog-max-len 参数设置耗时命令的最大记录条数。 当 Redis 服务器检测到执行时间超过 slowlog-log-slower-than阈值的命令时，就会将该命令记录在慢查询日志(slow log) 中，这点和 MySQL 记录慢查询语句类似。 5. Redis 内存碎片 举个例子：操作系统为你分配了 32 字节的连续内存空间，而你存储数据实际只需要使用 24 字节内存空间，那这多余出来的 8 字节内存空间如果后续没办法再被分配存储其他数据的话，就可以被称为内存碎片。 1、Redis 存储数据的时候向操作系统申请的内存空间可能会大于数据实际需要的存储空间。 2、频繁修改 Redis 中的数据也会产生内存碎片。 如何清理 Redis 内存碎片？ config set activedefrag yes # 内存碎片占用空间达到 500mb 的时候开始清理 config set active-defrag-ignore-bytes 500mb # 内存碎片率大于 1.5 的时候开始清理 config set active-defrag-threshold-lower 50 主从架构、哨兵、分片集群 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-02-27 10:24:23 "},"Chapter3/Redis初级和实战.html":{"url":"Chapter3/Redis初级和实战.html","title":"Redis初级和实战","keywords":"","body":"Redis基础篇 简介 Redis是基于内存的键值数据库（NoSql），全名remote dictionary server，远程词典服务器。 Key是id，Value可以是Json格式、也可以是List 特性： 键值（key-value）型，value支持多种不同数据结构，功能丰富 单线程，每个命令具备原子性 低延迟，速度快（基于内存、IO多路复用、良好的编码）。 支持数据持久化（内存数据定期写入磁盘） 支持主从集群、分片集群（水平扩展） 支持多语言客户端 Nosql关联性是靠程序员自己维护的，比如放着嵌套json。 基本类型 常用命令 通用 指令 描述 KEYS 查看符合模板的所有key，不建议在生产环境设备上使用 DEL 删除一个指定的key EXISTS 判断key是否存在 EXPIRE 给一个key设置有效期，有效期到期时该key会被自动删除 TTL 查看一个KEY的剩余有效期 String String类型，也就是字符串类型，是Redis中最简单的存储类型，不过根据字符串的格式不同： string：普通字符串 int：整数类型，可以做自增、自减操作 float：浮点类型，可以做自增、自减操作 不管是哪种格式，底层都是字节数组形式存储，只不过是编码方式不同。字符串类型的最大空间不能超过512m. 命令 描述 SET 添加或者修改已经存在的一个String类型的键值对 GET 根据key获取String类型的value MSET 批量添加多个String类型的键值对 MGET 根据多个key获取多个String类型的value INCR 让一个整型的key自增1 INCRBY 让一个整型的key自增并指定步长，例如：incrby num 2 让num值自增2 INCRBYFLOAT 让一个浮点类型的数字自增并指定步长 SETNX 添加一个String类型的键值对，前提是这个key不存在，否则不执行 SETEX 添加一个String类型的键值对，并且指定有效期 Key的层级 例如我们的项目名称叫 heima，有user和product两种不同类型的数据，我们可以这样定义key： 可以把java对象序列化为json存储。缺点：修改某个属性的话，整个字符串都得修改 set heima:user:1 '{“id”:1, “name”: “Jack”, “age”: 21}' 可视化软件打开就有了层次结构。 KEY VALUE heima:user:1 {“id”:1, “name”: “Jack”, “age”: 21} heima:product:1 {“id”:1, “name”: “小米11”, “price”: 4999} Hash Hash类型，也叫散列，其value是一个无序字典，类似于Java中的HashMap结构.. 命令 描述 HSET key field value 添加或者修改hash类型key的field的值 HGET key field 获取一个hash类型key的field的值 HDEL key field 移除Hash中的字段 DEL key 移除整个Hash HMSET hmset 和 hset 效果相同 ，4.0之后hmset可以弃用了 HMGET 批量获取多个hash类型key的field的值 HGETALL 获取一个hash类型的key中的所有的field和value HKEYS 获取一个hash类型的key中的所有的field HVALS 获取一个hash类型的key中的所有的value HINCRBY 让一个hash类型key的字段值自增并指定步长 HSETNX 添加一个hash类型的key的field值，前提是这个field不存在，否则不执行 List Redis中的List类型与Java中的LinkedList类似，可以看做是一个双向链表结构。既可以支持正向检索和也可以支持反向检索。 有序 元素可以重复 插入和删除快 查询速度一般 常用来存储一个有序数据，例如：朋友圈点赞列表，评论列表等。 命令 描述 LPUSH key element … 向列表左侧插入一个或多个元素 LPOP key 移除并返回列表左侧的第一个元素，没有则返回nil RPUSH key element … 向列表右侧插入一个或多个元素 RPOP key 移除并返回列表右侧的第一个元素 LRANGE key star end 返回一段角标范围内的所有元素 BLPOP和BRPOP 与LPOP和RPOP类似，只不过在没有元素时等待指定时间，而不是直接返回nil 如何利用List结构模拟一个阻塞队列? 入口和出口在不同边 出队时采用BLPOP或BRPOP Set Redis的Set结构与Java中的HashSet类似，可以看做是一个value为null的HashMap。 因为也是一个hash表，因此具备与HashSet类似的特征 无序 元素不可重复 查找快 支持交集、并集、差集等功能 命令 描述 SADD key member … 向set中添加一个或多个元素 SREM key member … 移除set中的指定元素 SCARD key 返回set中元素的个数 SISMEMBER key member 判断一个元素是否存在于set中 SMEMBERS 获取set中的所有元素 SINTER key1 key2 … 求key1与key2的交集 SDIFF key1 key2 … 求key1与key2的差集 SUNION key1 key2 … 求key1和key2的并集 SortedSet Redis的SortedSet是一个可排序的set集合，与Java中的TreeSet有些类似，但底层数据结构却差别很大。 SortedSet中的每一个元素都带有一个score属性，可以基于score属性对元素排序，底层的实现是一个跳表（SkipList）加 hash表。 可排序 元素不重复 查询速度快 score相同，按字典序排序 因为SortedSet的可排序特性，经常被用来实现排行榜这样的功能。 命令 描述 ZADD key score member 添加一个或多个元素到sorted set ，如果已经存在则更新其score值 ZREM key member 删除sorted set中的一个指定元素 ZSCORE key member 获取sorted set中的指定元素的score值 ZRANK key member 获取sorted set 中的指定元素的排名 ZCARD key 获取sorted set中的元素个数 ZCOUNT key min max 统计score值在给定范围内的所有元素的个数 ZINCRBY key increment member 让sorted set中的指定元素自增，步长为指定的increment值 ZRANGE key min max 按照score排序后，获取指定排名范围内的元素 ZRANGEBYSCORE key min max 按照score排序后，获取指定score范围内的元素 ZDIFF、ZINTER、ZUNION 求差集、交集、并集 注意：所有的排名默认都是升序，如果要降序则在命令的Z后面添加REV即可。 Java客户端 Jedis 以Redis命令作为方法名称，学习成本低，简单实用。 但是Jedis实例是线程不安全的，多线程环境下需要基于连接池来使用 public class JedisConnectionFactory { private static final JedisPool jedisPool; static { //配置连接池 JedisPoolConfig jedisPoolConfig = new JedisPoolConfig(); //最多的Jedis jedisPoolConfig.setMaxTotal(8); //最多存在8个Jedis jedisPoolConfig.setMaxIdle(8); //最少存在0个Jedis jedisPoolConfig.setMinIdle(0); //如果jedis满了，1秒后就会报错，默认是-1（一直等待jedis） jedisPoolConfig.setMaxWaitMillis(200); //创建连接池对象 jedisPool = new JedisPool(jedisPoolConfig,\"localhost\",6379,1000,\"123\"); } public static Jedis getJedis(){ return jedisPool.getResource(); } } SpringDataRedis SpringData是Spring中数据操作的模块，包含对各种数据库的集成，其中对Redis的集成模块就叫做SpringDataRedis。 提供了对不同Redis客户端的整合（Lettuce和Jedis） 提供了RedisTemplate统一API来操作Redis 支持Redis的发布订阅模型 支持Redis哨兵和Redis集群 支持基于Lettuce的响应式编程 支持基于JDK、JSON、字符串、Spring对象的数据序列化及反序列化 支持基于Redis的JDKCollection实现 RedisTemplate可以接收任意Object作为值写入Redis，只不过写入前会把Object序列化为字节形式，默认是采用JDK序列化，得到的结果是这样的 缺点：可读性差，内存占用较大。 序列化方法1： 我们可以通过自定义RedisTemplate序列化的方式来解决。 public RedisTemplate redisTemplate(RedisConnectionFactory factory){ // 1.创建RedisTemplate对象 RedisTemplate redisTemplate = new RedisTemplate<>(); // 2.设置连接工厂 redisTemplate.setConnectionFactory(factory); // 3.创建序列化对象 StringRedisSerializer stringRedisSerializer = new StringRedisSerializer(); GenericJackson2JsonRedisSerializer genericJackson2JsonRedisSerializer = new GenericJackson2JsonRedisSerializer(); // 4.设置key和hashKey采用String的序列化方式 redisTemplate.setKeySerializer(stringRedisSerializer); redisTemplate.setHashKeySerializer(stringRedisSerializer); // 5.设置value和hashValue采用json的序列化方式 redisTemplate.setValueSerializer(genericJackson2JsonRedisSerializer); redisTemplate.setHashValueSerializer(genericJackson2JsonRedisSerializer); return redisTemplate; } 此时我们已经将RedisTemplate的key设置为String序列化，value设置为Json序列化的方式，执行插入对象。 如上图所示，为了在反序列化时知道对象的类型，JSON序列化器会将类的class类型写入json结果中，存入Redis，会带来额外的内存开销。 序列化方法2： 为了节省内存空间，我们并不会使用JSON序列化器来处理value，而是统一使用String序列化器，StringRedisTemplate默认使用的就是String序列化器，当然这只能存储String类型的key和value。当需要存储Java对象时，使用ObjectMappe手动完成对象的序列化和反序列化。 @SpringBootTest class RedisStringTemplateTest { @Resource private StringRedisTemplate stringRedisTemplate; @Test void testSaveUser() throws JsonProcessingException { // 1.创建一个Json序列化对象 ObjectMapper objectMapper = new ObjectMapper(); // 2.将要存入的对象通过Json序列化对象转换为字符串 String userJson1 = objectMapper.writeValueAsString(new User(\"Vz\", 21)); // 3.通过StringRedisTemplate将数据存入redis stringRedisTemplate.opsForValue().set(\"user:100\",userJson1); // 4.通过key取出value String userJson2 = stringRedisTemplate.opsForValue().get(\"user:100\"); // 5.由于取出的值是String类型的Json字符串，因此我们需要通过Json序列化对象来转换为java对象 User user = objectMapper.readValue(userJson2, User.class); // 6.打印结果 System.out.println(\"user = \" + user); } } 黑马点评 Session理解 当用户首次访问Web应用时，服务器会自动创建一个新的session。这通常是通过调用HttpServletRequest.getSession(true)来实现的，其中true参数表示如果不存在session，则创建一个新的session，session有一个id，id被存储在客户端的cookie中。 如果客户端的cookie没有被删除，当用户再次请求服务器时候，服务器就能根据cookie中的sessionid来在tomcat中获得session。 如果用户长时间没有访问服务器，服务器的session可能会过期，即使此时客户端的cookie没有被删除，也会要求用户重新登录，服务器会为用户分配一个新的Session ID，并将其发送回客户端。客户端（浏览器）会更新Cookie中的Session ID，以便后续的请求使用新的会话标识。这样做是为了确保会话的安全性，防止旧的、可能被泄露的Session ID被滥用。 Session管理和Session ID的创建、发送和更新都由Servlet容器（如Tomcat、Jetty等）自动处理。 Session共享问题 Redis登录逻辑分析 可以使用Hash，也可以采用String存取json文件。 前段如何存储token？我们返回给前端token，前端可使用浏览器的SessionStorage存储token，然后给ajox添加一个拦截器，所有从前端发送的请求都在header里面加上一个字段'authorization'存放token。 用户登录状态？业务要求：只要你访问了页面，就应该刷新redis有效期。 缓存 缓存的优点： 降低后端负担 提高读写效率，降低响应时间 缓存的缺点： 数据一致性成本 代码维护成本 运维成本 商户查询缓存 缓存更新策略 ​ 由于我们的缓存数据源来自数据库，而数据库的数据是会发生变化的，因此，如果当数据库中数据发生变化，而缓存却没有同步，此时就会有一致性问题存在：用户使用缓存中的过时数据，从而影响业务，产品口碑等。 方案三解释： 我们进行了十次更新，都在缓存中执行。第十次更新后，线程把缓存写到数据库中，这样数据库只更新了一次，效率高。 缺点：难以保证数据一致性，如果缓存丢失，线程还没来得及写入数据库，就会发生问题。 是更新缓存，还是删除缓存？为了避免无效的写缓存（很长一段时间压根没人来查缓存），我们可以直接删除缓存。 更新缓存还会造成线程安全问题。 是先删除缓存，还是先更新数据库？（要保证线程安全问题） 方案2，需要查询缓存的时候恰好缓存超时过期，且在写入缓存这种微妙级操作中，又来了一个线程去更新数据库和缓存。 缓存穿透 一般采用方案1：缓存空对象。 缓存空对象缺点： 我如果乱编不存在的id，redis中会出现大量无效数据，造成额外内存消耗，当然应对措施是给数据设置一个较短的ddl。 还可能出现短期不一致，如果这个数据被插入数据库中了，redis还是会返回null，当然应对措施是插入新数据的时候删除redis。 布隆过滤缺点： 可能会存在误判，布隆过滤拒绝的，数据库中一定不存在，布隆过滤允许的，可能数据库中不存在，是个概率拦截模型。 还有以下措施： 增强id的复杂度，避免被猜测id规律 做好数据的基础格式校验，校验id是否合法，过短和过长的id直接可以pass掉 加强用户权限校验，比如一些页面必须登录才能操作 做好热点参数的限流，监测到商品被访问的特别多，可以加限流 查询商铺： 缓存雪崩 给不同的Key的TTL添加随机值，让其在不同时间段分批失效 利用Redis集群提高服务的可用性（使用一个或者多个哨兵(Sentinel)实例组成的系统，对redis节点进行监控，在主节点出现故障的情况下，能将从节点中的一个升级为主节点，进行故障转义，保证系统的可用性。 给缓存业务添加降级限流策略（自然灾害导致Redis服务器受损，为了保护数据库，我们可以采取快速失败，避免业务继续往后） 业务添加多级缓存（浏览器访问静态资源时，优先读取浏览器本地缓存；访问非静态资源（ajax查询数据）时，访问服务端；请求到达Nginx后，优先读取Nginx本地缓存；如果Nginx本地缓存未命中，则去直接查询Redis（不经过Tomcat）；如果Redis查询未命中，则查询Tomcat；请求进入Tomcat后，优先查询JVM进程缓存；如果JVM进程缓存未命中，则查询数据库） 缓存击穿 发生情况？ 一件秒杀中的商品的key突然失效了，大家都在疯狂抢购，那么这个瞬间就会有无数的请求访问去直接抵达数据库，从而造成缓存击穿。 如果重建缓存需要多个表查询，这样重建过程可能得到几百毫秒，这时候又来无数请求访问，都在重建，数据库可能会宕机。 与缓存雪崩区别 缓存雪崩是大量的key同时失效，缓存击穿是热门的key失效。 互斥锁： ​ 利用锁的互斥性，假设线程过来，只能一个人线程去访问数据库，重建缓存，其他线程只能休眠一会，在尝试重新查询缓存。从而避免对数据库频繁访问产生过大压力，但这也会影响查询的性能，将查询的性能从并行变成了串行。 ​ 缺点在于有锁的情况，就可能死锁：我在进行业务的时候需要查询多个缓存，但是一个缓存在别的业务里面被加了锁，别的业务又需要你这个业务加的锁，就出现了死锁：无限等待。 逻辑过期： ​ 我们之所以会出现缓存击穿问题，主要原因是在于我们对key设置了TTL，如果我们不设置TTL，那么就不会有缓存击穿问题，但是不设置TTL，数据又会一直占用我们的内存，所以我们可以采用逻辑过期方案， ​ 假设线程1去查询缓存，然后从value中判断当前数据已经过期了，此时线程1去获得互斥锁，那么其他线程会进行阻塞，获得了锁的进程他会开启一个新线程去进行之前的重建缓存数据的逻辑，直到新开的线程完成者逻辑之后，才会释放锁，而线程1直接进行返回，假设现在线程3过来访问，由于线程2拿着锁，所以线程3无法获得锁，线程3也直接返回数据（但只能返回旧数据，牺牲了数据一致性，换取性能上的提高），只有等待线程2重建缓存数据之后，其他线程才能返回正确的数据。 ​ 优点是不会像互斥锁一样，很多线程一直等待，性能好。 ​ 缺点是在重建完缓存数据之前，返回的都是脏数据，而且有额外的内存消耗。 两种方案：一种保证一致性，一种保证可用性，都可以。 互斥锁 逻辑过期 全局唯一ID 订单表如果使用数据库自增ID就会存在一些问题 id规律性太明显，那么对于用户或者竞争对手，就很容易猜测出我们的一些敏感信息。 受单表数据量的限制, MySQL的单表容量不宜超过500W，数据量过大之后，我们就要进行拆库拆表，多个表之间id会重复。 全局id特性 唯一性 高可用，不能挂掉 高性能，不能耽误时间 递增性，随时间递增 安全性，不能太有规律 前面32位是时间戳，后面是计数器，计数器可以设定为每天的销售量，存入redis中。 @Component public class RedisIdWorker { @Autowired private StringRedisTemplate stringRedisTemplate; //设置起始时间，我这里设定的是2022.01.01 00:00:00，从1970年到现在的秒数。 public static final Long BEGIN_TIMESTAMP = 1640995200L; //序列号长度 public static final Long COUNT_BIT = 32L; //keyprefix是业务名 public long nextId(String keyPrefix){ //1. 生成时间戳 LocalDateTime now = LocalDateTime.now(); long currentSecond = now.toEpochSecond(ZoneOffset.UTC); long timeStamp = currentSecond - BEGIN_TIMESTAMP; //2. 生成序列号，以天为单位，每天是不同的key String date = now.format(DateTimeFormatter.ofPattern(\"yyyy:MM:dd\")); //不会空指针，如果是新的一天的第一条数据，会自动插入一个1 long count = stringRedisTemplate.opsForValue().increment(\"inc:\" + keyPrefix + \":\" + date); //3. 拼接并返回，简单位运算 return timeStamp 秒杀功能 超卖问题 假设现在只剩下一张优惠券，线程1过来查询库存，判断库存数大于1，但还没来得及去扣减库存，此时库线程2也过来查询库存，发现库存数也大于1，那么这两个线程都会进行扣减库存操作，最终相当于是多个线程都进行了扣减库存，那么此时就会出现超卖问题。 超卖问题是典型的多线程安全问题，针对这一问题的常见解决方案就是加锁：而对于加锁，我们通常有两种解决方案 悲观锁 悲观锁认为线程安全问题一定会发生，因此在操作数据之前先获取锁，确保线程串行执行 例如Synchronized、Lock等，都是悲观锁 乐观锁 乐观锁认为线程安全问题不一定会发生，因此不加锁，只是在更新数据的时候再去判断有没有其他线程对数据进行了修改 如果没有修改，则认为自己是安全的，自己才可以更新数据 如果已经被其他线程修改，则说明发生了安全问题，此时可以重试或者异常 悲观锁的性能低。 乐观锁 版本号法： 添加一个版本号，每次修改的时候会对版本号进行加1，修改条件也判断版本号是不是之前的版本号。 实际上利用了mysql的update语句带锁。 CAS方法 Compare-And-Swap 可以进行简化，直接在where 语句后面判断此时的库存是不是之前查询的库存，这和版本号做的一样的事情。 但是缺点是成功率太低！ 我们可以直接把语句继续优化，库存大于0就行。 但是有些问题只能用相等比较，解决方法是把数据存在多张表，这样每个锁的数据就少，线程可以抢多个锁。 二者比较 悲观锁：添加同步锁，让线程串行执行 优点:简单粗暴 缺点:性能一般 乐观锁：不加锁，在更新时判断是否有其它线程在修改 优点:性能好 缺点:存在成功率低的问题 一人一单 只能用悲观锁，因为乐观锁是修改，之前有数据才能比较数据有没有变。我们业务是查询，只能用悲观锁。 代理对象 在 Spring 中，事务管理是通过代理实现的，因此在获取数据库事务时必须使用代理对象。 this 是普通对象：在同步方法中使用 this 作为锁，意味着你是在锁住当前对象的实例。如果你在这个实例的方法中调用其他需要事务管理的方法，直接使用 this 可能不会触发 AOP 相关的增强，因为 AOP 代理只会在通过代理对象进行的调用时生效。 需要代理对象来获取事务功能：为了利用 AOP 的切面功能（比如事务管理），必须使用 AopContext.currentProxy() 来获得当前的代理对象。这样可以确保执行的方法具有同步及其它切面逻辑的 Enhancements。 使用Synchronized修饰事务方法，同步为什么会失效? spring事务管理中，使用Synchronized修饰事务方法，同步为什么会失效_spring synchronized-CSDN博客 简述：当A线程执行完保存操作后就会去释放锁，而此时还没有提交事务，B线程获取锁后，通过用户名查询，由于数据库隔离级别，不能查询到未提交的数据，所以B线程进行了二次插入操作，等执行完后它们一起提交事务，就会出现脏写这种线程安全问题了。 为什么要用intern public Result seckillVoucher(Long voucherId) { // 1 查询优惠卷 SeckillVoucher seckillVoucher = seckillVoucherService.getById(voucherId); // 2 判断开始时间 if(seckillVoucher.getBeginTime().isAfter(LocalDateTime.now())){ return Result.fail(\"秒杀未开始！\"); } // 3 判断结束时间 if(seckillVoucher.getEndTime().isBefore(LocalDateTime.now())){ return Result.fail(\"秒杀已结束！\"); } // 4 判断库存 if(seckillVoucher.getStock() 0) { return Result.fail(\"不能重复下单！\"); } // 7 扣减库存,乐观锁 boolean success = seckillVoucherService.update() .setSql(\"stock = stock - 1\") .eq(\"voucher_id\", voucherId) .gt(\"stock\",0) .update(); if(!success){ return Result.fail(\"库存不足！\"); } // 8 创建订单 VoucherOrder voucherOrder = new VoucherOrder(); orderId = redisIdWorker.nextId(\"order\"); voucherOrder.setId(orderId); voucherOrder.setUserId(userId); voucherOrder.setVoucherId(voucherId); save(voucherOrder); // 9 返回订单id return Result.ok(orderId); } 集群环境下的并发问题 ​ 我们部署了多个Tomcat，每个Tomcat都有一个属于自己的jvm，那么假设在服务器A的Tomcat内部，有两个线程，即线程1和线程2，如果这两个线程的锁对象是同一个，比如“352”字符串为锁，是可以实现互斥的。 ​ 但是如果在另一个Tomcat的内部，又有两个线程，但是他们的锁对象虽然写的和服务器A一样是“352”，线程3和线程4可以实现互斥，但是却无法和线程1和线程2互斥。 ​ 因为锁机制是在 Java 虚拟机的内部实现的，而每个 JVM 都有自己的内存空间和对象。如果你在服务器A上将字符串“352”用作锁，那么这个锁只对服务器A的 JVM 内部的线程（如线程1和线程2）有效。线程3和线程4虽然使用的是同样的字符串“352”来作为锁，但这个字符串对象在它们自己的 JVM 中是一个全新的对象，完全与服务器A中的“352”锁无关 分布式锁 满足分布式系统或集群模式下多进程（JVM）课件并且可以互斥的锁。 应该满足： 可见性：多个进程都能看到相同的结果。 互斥：互斥是分布式锁的最基本条件，使得程序串行执行 高可用：程序不易崩溃，时时刻刻都保证较高的可用性 高性能：由于加锁本身就让性能降低，所以对于分布式锁需要他较高的加锁性能和释放锁性能 安全性：如果锁没释放，程序崩了，会不会发生死锁。 MySQL：本身就带有锁机制，但是由于MySQL的性能一般，所以采用分布式锁的情况下，使用MySQL作为分布式锁比较少见，但是mysql安全性能较好，即使程序崩了，锁也会释放。 Redis：Redis作为分布式锁是非常常见的一种使用方式，现在企业级开发中基本都是用Redis或者Zookeeper作为分布式锁，利用SETNX这个方法，如果插入Key成功，则表示获得到了锁，如果有人插入成功，那么其他人就回插入失败，无法获取到锁，利用这套逻辑完成互斥，从而实现分布式锁。释放锁删除key就行。 缺点是：如果程序崩了，只能利用锁的超时时间来释放锁。 Zookeeper： 创建锁节点： 客户端在 Zookeeper 中创建一个临时节点（znode），并设置为顺序节点（例如，命名为 /lock/lock-）。 Zookeeper 会自动为这个节点追加一个序列号，形成一个唯一标识，是递增的。 获取锁的顺序： 客户端获取所有以 /lock/ 为前缀的节点并排序。 判断当前客户端的节点是否是最小的序列号节点。如果是，则获得锁；如果不是，则需要等待。 等待锁： 当客户端创建的节点不是最小节点时，客户端需要监听比自己序列号小的节点的删除事件。 如果有其他客户端释放锁（删除其节点），则会触发监听，客户端将重新检查节点顺序，判断自己是否为最小节点。 释放锁： 客户端使用完共享资源后，删除自己创建的临时节点以释放锁。 这将触发其他等待锁的客户端，可能会让它们获得锁。 Mysql Redis Zookeeper 互斥 利用mysql本身的互斥锁机制 利用setnx这样的互斥命令 利用节点的唯一性和有序性实现互斥 高可用 好 好 好 高性能 一般 好 一般 安全性 断开连接，自动释放锁 利用锁超时时间，到期释放 临时节点，断开连接自动释放 Redis实现思路 获取锁：互斥、非阻塞（尝试一次，成功返回true，失败返回false） 释放锁：手动释放+超时释放（保底） 注意，不能使用setnx命令，因为使用setnx命令，要再使用一个expire设置过期时间，需要保证原子性，否则可能没来得及设置过期时间你的程序就宕机了，产生死锁。 SET lock thread01 NX EX 10 //成功返回ok，失败返回nil，NX是互斥，EX是超时时间 DEL lock 什么是阻塞式获取锁？ 如果锁已经被其他线程占用，该线程将会被挂起，直到锁被释放为止。 实现代码 注：代码不完全版本 public class SimpleRedisLock implements ILock{ private static final String KEY_PREFIX = \"lock:\"; private String name; //不是spring管理的类，不能自动给你注入 private StringRedisTemplate stringRedisTemplate; public SimpleRedisLock(String name, StringRedisTemplate stringRedisTemplate) { this.name = name; this.stringRedisTemplate = stringRedisTemplate; } @Override public boolean tryLock(long timeoutSec) { long threadId = Thread.currentThread().getId(); Boolean success = stringRedisTemplate .opsForValue().setIfAbsent(KEY_PREFIX + name, threadId + \"\", timeoutSec, TimeUnit.SECONDS); // 避免拆箱产生空指针 return Boolean.TRUE.equals(success); } @Override public void unlock() { stringRedisTemplate.delete(KEY_PREFIX + name); } } 使用分布式锁 SimpleRedisLock simpleRedisLock = new SimpleRedisLock(\"order:\" + userId, stringRedisTemplate); boolean isLock = simpleRedisLock.tryLock(1200); if(!isLock){ //代表一个人在开挂抢！ return Result.fail(\"不允许重复下单！\"); } try { VoucherOrderService proxy = (IVoucherOrderService) AopContext.currentProxy(); return proxy.createVoucherOrder(voucherId); } catch (IllegalStateException e) { throw new RuntimeException(e); } finally { simpleRedisLock.unlock(); } 误删问题 持有锁的线程1在锁的内部出现了阻塞，导致他的锁TTL到期，自动释放 此时线程2也来尝试获取锁，由于线程1已经释放了锁，所以线程2可以拿到 但是现在线程1阻塞完了，继续往下执行，要开始释放锁了 那么此时就会将属于线程2的锁释放，这就是误删别人锁的情况 解决方案就是在每个线程释放锁的时候，都判断一下这个锁是不是自己的，如果不属于自己，则不进行删除操作。 改进思路 在获取锁的时候存入线程标识（用UUID标识+线程id，在一个JVM中，ThreadId一般不会重复，但是我们现在是集群模式，有多个JVM，多个JVM之间可能会出现ThreadId重复的情况），在释放锁的时候先获取锁的线程标识，判断是否与当前线程标识一致 如果一致则释放锁 如果不一致则不释放锁 package com.hmdp.utils; import cn.hutool.core.lang.UUID; import org.springframework.data.redis.core.StringRedisTemplate; import java.util.concurrent.TimeUnit; public class SimpleRedisLock implements ILock{ private static final String KEY_PREFIX = \"lock:\"; private static final String ID_PREFIX = UUID.randomUUID().toString(true) + \"-\"; //业务名 private String name; //不是spring管理的类，不能自动给你注入 private StringRedisTemplate stringRedisTemplate; public SimpleRedisLock(String name, StringRedisTemplate stringRedisTemplate) { this.name = name; this.stringRedisTemplate = stringRedisTemplate; } @Override public boolean tryLock(long timeoutSec) { String threadId = ID_PREFIX + Thread.currentThread().getId(); Boolean success = stringRedisTemplate .opsForValue().setIfAbsent(KEY_PREFIX + name, threadId, timeoutSec, TimeUnit.SECONDS); // 避免拆箱产生空指针 return Boolean.TRUE.equals(success); } @Override public void unlock() { //获取线程标识 String threadId = ID_PREFIX + Thread.currentThread().getId(); //获取锁里面存的标识 String id = stringRedisTemplate.opsForValue().get(KEY_PREFIX + name); if(threadId.equals(id)){ //释放锁 stringRedisTemplate.delete(KEY_PREFIX + name); } } } 这样就完美了吗？还不是！！！ 假设线程1已经获取了锁，在判断标识一致之后，准备释放锁的时候，又出现了阻塞（例如JVM垃圾回收机制） 于是锁的TTL到期了，自动释放了 那么现在线程2趁虚而入，拿到了一把锁 但是线程1的逻辑还没执行完，那么线程1就会执行删除锁的逻辑（之前已经判断完了） 但是在阻塞前线程1已经判断了标识一致，所以现在线程1把线程2的锁给删了 那么就相当于判断标识那行代码没有起到作用 这就是删锁时的原子性问题 因为线程1的拿锁，判断标识，删锁，不是原子操作，所以我们要防止刚刚的情况 Lua脚本解决原子性 Redis提供了Lua脚本功能，在一个脚本中编写多条Redis命令，确保多条命令执行时的原子性。 -- 这里的KEYS[1]就是传入锁的key -- 这里的ARGV[1]就是线程标识 -- 比较锁中的线程标识与线程标识是否一致 if (redis.call('get', KEYS[1]) == ARGV[1]) then -- 一致则释放锁 return redis.call('del', KEYS[1]) end return 0 用Redis命令来调用脚本 EVAL script numkeys key [key ...] arg [arg ...] EVAL \"return redis.call('set', KEYS[1], ARGV[1])\" 1 name Lucy public class SimpleRedisLock implements ILock{ private static final String KEY_PREFIX = \"lock:\"; private static final String ID_PREFIX = UUID.randomUUID().toString(true) + \"-\"; //业务名 private String name; //不是spring管理的类，不能自动给你注入 private StringRedisTemplate stringRedisTemplate; //lua脚本执行,静态变量加静态代码块，只会加载一次脚本 private static final DefaultRedisScript UNLOCK_SCRIPT; static { UNLOCK_SCRIPT = new DefaultRedisScript<>(); //加载资源 UNLOCK_SCRIPT.setLocation(new ClassPathResource(\"unlock.lua\")); UNLOCK_SCRIPT.setResultType(Long.class); } public SimpleRedisLock(String name, StringRedisTemplate stringRedisTemplate) { this.name = name; this.stringRedisTemplate = stringRedisTemplate; } @Override public boolean tryLock(long timeoutSec) { String threadId = ID_PREFIX + Thread.currentThread().getId(); Boolean success = stringRedisTemplate .opsForValue().setIfAbsent(KEY_PREFIX + name, threadId, timeoutSec, TimeUnit.SECONDS); // 避免拆箱产生空指针 return Boolean.TRUE.equals(success); } @Override public void unlock() { //获取线程标识 String threadId = ID_PREFIX + Thread.currentThread().getId(); //调用lua脚本 stringRedisTemplate.execute(UNLOCK_SCRIPT, Collections.singletonList(KEY_PREFIX + name), threadId); } 总结 实现思路： 利用set nx ex获取锁，并设置过期时间，保存线程标示 释放锁时先判断线程标示是否与自己一致，一致则删除锁。（Lua脚本保持原子性） Redis分布式锁特性： 利用set nx满足互斥性 利用set ex保证故障时锁依然能释放，避免死锁，提高安全性 利用Redis集群保证高可用和高并发特性 可重入锁与不可重入锁 可重入锁：一个线程可以多次获得同一把锁，而不会导致死锁。每当线程获取锁时，它的重入计数增加；在释放锁时，计数减少，当计数为零时，锁才真正释放。 可以防止因同一线程未释放锁而引发的死锁问题。 Java 中的 ReentrantLock和synchronized 就是一个可重入锁。 import java.util.concurrent.locks.ReentrantLock; public class ReentrantLockExample { private final ReentrantLock lock = new ReentrantLock(); public void methodA() { lock.lock(); try { methodB(); // 可以重入 } finally { lock.unlock(); } } public void methodB() { lock.lock(); // 允许重入 try { // do something } finally { lock.unlock(); } } } 不可重入锁：不可重入锁是指一个线程如果已经持有该锁，就无法再获得同一把锁。如果线程试图再次获取锁，就会导致死锁。（因为自己的业务没有完成，不会释放锁，又一直等待锁的释放） Redisson分布式锁 自己锁存在的问题 基于SETNX实现的分布式锁存在以下问题 重入问题 重入问题是指获取锁的线程，可以再次进入到相同的锁的代码块中，可重入锁的意义在于防止死锁，例如在HashTable这样的代码中，它的方法都是使用synchronized修饰的，加入它在一个方法内调用另一个方法，如果此时是不可重入的，那就死锁了。所以可重入锁的主要意义是防止死锁，我们的synchronized和Lock锁都是可重入的 不可重试 我们编写的分布式锁只能尝试一次，失败了就返回false，没有重试机制。但合理的情况应该是：当线程获取锁失败后，他应该能再次尝试获取锁。因此有些业务场景就不适合了，但刚才我们的一人一单是适合的。 超时释放 我们在加锁的时候增加了TTL，这样我们可以防止死锁，但是如果卡顿(阻塞)时间太长，超时也会导致锁的释放。虽然我们采用Lua脚本来防止删锁的时候，误删别人的锁，但问题是自己的锁没锁住，也有安全隐患。如果超时时间太长，自己宕机了，别人得等超时才能拿到锁，浪费时间。超时时间是个问题。 主从一致性 如果Redis提供了主从集群，那么当我们向集群写数据时，主机需要异步的将数据同步给从机，万一在同步之前，主机宕机了(主从同步存在延迟，虽然时间很短，但还是发生了)，那么又会出现线程不安全问题，因为其他线程在从机里没有看到锁。实际上发生概率极低，因为同步延迟很低。 什么是Redisson ​ Redisson是一个在Redis的基础上实现的Java驻内存数据网格(In-Memory Data Grid)。它不仅提供了一系列的分布式Java常用对象，还提供了许多分布式服务，其中就包含了各种分布式锁的实现： 可重入锁(Reentrant Lock) 公平锁(Fair Lock) 联锁(MultiLock) 红锁(RedLock) 读写锁(ReadWriteLock) 信号量(Semaphore) 可过期性信号量(PermitExpirableSemaphore) 闭锁(CountDownLatch) 使用方法 加Redisson依赖 org.redisson redisson 3.13.6 配置Redisson客户端（连接地址，密码） @Configuration public class RedissonConfig { @Bean public RedissonClient redissonClient(){ Config config = new Config(); config.useSingleServer().setAddress(\"redis://localhost:6379\").setPassword(\"123\"); return Redisson.create(config); } } 使用锁（获取锁对象、尝试获取锁、释放锁） 参数解释： waitTime 指的是线程在尝试获取锁时所愿意等待的最大时间。如果在这个时间内，锁仍然被其他线程持有，则该线程将放弃获取锁并返回 false。 leaseTime 是指在成功获取到锁后，锁的有效时间（TTL）。在这个时间内，持有锁的线程必须完成其操作或者显式释放锁，否则锁会自动过期并被释放。 三种使用方法： 获取锁，如果什么都不传，则是获取锁失败后直接返回false，默认锁自动释放时间是30S； 如果传三个参数，第一个参数是waitTime等待时间，第二个是leaseTime（expire设置到redsi中），第三个是时间单位。 如果传入两个参数，第一个参数是waitTime等待时间，第二个参数是单位。leasetime为-1（锁自动释放时间为看门狗释放时间=30s） 注意： 只有当你使用 tryLock(long waitTime, long leaseTime, TimeUnit unit) 并将 leaseTime 设置为 -1 时，看门狗机制才会被激活，从而实现自动续约锁的功能。 RLock lock = redissonClient.getLock(\"lock:order:\" + userId); //第一个是等待时间，第二个是超市释放时间，第三个是时间单位 //无参数的话，第一个是默认不等待，第二个是30s boolean isLock = lock.tryLock(); if(!isLock){ //代表一个人在开挂抢！ return Result.fail(\"不允许重复下单！\"); } try { IVoucherOrderService proxy = (IVoucherOrderService) AopContext.currentProxy(); return proxy.createVoucherOrder(voucherId); } catch (IllegalStateException e) { throw new RuntimeException(e); } finally { lock.unlock(); } 原理 可重入 ​ 采用redis的hash结构来存储锁，其中外层key表示这把锁是否存在，内层key则记录当前这把锁被哪个线程持有，value存当前这个锁被获取了几次。 ​ 获取锁，value++，释放锁，value--，这二者都需要先判断这个锁是不是自己的。 ​ 获取和释放锁都需要重置更新次数，给业务留足够的时间。 ​ 为了保证原子性，所以流程图中的业务逻辑也是需要我们用Lua来实现的。 获取锁： 释放锁： 如果判断出锁不是自己的，说明自己业务超时了，锁被释放了，不用管就行了，防止误删别人的锁。 可重试 获取锁成功，返回nil（java里面接受的是null） 获取锁失败，返回剩余到期时间 利用了信号量和订阅，不是盲目尝试，是有别人释放锁了才尝试（如果此时有等待时间和锁未超时） public boolean tryLock(long waitTime, long leaseTime, TimeUnit unit) throws InterruptedException { long time = unit.toMillis(waitTime); long current = System.currentTimeMillis(); long threadId = Thread.currentThread().getId(); Long ttl = this.tryAcquire(waitTime, leaseTime, unit, threadId); //判断ttl是否为null if (ttl == null) { //获取锁成功 return true; } else { //计算当前时间与获取锁时间的差值，让等待时间减去这个值 time -= System.currentTimeMillis() - current; //如果消耗时间太长了，直接返回false，获取锁失败 if (time subscribeFuture = this.subscribe(threadId); //在剩余时间内，没等到 if (!subscribeFuture.await(time, TimeUnit.MILLISECONDS)) { if (!subscribeFuture.cancel(false)) { subscribeFuture.onComplete((res, e) -> { if (e == null) { //取消订阅 this.unsubscribe(subscribeFuture, threadId); } }); } this.acquireFailed(waitTime, unit, threadId); return false; } else { try { //如果剩余时间内等到了别人释放锁的信号，再次计算当前剩余最大等待时间 time -= System.currentTimeMillis() - current; if (time = 0L && ttl 0L); this.acquireFailed(waitTime, unit, threadId); var16 = false; return var16; } } finally { this.unsubscribe(subscribeFuture, threadId); } } } } } 超时续约 为什么需要超时释放？ 如果某个操作耗时较长，而你设置的锁的过期时间（leaseTime）太短，操作在完成之前锁就会自动释放，其他线程或进程可能会获取到锁，从而导致并发访问共享资源，出现数据不一致或竞态条件。 如果你为了避免锁过期问题，将锁的持有时间设置得很长，万一持锁的节点意外宕机或网络中断，锁就可能会被“永久占用”，导致其他线程/进程无法获取锁，最终导致系统死锁或效率极低。 解决方案是通过设置合理的锁过期时间并配合自动续约机制： 锁的默认过期时间可以较短，例如 30秒。 如果线程正常执行任务并需要长时间持有锁，看门狗机制会自动续约，确保锁不会意外释放。 但如果持锁的线程异常退出或崩溃，锁将不会被续约，最终自动释放，避免死锁。 因此，我们应该定时刷新redis中存的有效期。 这时候看门狗就出现了，前提是leaseTime为-1，它会每隔 10 秒对锁进行续约，续约时间为 30 秒，直到锁被释放为止。 EXPIRATION_RENEWAL_MAP：这是一个 Map 类型的集合，用于存储所有活跃的锁和它们的续约信息。键是锁的名字，值是 ExpirationEntry 对象。 ExpirationEntry：保存与锁相关的线程 ID 和续约信息的对象，用来标识某个锁的线程是否已经设置了自动续约任务。 threadId：表示当前线程的 ID。 如果 oldEntry 不为空，说明 EXPIRATION_RENEWAL_MAP 中已经有了与该锁对应的续约任务。这通常发生在线程重入的情况下，即同一个线程再次获取同一个锁。 因为这个锁已经有自动续约任务在运行，因此无需重复启动新的定时任务，只需将当前线程的 threadId 添加到 oldEntry 中，表示该线程重入锁。 private void scheduleExpirationRenewal(long threadId) { ExpirationEntry entry = new ExpirationEntry(); //不存在，才put，表明是第一次进入，不是重入 ExpirationEntry oldEntry = (ExpirationEntry)EXPIRATION_RENEWAL_MAP.putIfAbsent(this.getEntryName(), entry); if (oldEntry != null) { //线程重入，不用再次调定时任务了，因为任务已经在重复运行了 oldEntry.addThreadId(threadId); } else { //如果是第一次进入，则开启一个定时任务 entry.addThreadId(threadId); this.renewExpiration(); } } private void renewExpiration() { ExpirationEntry ee = (ExpirationEntry)EXPIRATION_RENEWAL_MAP.get(this.getEntryName()); if (ee != null) { //Timeout是一个定时任务 Timeout task = this.commandExecutor.getConnectionManager().newTimeout(new TimerTask() { public void run(Timeout timeout) throws Exception { ExpirationEntry ent = (ExpirationEntry)RedissonLock.EXPIRATION_RENEWAL_MAP.get(RedissonLock.this.getEntryName()); if (ent != null) { Long threadId = ent.getFirstThreadId(); if (threadId != null) { //重置有效期 RFuture future = RedissonLock.this.renewExpirationAsync(threadId); future.onComplete((res, e) -> { if (e != null) { RedissonLock.log.error(\"Can't update lock \" + RedissonLock.this.getName() + \" expiration\", e); } else { if (res) { //然后调用自己，递归重置有效期 RedissonLock.this.renewExpiration(); } } }); } } } //internalLockLeaseTime是之前WatchDog默认有效期30秒，那这里就是 30 / 3 = 10秒之后，才会执行 }, this.internalLockLeaseTime / 3L, TimeUnit.MILLISECONDS); ee.setTimeout(task); } } protected RFuture renewExpirationAsync(long threadId) { return this.evalWriteAsync(this.getName(), LongCodec.INSTANCE, RedisCommands.EVAL_BOOLEAN, \"if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then redis.call('pexpire', KEYS[1], ARGV[1]); return 1; end; return 0;\", Collections.singletonList(this.getName()), this.internalLockLeaseTime, this.getLockName(threadId)); } 这个任务什么时候停止？自然是释放锁的时候停止 void cancelExpirationRenewal(Long threadId) { //将之前的线程终止掉 ExpirationEntry task = (ExpirationEntry)EXPIRATION_RENEWAL_MAP.get(this.getEntryName()); if (task != null) { if (threadId != null) { task.removeThreadId(threadId); } if (threadId == null || task.hasNoThreads()) { //获取之前的定时任务 Timeout timeout = task.getTimeout(); if (timeout != null) { //取消 timeout.cancel(); } EXPIRATION_RENEWAL_MAP.remove(this.getEntryName()); } } } 可重试和超时续约总结 这里的获取失败，是指别的线程占用了锁，不是自己的线程，因为可重入保证了自己线程占用也可以获取成功。 释放锁，也是指可重入意义上的释放锁，也就是value为0。 Redisson分布式锁原理 可重入：利用hash结构记录线程id和重入次数 可重试：利用信号量和PubSub功能实现等待、唤醒，获取锁失败的重试机制，不浪费cpu资源 超时续约：利用watchD0g，每隔一段时间(releaseTime/3)，重置超时时间，也就是重新expire。 主从一致性 ​ 在 Redis 主从架构下，锁通常是创建在主节点上的，因为主节点负责写操作。从节点则主要用于读操作。如果主节点宕机并且发生了主从切换（即从节点被提升为新的主节点），可能会出现以下问题： 锁丢失：主节点上创建的锁无法及时同步到从节点。在主节点宕机之前的锁状态没有被复制，从节点不知道锁的存在，导致其他客户端可能错误地认为没有锁存在。 数据不一致：由于 Redis 主从复制是异步的，因此在主节点发生故障之前，锁的状态可能尚未完全同步到从节点，造成分布式锁的不可用或错误释放 解决方案：联锁 RedLock 通过在多个 Redis 实例（通常是 5 个）上同时获取锁，来确保即使某个 Redis 实例宕机，锁依然安全有效。 我们先使用虚拟机额外搭建两个Redis节点。 配置redis。 @Configuration public class RedissonConfig { @Bean public RedissonClient redissonClient(){ Config config = new Config(); config.useSingleServer().setAddress(\"redis://localhost:6379\").setPassword(\"123\"); return Redisson.create(config); } @Bean public RedissonClient redissonClient2(){ Config config = new Config(); config.useSingleServer().setAddress(\"redis://192.168.140.132:6379\").setPassword(\"123\"); return Redisson.create(config); } } 使用MultiLock @Resource private RedissonClient redissonClient; @Resource private RedissonClient redissonClient2; private RLock lock; @BeforeEach void setUp() { RLock lock1 = redissonClient.getLock(\"lock\"); RLock lock2 = redissonClient2.getLock(\"lock\"); //创建联锁 lock = redissonClient.getMultiLock(lock1, lock2); } @Test void method1() throws InterruptedException { boolean success = lock.tryLock(1, TimeUnit.SECONDS); if (!success) { log.error(\"获取锁失败，1\"); return; } try { log.info(\"获取锁成功，1\"); Thread.sleep(200000); method2(); } finally { log.info(\"释放锁，1\"); lock.unlock(); } } void method2() { boolean success = lock.tryLock(); if (!success) { log.error(\"获取锁失败，2\"); return; } try { log.info(\"获取锁成功，2\"); } finally { log.info(\"释放锁，2\"); lock.unlock(); } } 可变参数，把你加入的全部Lock放入集合中 public RedissonMultiLock(RLock... locks) { if (locks.length == 0) { throw new IllegalArgumentException(\"Lock objects are not defined\"); } else { //可变参数，把参数放入集合中 this.locks.addAll(Arrays.asList(locks)); } } public boolean tryLock(long waitTime, long leaseTime, TimeUnit unit) throws InterruptedException { long newLeaseTime = -1L; //如果传入了释放时间 if (leaseTime != -1L) { //再判断一下是否有等待时间 if (waitTime == -1L) { //如果没传等待时间，不重试，则只获得一次 newLeaseTime = unit.toMillis(leaseTime); } else { //想要重试，耗时较久，万一释放时间小于等待时间，则会有问题，所以这里将等待时间乘以二 newLeaseTime = unit.toMillis(waitTime) * 2L; } } //获取当前时间 long time = System.currentTimeMillis(); //剩余等待时间 long remainTime = -1L; if (waitTime != -1L) { remainTime = unit.toMillis(waitTime); } //锁等待时间，与剩余等待时间一样 long lockWaitTime = this.calcLockWaitTime(remainTime); //锁失败的限制，源码返回是的0 int failedLocksLimit = this.failedLocksLimit(); //已经获取成功的锁 List acquiredLocks = new ArrayList(this.locks.size()); //迭代器，用于遍历 ListIterator iterator = this.locks.listIterator(); while(iterator.hasNext()) { RLock lock = (RLock)iterator.next(); boolean lockAcquired; try { //没有等待时间和释放时间，调用空参的tryLock if (waitTime == -1L && leaseTime == -1L) { lockAcquired = lock.tryLock(); } else { //否则调用带参的tryLock long awaitTime = Math.min(lockWaitTime, remainTime); //之前看过了 lockAcquired = lock.tryLock(awaitTime, newLeaseTime, TimeUnit.MILLISECONDS); } } catch (RedisResponseTimeoutException var21) { this.unlockInner(Arrays.asList(lock)); lockAcquired = false; } catch (Exception var22) { lockAcquired = false; } //判断获取锁是否成功 if (lockAcquired) { //成功则将锁放入成功锁的集合 acquiredLocks.add(lock); } else { //如果获取锁失败 //判断当前锁的数量，减去成功获取锁的数量，如果为0，则所有锁都成功获取，跳出循环 if (this.locks.size() - acquiredLocks.size() == this.failedLocksLimit()) { break; } //否则将拿到的锁都释放掉，全部推到重来 if (failedLocksLimit == 0) { this.unlockInner(acquiredLocks); //如果等待时间为-1，则不想重试，直接返回false if (waitTime == -1L) { return false; } failedLocksLimit = this.failedLocksLimit(); //将已经拿到的锁都清空 acquiredLocks.clear(); //将迭代器往前迭代，相当于重置指针，放到第一个然后重试获取锁 while(iterator.hasPrevious()) { iterator.previous(); } } else { --failedLocksLimit; } } //如果剩余时间不为-1，很充足 if (remainTime != -1L) { //计算现在剩余时间 remainTime -= System.currentTimeMillis() - time; time = System.currentTimeMillis(); //如果剩余时间为负数，则获取锁超时了 if (remainTime > futures = new ArrayList(acquiredLocks.size()); //迭代器用于遍历已经获取成功的锁 Iterator var24 = acquiredLocks.iterator(); while(var24.hasNext()) { RLock rLock = (RLock)var24.next(); //设置每一把锁的有效期，为什么需要设置？因为没使用看门狗 RFuture future = ((RedissonLock)rLock).expireAsync(unit.toMillis(leaseTime), TimeUnit.MILLISECONDS); futures.add(future); } var24 = futures.iterator(); while(var24.hasNext()) { RFuture rFuture = (RFuture)var24.next(); rFuture.syncUninterruptibly(); } } //但如果没设置有效期，则会触发WatchDog机制，自动帮我们设置有效期，所以大多数情况下，我们不需要自己设置有效期 return true; } 再次总结 不可重入Redis分布式锁: 原理:利用setnx的互斥性;利用ex避免死锁;释放锁时判断线程标示 缺陷:不可重入、无法重试、锁超时失效 可重入的Redis分布式锁: 原理:利用hash结构，记录线程标示和重入次数;利用watchDog延续锁时间;利用信号量控制锁重试等待 缺陷:redis宕机引起锁失效问题 Redisson的multiLock: 原理:多个独立的Redis节点，必须在所有节点都获取重入锁，才算获取锁成功 缺陷:运维成本高、实现复杂 其中multiLock可以看做多个Redis分布式锁。 Redis优化秒杀 原逻辑：速度慢 一个线程串行执行 很多操作都是要去操作数据库的 使用了悲观锁：分布式锁 核心：读写分离 ​ 读取的操作比较快，写入的操作比较慢，我们就可以分开处理。 ​ 我们将耗时较短的查询逻辑判断放到Redis中，例如：库存是否充足，是否一人一单这样的操作，只要满足这两条操作，那我们是一定可以下单成功的，不用等数据真的写进数据库，因为这样实际上用户已经完成了下单，他获得了订单号，只要去支付就行了。 ​ 然后后台再开一个线程，后台线程再去读取阻塞队列里的消息，执行操作慢的数据库写操作。 ​ 库存可以用String存，可以用Set存这个优惠卷被哪些用户下了单。 ​ 因为这是多个操作，需要保证操作的原子性，可以使用Lua脚本。 新增秒杀优惠券的同时，将优惠券信息保存到Redis中 基于Lua脚本，判断秒杀库存、一人一单，决定用户是否抢购成功 如果抢购成功，将优惠券id和用户id封装后存入阻塞队列 开启线程任务，不断从阻塞队列中获取信息，实现异步下单功能 lua脚本 -- 订单id local voucherId = ARGV[1] -- 用户id local userId = ARGV[2] -- 优惠券key local stockKey = 'seckill:stock:' .. voucherId -- 订单key local orderKey = 'seckill:order:' .. voucherId -- 判断库存是否充足 if (tonumber(redis.call('get', stockKey)) @Service public class VoucherOrderServiceImpl extends ServiceImpl implements IVoucherOrderService { @Resource private ISeckillVoucherService seckillVoucherService; @Resource private RedisIdWorker redisIdWorker; @Autowired private StringRedisTemplate stringRedisTemplate; @Resource private RedissonClient redissonClient; private BlockingQueue orderTasks = new ArrayBlockingQueue<>(1024 * 1024); private static final ExecutorService SECKILL_ORDER_EXECUTOR = Executors.newSingleThreadExecutor(); //秒杀业务需要在类初始化之后，就立即执行 @PostConstruct private void init(){ //开启了独立的子线程 SECKILL_ORDER_EXECUTOR.submit(new VoucherOrderHandler()); } private IVoucherOrderService proxy; private class VoucherOrderHandler implements Runnable{ @Override public void run() { while(true){ try { //如果队列为空就会等待 VoucherOrder voucherOrder = orderTasks.take(); proxy.createVoucherOrder(voucherOrder); } catch (InterruptedException e) { throw new RuntimeException(\"处理订单异常\"); } } } } @Transactional //单线程不会发生并发问题，即使是多线程，只执行update语句也不会有问题 public void createVoucherOrder(VoucherOrder voucherOrder) { // 扣减库存,乐观锁 seckillVoucherService.update() .setSql(\"stock = stock - 1\") .eq(\"voucher_id\", voucherOrder.getVoucherId()) .gt(\"stock\",0) .update(); // 创建订单 save(voucherOrder); } //lua脚本执行,静态变量加静态代码块，只会加载一次脚本 private static final DefaultRedisScript SECKILL_SCRIPT; static { SECKILL_SCRIPT = new DefaultRedisScript<>(); //加载资源 SECKILL_SCRIPT.setLocation(new ClassPathResource(\"seckill.lua\")); SECKILL_SCRIPT.setResultType(Long.class); } @Override public Result seckillVoucher(Long voucherId) { Long userId = UserHolder.getUser().getId(); //1执行脚本,第一个参数是脚本，第二个参数是KEY的list结合，第三个参数是可变参数（其他变量） Long isSuccess = stringRedisTemplate.execute( SECKILL_SCRIPT, Collections.emptyList(), voucherId.toString(), userId.toString() ); //2如果返回的不是0，表示下单失败 if(isSuccess == 1L){ return Result.fail(\"库存不足！\"); } else if(isSuccess == 2L){ return Result.fail(\"不允许重复下单！\"); } long orderId = redisIdWorker.nextId(\"order\"); //3 订单加入到阻塞队列 VoucherOrder voucherOrder = new VoucherOrder(orderId, userId, voucherId); //子线程看不到代理对象，因此把代理对象弄成成员变量。 proxy = (IVoucherOrderService) AopContext.currentProxy(); orderTasks.add(voucherOrder); return Result.ok(orderId); } } 存在的问题？ 内存限制问题：我们现在使用的是JDK里的阻塞队列，它使用的是JVM的内存，如果在高并发的条件下，无数的订单都会放在阻塞队列里，可能就会造成内存溢出，所以我们在创建阻塞队列时，设置了一个长度，但是如果真的存满了，再有新的订单来往里塞，那就塞不进去了，存在内存限制问题 数据安全问题：服务器宕机了，JVM没有持久化机制，阻塞队列数据消失，用户明明下单了，但是数据库里没看到 消息队列优化秒杀 什么是消息队列 什么是消息队列？字面意思就是存放消息的队列，最简单的消息队列模型包括3个角色 消息队列：存储和管理消息，也被称为消息代理（Message Broker） 生产者：发送消息到消息队列 消费者：从消息队列获取消息并处理消息 消息队列还应该： 不受JVM内存限制，可以存很多东西。 有持久化机制，服务宕机还是重启，数据不会消失。 有消息确认机制，没有确认的消息会在队列一直存在，直到消费者确认。 有序性，先来的消息要先处理。 ​ 使用消息队列的好处在于解耦：举个例子，快递员(生产者)把快递放到驿站/快递柜里去(Message Queue)去，我们(消费者)从快递柜/驿站去拿快递，这就是一个异步，如果耦合，那么快递员必须亲自上楼把快递递到你手里，服务当然好，但是万一我不在家，快递员就得一直等我，浪费了快递员的时间。所以解耦还是非常有必要的。 那么在这种场景下我们的秒杀就变成了：在我们下单之后，利用Redis去进行校验下单的结果，然后在通过队列把消息发送出去，然后在启动一个线程去拿到这个消息，完成解耦，同时也加快我们的响应速度 这里我们可以直接使用一些现成的(MQ)消息队列，如kafka，rabbitmq等，但是如果没有安装MQ，我们也可以使用Redis提供的MQ方案(学完Redis我就去学微服务) Redis提供了三种不同的方式来实现消息队列: list结构:基于List结构模拟消息队列 Pubsub:基本的点对点消息模型 Stream:比较完善的消息队列模型 List模拟消息队列 优点 利用Redis存储，不受限于JVM内存上限 基于Redis的持久化机制，数据安全性有保障 可以满足消息有序性 缺点 无法避免消息丢失，你取走了消息，List数据没了，但是你挂了，消息永远也没了 只支持单消费者(一个消费者把消息拿走了，其他消费者就看不到这条消息了) PubSub消息队列 优点： 采用发布订阅模型，支持多生产，多消费 缺点： 不支持数据持久化，list支持是因为list本身就是一个数据结构。 无法避免消息丢失（如果向频道发送了消息，却没有人订阅该频道，那发送的这条消息就丢失了） 消息堆积有上限（消费者的缓存（内存）是有限的） Stream的单消费者模式 Stream是新数据类型 发送消息： 默认是，不阻塞读取，读不到就返回nal STREAM类型消息队列的XREAD命令特点 消息可回溯，读取后不消失，永久的保存在队列中 一个消息可以被多个消费者读取 可以阻塞读取 有漏读消息的风险 Stream的消费者组 消息分流 队列中的消息会分留给组内的不同消费者，而不是重复消费者，从而加快消息处理的速度。如果想重复消费，可以创建多个消费者组。 消息标识 消费者组会维护一个标识，记录组内最后一个被处理的消息，哪怕消费者宕机重启，还会从标识之后读取消息，确保每一个消息都会被消费。注意组内只会维护一个标记。 消息确认 消费者获取消息后，消息处于pending状态，并存入一个pending-list，当处理完成后，需要通过XACK来确认消息，标记消息为已处理，才会从pending-list中移除。这样即使宕机，也可以在pending-list中读取消息。每个消费者有自己的pending-list。消费者组也会维护一个共同的pending-list。 创建消费者组 可以看出流（队列）和消费者组是多对多的关系 XGROUP CREATE key groupName ID [MKSTREAM] XGROUP create s1 g1 0 MKSTREAM key：流的名称 groupName：消费者组名称 ID：起始ID标识，代表队列中的最后一个消息，0代表队列中的第一个消息。如果队列之前有消息，不想要可以选择使用$。 MKSTREAM：队列不存在时自动创建队列 删除消费者组 XGROUP DESTORY key groupName XGROUP DESTROY s1 g1 key：流的名称 groupName：消费者组名称 向队列添加消息 XADD key [NOMKSTREAM] [MAXLEN|MINID [=|~] threshold [LIMIT count]] *|ID field value [field value ...] XADD s1 * k1 v1 key：队列名称 NOMKSTREAM:队列没有的话自动创建队列，默认没有队列自动创建。 [MAXLEN|MINID [=|~] threshold [LIMIT count]] ：设置消息队列的最大消息数量，不设置则无上限 *|ID：*代表由Redis自动生成。格式是”时间戳-递增数字”，例如”114514114514-0” field value [field value …]: 发送到队列中的消息，称为Entry。格式就是多个key-value键值对 从消费者组读取消息 XREADGROUP GROUP group consumer [COUNT count] [BLOCK milliseconds] [NOACK] STREAMS key [keys ...] ID [ID ...] XREADGROUP group g1 c1 count 1 block 200 STREAMS s1 > group：消费者组名称 consumer：消费者名，如果消费者不存在，会自动创建一个消费者 count：本次查询的最大消息数量 BLOCK milliseconds：当前没有消息时的最大等待时间 NOACK：无需手动ACK，获取到消息后自动确认（一般不用，我们都是手动确认） STREAMS key：指定队列名称 ID获取消息的起始ID >：从下一个未消费的消息开始，正常情况下用这个。 其他：根据指定id从pending-list中获取已消费但未确认的消息，例如0，是从pending-list中的第一个消息开始，异常情况下从这里开始。 专门读取未确认的消息 XPENDING group [[IDLE min-idle-time] start end count [consumer]] XPENDING g1 - + 10 group：消费者组 start end：id范围，如果是 - +，表示所有范围我都要 count：取几个消息 consumer： 消费者，不指定就是整个消费者组的未确认消息 确认消息 XACK key group ID [ID ...] XACK s1 g1 1729558781707-0 key：队列名称 group：消费者组 ID：消息的id 伪代码 优点 消息可回溯 可以多消费者争抢消息，加快消费速度 可以阻塞读取 没有消息漏读的风险 有消息确认机制，保证消息至少被消费一次 缺点 相比于专业的消息中间件 依赖于Redis，不能保证万无一失。 没有生产者确认机制。 -- 订单id local voucherId = ARGV[1] -- 用户id local userId = ARGV[2] -- 新增orderId，但是变量名用id就好，因为VoucherOrder实体类中的orderId就是用id表示的 local id = ARGV[3] -- 优惠券key local stockKey = 'seckill:stock:' .. voucherId -- 订单key local orderKey = 'seckill:order:' .. voucherId -- 判断库存是否充足 if (tonumber(redis.call('get', stockKey)) private class VoucherOrderHandler implements Runnable{ @Override public void run() { while(true){ try { //1. 获取消息队列中的订单信息 XREADGROUP GROUP g1 c1 COUNT 1 BLOCK 2000 STREAMS stream.orders > List> list = stringRedisTemplate.opsForStream().read( Consumer.from(\"g1\", \"c1\"), StreamReadOptions.empty().count(1).block(Duration.ofSeconds(2)), StreamOffset.create(\"stream.orders\", ReadOffset.lastConsumed()) ); //2 获取失败，继续下次循环 if(list == null || list.isEmpty()){ continue; } handleVoucherOrder(list); } catch (Exception e) { log.error(\"处理消息异常！\"); handlePendingList(); } } } private void handlePendingList() { while(true){ try { //1. 获取pending-list中的订单信息 XPENDING stream.orders g1 - + 10 List> list = stringRedisTemplate.opsForStream().read( Consumer.from(\"g1\", \"c1\"), StreamReadOptions.empty().count(1), StreamOffset.create(\"stream.orders\", ReadOffset.from(\"0\")) ); //2 没有消息，结束 if(list == null || list.isEmpty()){ break; } //3 有消息，处理消息 handleVoucherOrder(list); } catch (Exception e) { log.error(\"处理pending-list消息异常！\", e); //休眠一会 try { Thread.sleep(50); } catch (InterruptedException ex) { throw new RuntimeException(ex); } } } } private void handleVoucherOrder(List> list) { //因为我们只取一个消息 MapRecord record = list.get(0); //消息的id RecordId recordId = record.getId(); //我们放进去的键值对 Map value = record.getValue(); //处理消息：创建订单 VoucherOrder voucherOrder = BeanUtil.fillBeanWithMap(value, new VoucherOrder(), true); proxy.createVoucherOrder(voucherOrder); //ack确认!! stringRedisTemplate.opsForStream().acknowledge(\"stream.orders\", \"g1\", recordId); } } 达人探店 发布探店笔记 @PostMapping public Result saveBlog(@RequestBody Blog blog) { // 获取登录用户 UserDTO user = UserHolder.getUser(); blog.setUserId(user.getId()); // 保存探店博文 blogService.save(blog); // 返回id return Result.ok(blog.getId()); } 查看探店笔记 public Result queryBlogById(Long id) { Blog blog = this.getById(id); if(blog == null){ return Result.fail(\"笔记不存在！\"); } // 查询blog有没有被当前用户点，赋值userIcon和name queryBlogUser(blog); // 查询blog有没有被当前用户点赞，赋值isLiked isBlogLiked(blog); return Result.ok(blog); } private void queryBlogUser(Blog blog) { Long userId = blog.getUserId(); User user = userService.getById(userId); blog.setName(user.getNickName()); blog.setIcon(user.getIcon()); } 点赞功能 需求 同一个用户只能对同一篇笔记点赞一次，再次点击则取消点赞 如果当前用户已经点赞，则点赞按钮高亮显示（前端已实现，判断字段Blog类的isLike属性） 实现步骤 修改点赞功能，利用Redis中的set集合来判断是否点赞过，未点赞则点赞数+1，已点赞则点赞数-1 修改根据id查询的业务，判断当前登录用户是否点赞过，赋值给isLike字段 修改分页查询Blog业务，判断当前登录用户是否点赞过，赋值给isLike字段 为什么不用数据库，加一张表，存着是店铺id和用户id？ 没必要，对数据库压力大，redis就可以。 点赞功能： public Result likeBlog(Long id) { Long userId = UserHolder.getUser().getId(); String key = RedisConstants.BLOG_LIKED_KEY + id; // 1.判断当前登录用户有没有点赞 Boolean isLike = stringRedisTemplate.opsForSet(). isMember(key, userId.toString()); // 2已经点赞了，取消赞 if(BooleanUtil.isTrue(isLike)){ // 2.1修改数据库 boolean isSuccess = update().setSql(\"liked = liked - 1\").eq(\"id\", id).update(); // 2.2redis的set集合中删除 if(isSuccess){ stringRedisTemplate.opsForSet().remove(key, userId.toString()); } } else{ // 3 没有点赞 // 3.1修改数据库 boolean isSuccess = update().setSql(\"liked = liked + 1\").eq(\"id\", id).update(); // 3.2增加到redis if(isSuccess){ stringRedisTemplate.opsForSet().add(key, userId.toString()); } } return Result.ok(); } 查询blog功能 @Override public Result queryBlogById(Long id) { Blog blog = this.getById(id); if(blog == null){ return Result.fail(\"笔记不存在！\"); } // 查询blog有没有被当前用户点，赋值userIcon和name queryBlogUser(blog); // 查询blog有没有被当前用户点赞，赋值isLiked isBlogLiked(blog); return Result.ok(blog); } private void isBlogLiked(Blog blog){ Long userId = UserHolder.getUser().getId(); String key = RedisConstants.BLOG_LIKED_KEY + blog.getId(); // 1.判断当前登录用户有没有点赞 Boolean isLike = stringRedisTemplate.opsForSet(). isMember(key, userId.toString()); // 2.根据点赞信息赋值 blog.setIsLike(BooleanUtil.isTrue(isLike)); } 点赞排行榜 按照时间排序:把之前的set改为sortset。 private void isBlogLiked(Blog blog){ UserDTO userDTO = UserHolder.getUser(); if(userDTO == null){ //用户未登录，无需查询 return; } Long userId = userDTO.getId(); String key = RedisConstants.BLOG_LIKED_KEY + blog.getId(); // 1.判断当前登录用户有没有点赞 Double score = stringRedisTemplate.opsForZSet().score(key, userId.toString()); // 2.根据点赞信息赋值 blog.setIsLike(score != null); } public Result likeBlog(Long id) { Long userId = UserHolder.getUser().getId(); String key = RedisConstants.BLOG_LIKED_KEY + id; // 1.判断当前登录用户有没有点赞 Double score = stringRedisTemplate.opsForZSet().score(key, userId.toString()); // 2已经点赞了，取消赞 if(score != null){ // 2.1修改数据库 boolean isSuccess = update().setSql(\"liked = liked - 1\").eq(\"id\", id).update(); // 2.2redis的set集合中删除 if(isSuccess){ stringRedisTemplate.opsForZSet().remove(key, userId.toString()); } } else{ // 3 没有点赞 // 3.1修改数据库 boolean isSuccess = update().setSql(\"liked = liked + 1\").eq(\"id\", id).update(); // 3.2增加到redis if(isSuccess){ stringRedisTemplate.opsForZSet().add(key, userId.toString(), System.currentTimeMillis()); } } return Result.ok(); } 注意，mysql查询使用in语句 ，比如in(5,4,3,2,1)，mysql是按照1 2 3 4 5去查询。如果想按照给的顺序查询，需要: order by field(id, 5, 4, 3, 2, 1) public Result queryBlogLikes(Long id) { String key = RedisConstants.BLOG_LIKED_KEY + id; //前五名 Set top5 = stringRedisTemplate.opsForZSet().range(key, 0, 4); if(top5 == null || top5.isEmpty()){ return Result.ok(Collections.emptyList()); } List ids = top5.stream().map(Long::valueOf).collect(Collectors.toList()); //这里注意， where id in (5, 1) 会乱序，会按照你给定in集合的字典序排序，而不是你指定的顺序 //可以使用order by field(id, 5, 1)排序 // List users = userService.listByIds(ids); String idStr = StrUtil.join(\",\", ids); List users = userService.query() .in(\"id\", ids) .last(\"order by field ( id, \" + idStr + \")\") .list(); List userDTOS = BeanUtil.copyToList(users, UserDTO.class); return Result.ok(userDTOS); } 好友关注 关注 直接插入数据库就行，删除就是删除数据库 public Result follow(Long followUserId, Boolean isFollow) { Long userId = UserHolder.getUser().getId(); if(isFollow){ //新增关注 Follow follow = new Follow(userId, followUserId); save(follow); } else{ //取消关注 QueryWrapper queryWrapper = new QueryWrapper() .eq(\"user_id\", userId) .eq(\"follow_user_id\", followUserId); remove(queryWrapper); } return Result.ok(); } 共同关注 用redis中两个set求交集就可以。 public Result follow(Long followUserId, Boolean isFollow) { Long userId = UserHolder.getUser().getId(); String key = RedisConstants.FOLLOWS_KEY + userId; if(isFollow){ //新增关注 Follow follow = new Follow(userId, followUserId); boolean isSuccess = save(follow); if(isSuccess){ stringRedisTemplate.opsForSet().add(key, followUserId.toString()); } } else{ //取消关注 QueryWrapper queryWrapper = new QueryWrapper() .eq(\"user_id\", userId) .eq(\"follow_user_id\", followUserId); boolean isSuccess = remove(queryWrapper); if(isSuccess){ stringRedisTemplate.opsForSet().remove(key, followUserId.toString()); } } return Result.ok(); } public Result followCommons(Long id) { //1.获取当前用户 Long userId = UserHolder.getUser().getId(); //2.求交集 String key1 = RedisConstants.FOLLOWS_KEY + userId; String key2 = RedisConstants.FOLLOWS_KEY + id; Set intersect = stringRedisTemplate.opsForSet().intersect(key1, key2); //3.交集为空，返回空对象 if(intersect == null){ return Result.ok(Collections.emptyList()); } //4.解析用户id List userIds = intersect.stream().map(Long::valueOf).collect(Collectors.toList()); //5.查询用户 List users = userService.listByIds(userIds); //6.封装DTO List userDTOS = BeanUtil.copyToList(users, UserDTO.class); return Result.ok(userDTOS); } Feed流 两种模式 Timeline：不做内容筛选，简单的按照内容发布时间排序，常用于好友或关注(B站关注的up，朋友圈等) 优点：信息全面，不会有缺失，并且实现也相对简单 缺点：信息噪音较多，用户不一定感兴趣，内容获取效率低 智能排序：利用智能算法屏蔽掉违规的、用户不感兴趣的内容，推送用户感兴趣的信息来吸引用户 优点：投喂用户感兴趣的信息，用户粘度很高，容易沉迷 缺点：如果算法不精准，可能会起到反作用（给你推的你都不爱看） TimeLine的三种实现 拉模式 也叫读扩散， 该模式的核心含义是：当张三和李四、王五发了消息之后，都会保存到自己的发件箱中，如果赵六要读取消息，那么他会读取他自己的收件箱，此时系统会从他关注的人群中，将他关注人的信息全都进行拉取，然后进行排序 优点：比较节约空间，因为赵六在读取信息时，并没有重复读取，并且读取完之后，可以将他的收件箱清除 缺点：有延迟，当用户读取数据时，才会去关注的人的时发件箱中拉取信息，假设该用户关注了海量用户，那么此时就会拉取很多信息，对服务器压力巨大 推模式 也叫写扩散 推模式是没有写邮箱的，当张三写了一个内容，此时会主动把张三写的内容发送到它粉丝的收件箱中，假设此时李四再来读取，就不用再去临时拉取了 优点：时效快，不用临时拉取 缺点：内存压力大，假设一个大V发了一个动态，很多人关注他，那么就会写很多份数据到粉丝那边去 推拉结合 推拉模式是一个折中的方案。 站在发件人这一边，如果是普通人，那么我们采用写扩散的方式，直接把数据写入到他的粉丝收件箱中，因为普通人的粉丝数量较少，所以这样不会产生太大压力。 但如果是大V，那么他是直接将数据写入一份到发件箱中去，在直接写一份到活跃粉丝的收件箱中。 站在收件人这边来看，如果是活跃粉丝，那么大V和普通人发的都会写到自己的收件箱里。 但如果是普通粉丝，由于上线不是很频繁，所以等他们上线的时候，再从发件箱中去拉取信息。 我们的点评网，没有过千万粉丝，用户量少，可以使用推模式。 推送实现 public Result saveBlog(Blog blog) { //1.获取登录用户 UserDTO userDTO = UserHolder.getUser(); blog.setUserId(userDTO.getId()); //2.保存探店博客 boolean isSuccess = save(blog); if(!isSuccess){ return Result.fail(\"新增笔记失败\"); } //3查询当前登录用户的粉丝 List follows = followService.query().eq(\"follow_user_id\", userDTO.getId()).list(); for (Follow follow : follows) { //4.推送 String key = RedisConstants.FEED_KEY + follow.getUserId(); stringRedisTemplate.opsForZSet().add(key, blog.getId().toString(), System.currentTimeMillis()); } return Result.ok(blog.getId()); } 滚动分页查询理论 由于我们的feed流是动态添加的，不能使用传统的按角标方式进行查询，否则会查重复。 我们需要记录每次操作的最后一条，然后从这个位置去开始读数据。 我们需要记录每次操作的最后一条，然后从这个位置去开始读数据 举个例子：我们从t1时刻开始，拿到第一页数据，拿到了10~6，然后记录下当前最后一次读取的记录，就是6，t2时刻发布了新纪录，此时这个11在最上面，但不会影响我们之前拿到的6，此时t3时刻来读取第二页，第二页读数据的时候，从6-1=5开始读，这样就拿到了5~1的记录。我们在这个地方可以使用SortedSet来做，使用时间戳来充当表中的1~10 因此不能使用SortedSet 的 Range语句，这也是用下表查询，使用的是 ZREVRANGEBYSCORE key max min [WITHSCORES] [LIMIT offset count] ZREVRANGEBYSCORE ：降序按照得分查，注意我们需要的是按照时间戳降序查，越新的blog时间戳越大。 key： sortedset的键 max：最大得分 min：最小得分 [WITHSCORES]：是否显示分数 [LIMIT offset count]：offset：从最大得分的第几位开始查，count表示查询几个 如果是第一次查询，max给当前时间戳，offset填0就行。 如果不是第一次查询：max给上一次查询的最小时间戳，offset给上一次查询结果中，与最小时间戳一样元素的个数。 min不需要管，count是我们自己设计的。 滚动分页查询实现 第一次进入，后台没有返回给你数据，自然就是没有offset。 后面你进行滚动，滚到底触发一次新的查询（后端返回给你此页的数据，业内滚动不触发查询），查询中带上了上一次返回给你的offset和lastId。 @GetMapping(\"/of/follow\") public Result queryBlogOfFollow(@RequestParam(\"lastId\") Long max, @RequestParam(value = \"offset\", defaultValue = \"0\") Integer offset){ return blogService.queryBlogOfFollow(max, offset); } @Override public Result queryBlogOfFollow(Long max, Integer offset) { //1.获取当前用户 UserDTO userDTO = UserHolder.getUser(); //2.去收件箱里查询 String key = RedisConstants.FEED_KEY + userDTO.getId(); Set> typedTuples = stringRedisTemplate.opsForZSet() .reverseRangeByScoreWithScores(key, 0, max, offset, 2); //3.判断是否有blog if(typedTuples == null || typedTuples.isEmpty()){ return Result.ok(); } //4.解析 ArrayList ids = new ArrayList<>(typedTuples.size()); long minScore = 0; int cnt = 1; for (ZSetOperations.TypedTuple typedTuple : typedTuples) { //存进去的blogId ids.add(Long.valueOf(typedTuple.getValue())); //时间戳 long score = typedTuple.getScore().longValue(); if(score == minScore){ cnt ++; } else{ minScore = score; cnt = 1; } } //5.根据ids去查询blog，注意必须要有序 String idStr = StrUtil.join(\",\", ids); List blogs = query() .in(\"id\", ids) .last(\"order by field ( id, \" + idStr + \")\") .list(); //6.关联信息 for (Blog blog : blogs) { // 赋值userIcon和name queryBlogUser(blog); // 查询blog有没有被当前用户点赞，赋值isLiked isBlogLiked(blog); } //7.封装scrollResult ScrollResult scrollResult = new ScrollResult(blogs, minScore, cnt); return Result.ok(scrollResult); } 附近商户 GEO结构 GEO就是Geolocation的简写形式，代表地理坐标。 Redis在3.2版本中加入了对GEO的支持，允许存储地理坐标信息，帮助我们根据经纬度来检索数据。 内部使用Zset实现。 常用命令 GEOADD：添加一个地理空间信息，包含：经度（longitude）、纬度（latitude）、值（member） 返回值：添加到sorted set元素的数目，但不包括已更新score的元素 复杂度：每⼀个元素添加是O(log(N)) ，N是sorted set的元素数量 GEOADD key longitude latitude member [longitude latitude member …] GEOADD china 13.361389 38.115556 \"shanghai\" 15.087269 37.502669 \"beijing\" GEODIST：计算指定的两个点之间的距离并返回 如果两个位置之间的其中⼀个不存在， 那么命令返回空值。 默认使用米作为单位。 GEODIST 命令在计算距离时会假设地球为完美的球形， 在极限情况下， 这⼀假设最⼤会造成 0.5% 的误差 返回值：计算出的距离会以双精度浮点数的形式被返回。 如果给定的位置元素不存在， 那么命令返回空值 GEODIST key member1 member2 [m|km|ft|mi] GEODIST china beijing shanghai km GEOPOS：返回指定member的坐标 返回值：GEOPOS 命令返回一个数组， 数组中的每个项都由两个元素组成： 第一个元素为给定位置元素的经度， 而第二个元素则为给定位置元素的纬度。当给定的位置元素不存在时， 对应的数组项为空值 复杂度：O(log(N)) GEOPOS key member [member …] geopos china beijing shanghai GEOHASH：将指定member的坐标转化为hash字符串形式并返回，可以节省空间。 复杂度：O(log(N)) GEOHASH key member [member …] GEOHASH china beijing shanghai GEOSEARCH：在指定范围内搜索member，并按照与制定点之间的距离排序后返回，范围可以使圆形或矩形 返回值： 在没有给定任何 WITH 选项的情况下， 命令只会返回一个像 [“New York”,”Milan”,”Paris”] 这样的线性（linear）列表。 在指定了 WITHCOORD 、 WITHDIST 、 WITHHASH 等选项的情况下， 命令返回一个二层嵌套数组， 内层的每个子数组就表示一个元素。 在返回嵌套数组时， 子数组的第一个元素总是位置元素的名字。 至于额外的信息， 则会作为子数组的后续元素， 按照以下顺序被返回： 以浮点数格式返回的中心与位置元素之间的距离， 单位与用户指定范围时的单位一致。 geohash 整数。 由两个元素组成的坐标，分别为经度和纬度 复杂度： GEOSEARCH key [FROMMEMBER member] [FROMLONLAT longitude latitude] [BYRADIUS radius m|km|ft|mi] [BYBOX width height m|km|ft|mi] [ASC|DESC] [COUNT count [ANY]] [WITHCOORD] [WITHDIST] [WITHHASH] geosearch china FROMLONLAT 15 37 BYRADIUS 200 km ASC WITHCOORD WITHDIST WITHCOORD:返回结果时会附加每个结果的坐标。 WITHDIST: 如果包含此选项，返回结果时会附加每个结果与查询点的距离。 预热数据 把同类型的放入redis中，方便之后按照距离从redis中查询。 public void loadShopData(){ //1.查询店铺信息 List list = shopService.list(); //2.按照店铺id进行分组 Map> mp = list.stream().collect(Collectors.groupingBy(Shop::getTypeId)); //3.分批写入redis for (Map.Entry> entry : mp.entrySet()) { //3.1获取id Long typeId = entry.getKey(); String key = \"shop:geo:\" + typeId; //3.2获取同类型的店铺集合 List shopList = entry.getValue(); List> locations = new ArrayList<>(shopList.size()); for (Shop shop : shopList) { //stringRedisTemplate.opsForGeo().add(key, new Point(shop.getX(), shop.getY()), shop.getId().toString()); locations.add(new RedisGeoCommands.GeoLocation<>( shop.getId().toString(), new Point(shop.getX(), shop.getY()))); } stringRedisTemplate.opsForGeo().add(key, locations); } } 查询 @Override public Result queryShopByType(Integer typeId, Integer current, Double x, Double y) { //1.判断需不需要查询距离 if(x == null || y == null){ Page page = query() .eq(\"type_id\", typeId) .page(new Page<>(current, SystemConstants.DEFAULT_PAGE_SIZE)); return Result.ok(page); } //2.计算分页参数 int from = (current - 1) * SystemConstants.DEFAULT_PAGE_SIZE; int end = current * SystemConstants.DEFAULT_PAGE_SIZE; //3.redis做分页查询,结果里面是店铺id和距离 String key = \"shop:geo:\" + typeId; //这里查询的是0到end GeoResults> results = stringRedisTemplate.opsForGeo().search( key, GeoReference.fromCoordinate(x, y), new Distance(5000), RedisGeoCommands.GeoSearchCommandArgs.newGeoSearchArgs().includeDistance().limit(end) ); //4.解析出id if(results == null){ return Result.ok(Collections.emptyList()); } List>> list = results.getContent(); if(list.size() ids = new ArrayList<>(list.size()); Map distanceMap = new HashMap<>(list.size()); list.stream().skip(from).forEach(result -> { //4.2获取店铺id String shopIdStr = result.getContent().getName(); ids.add(Long.valueOf(shopIdStr)); //4.3获取店铺距离 Distance distance = result.getDistance(); distanceMap.put(shopIdStr, distance); }); //5根据id去查询数据库 String idStr = StrUtil.join(\",\", ids); List shops = query() .in(\"id\", ids) .last(\"order by field ( id, \" + idStr + \")\") .list(); for (Shop shop : shops) { shop.setDistance(distanceMap.get(shop.getId().toString()).getValue()); } return Result.ok(shops); } 用户签到 我们针对签到功能完全可以通过MySQL来完成，例如下面这张表 用户签到一次，就是一条记录，假如有1000W用户，平均没人每年签到10次，那这张表一年的数据量就有1亿条 Field Type Collation Null Key Default Extra Comment id bigint unsigned (NULL) NO PRI (NULL) auto_increment 主键 user_id bigint unsigned (NULL) NO (NULL) 用户id year year (NULL) NO (NULL) 签到的年 month tinyint (NULL) NO (NULL) 签到的月 date date (NULL) NO (NULL) 签到的日期 is_backup tinyint unsigned (NULL) YES (NULL) 是否补签 如何简化？可以用二进制存，第x位是1，表示第x+1天签到了。 Redis中是利用String类型数据结构实现BitMap，因此最大上限是512M，转换为bit则是2^32个bit位 用用户结合年和月作为key，值是当月的签到情况。 SETBIT：向指定位置（offset）存入一个0或1 GETBIT：获取指定位置（offset）的bit值 BITCOUNT：统计BitMap中值为1的bit位的数量 BITFIELD：操作（查询、修改、自增）BitMap中bit数组中的指定位置（offset）的值，常用于批量查询，返回的是十进制 //从第0号位开始，拿dayOfMonth个 BITFIELD key GET u[dayOfMonth] 0 BITFIELD_RO：获取BitMap中bit数组，并以十进制形式返回 BITOP：将多个BitMap的结果做位运算（与、或、异或） BITPOS：查找bit数组中指定范围内第一个0或1出现的位置 @Override public Result sign() { //1.获取当前用户 Long userId = UserHolder.getUser().getId(); //2.获取日期 LocalDateTime now = LocalDateTime.now(); String formatTime = now.format(DateTimeFormatter.ofPattern(\":yyyy/MM\")); //3.拼接key String key = USER_SIGN_KEY + userId + formatTime; //4.获取本月是第几天 int dayOfMonth = now.getDayOfMonth() - 1; //5.存入redis stringRedisTemplate.opsForValue().setBit(key, dayOfMonth, true); return Result.ok(); } 连续签到统计： public Result signCount() { //1.获取当前用户 Long userId = UserHolder.getUser().getId(); //2.获取日期 LocalDateTime now = LocalDateTime.now(); String formatTime = now.format(DateTimeFormatter.ofPattern(\":yyyy/MM\")); //3.拼接key String key = USER_SIGN_KEY + userId + formatTime; //4.获取本月是第几天 int dayOfMonth = now.getDayOfMonth(); //5.获取本月全部记录 BITFIELD sign:5:202203 GET U14 0 List result = stringRedisTemplate.opsForValue().bitField( key, BitFieldSubCommands.create() .get(BitFieldSubCommands.BitFieldType.unsigned(dayOfMonth)) .valueAt(0)); if(result == null || result.size() == 0){ return Result.ok(0); } //6.循环遍历 Long num = result.get(0); int cnt = 0; while(true){ if((num & 1) == 1){ cnt ++; } else{ break; } num >>>= 1; } return Result.ok(cnt); } UV统计 UV：全称Unique Visitor，也叫独立访客量，是指通过互联网访问、浏览这个网页的自然人。1天内同一个用户多次访问该网站，只记录1次。 PV：全称Page View，也叫页面访问量或点击量，用户每访问网站的一个页面，记录1次PV，用户多次打开页面，则记录多次PV。往往用来衡量网站的流量。 ​ UV统计在服务端做会很麻烦，因为要判断该用户是否已经统计过了，需要将统计过的信息保存，但是如果每个访问的用户都保存到Redis中，那么数据库会非常恐怖，那么该如何处理呢？ ​ HyperLogLog(HLL)是从Loglog算法派生的概率算法，用户确定非常大的集合基数，而不需要存储其所有值，而且多次插入一样的值只插入一次，因为天生适合存储UV。算法相关原理可以参考下面这篇文章：https://juejin.cn/post/6844903785744056333#heading-0 ​ Redis中的HLL是基于string结构实现的，单个HLL的内存永远小于16kb，内存占用低的令人发指！作为代价，其测量结果是概率性的，有小于0.81％的误差。不过对于UV统计来说，这完全可以忽略。 PFADD key element [element...] summary: Adds the specified elements to the specified HyperLogLog PFCOUNT key [key ...] Return the approximated cardinality of the set(s) observed by the HyperLogLog at key(s). Redis高级 分布式缓存 单点redis缺点： 数据丢失问题：Redis是内存存储，服务重启可能会丢失数据 并发能力问题：单节点Redis并发能力虽然不错，但也无法满足如618这样的高并发场景 故障恢复问题：如果Redis宕机，则服务不可用，需要一种自动的故障恢复手段 存储能力问题：Redis基于内存，单节点能存储的数据量难以满足海量数据需求 Redis的持久化 1.1RDB持久化 RDB全称Redis Database Backup file（Redis数据备份文件），也被叫做Redis数据快照。 简单来说就是把内存中的所有数据都记录到磁盘中，不是只写更新的数据。 当Redis实例故障重启后，从磁盘读取快照文件，恢复数据。 快照文件称为RDB文件，默认是保存在当前运行目录。 1.1.1执行时机 RDB持久化在四种情况下会执行： 执行save命令 执行bgsave命令 Redis停机时 触发RDB条件时 1）save命令 执行下面的命令，可以立即执行一次RDB： save命令会导致主进程执行RDB，这个过程中其它所有命令都会被阻塞。只有在数据迁移时可能用到。 2）bgsave命令 下面的命令可以异步执行RDB： 这个命令执行后会开启独立进程完成RDB，主进程可以持续处理用户请求，不受影响。 3）停机时 Redis停机时会执行一次save命令，实现RDB持久化。 4）触发RDB条件 Redis内部有触发RDB的机制，可以在redis.conf文件中找到，格式如下： # 900秒内，如果至少有1个key被修改，则执行bgsave ， 如果是save \"\" 则表示禁用RDB save 900 1 save 300 10 save 60 10000 RDB的其它配置也可以在redis.conf文件中设置： # 是否压缩 ,建议不开启，压缩也会消耗cpu，磁盘的话不值钱 rdbcompression yes # RDB文件名称 dbfilename dump.rdb # 文件保存的路径目录 dir ./ 1.1.2.RDB原理 bgsave开始时会fork主进程得到子进程，子进程共享主进程的内存数据。完成fork后读取内存数据并写入 RDB 文件。 fork采用的是copy-on-write技术： 当主进程执行读操作时，访问共享内存； 当主进程执行写操作时，则会拷贝一份数据，执行写操作，极限情况内存数据会翻倍。 1.1.3.小结 RDB方式bgsave的基本流程？ fork主进程得到一个子进程，共享内存空间 子进程读取内存数据并写入新的RDB文件 用新RDB文件替换旧的RDB文件 RDB会在什么时候执行？save 60 1000代表什么含义？ 默认是服务停止时 代表60秒内至少执行1000次修改则触发RDB RDB的缺点？ RDB执行间隔时间长，两次RDB之间写入数据有丢失的风险 fork子进程、压缩、写出RDB文件都比较耗时 1.2.AOF持久化 1.2.1.AOF原理 AOF全称为Append Only File（追加文件）。Redis处理的每一个写命令都会记录在AOF文件，可以看做是命令日志文件。 1.2.2.AOF配置 AOF默认是关闭的，需要修改redis.conf配置文件来开启AOF： # 是否开启AOF功能，默认是no appendonly yes # AOF文件的名称 appendfilename \"appendonly.aof\" AOF的命令记录的频率也可以通过redis.conf文件来配： # 表示每执行一次写命令，立即记录到AOF文件 appendfsync always # 写命令执行完先放入AOF缓冲区，然后表示每隔1秒将缓冲区数据写到AOF文件，是默认方案 appendfsync everysec # 写命令执行完先放入AOF缓冲区，由操作系统决定何时将缓冲区内容写回磁盘 appendfsync no 三种策略对比： 1.2.3.AOF文件重写 因为是记录命令，AOF文件会比RDB文件大的多。而且AOF会记录对同一个key的多次写操作，但只有最后一次写操作才有意义。通过执行bgrewriteaof命令，可以让AOF文件执行重写功能，用最少的命令达到相同效果。 Redis也会在触发阈值时自动去重写AOF文件。阈值也可以在redis.conf中配置： # AOF文件比上次文件 增长超过多少百分比则触发重写 auto-aof-rewrite-percentage 100 # AOF文件体积最小多大以上才触发重写 auto-aof-rewrite-min-size 64mb 1.3.RDB与AOF对比 RDB和AOF各有自己的优缺点，如果对数据安全性要求较高，在实际开发中往往会结合两者来使用。 Redis 在重新启动时会根据持久化的策略读取 RDB 和 AOF 文件来恢复内存中的数据。 RDB 文件恢复： 如果 Redis 检测到存在 RDB 文件（通常是 dump.rdb），它会在启动时优先加载这个快照文件。 加载 RDB 文件时，Redis 会将快照中的所有数据一次性加载到内存中，使得内存的状态恢复到生成快照时的状态。 AOF 文件恢复： 如果设置了 AOF 持久化（通常称为 appendonly.aof），Redis 在重启时也会检查 AOF 文件。 如果 AOF 文件存在且 appendfsync 配置为 everysec 或者其他值，Redis 会使用 AOF 文件中的命令逐条重放，来恢复内存中的数据。 恢复过程 优先顺序 Redis 在启动时会先尝试加载 RDB 文件，如果 RDB 文件存在且有效，Redis 会加载它。 如果没有 RDB 文件，或者 RDB 文件加载失败，Redis 将尝试加载 AOF 文件。 数据一致性 使用 RDB 文件重启时，可能会丢失自最后一次快照以来的一些数据（例如在最后一次 RDB 持久化之后的写操作）。 使用 AOF 文件时，如果 AOF 文件是完整的，所有的写操作都将被回放，通常能更好地保证数据一致性，但相对 RDB 可能会导致更长的恢复时间。 Redis主从 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-02-08 17:57:17 "},"Chapter3/Redis高级和原理.html":{"url":"Chapter3/Redis高级和原理.html","title":"Redis高级和原理","keywords":"","body":"分布式缓存 单点redis缺点： 数据丢失问题：Redis是内存存储，服务重启可能会丢失数据 并发能力问题：单节点Redis并发能力虽然不错，但也无法满足如618这样的高并发场景 故障恢复问题：如果Redis宕机，则服务不可用，需要一种自动的故障恢复手段 存储能力问题：Redis基于内存，单节点能存储的数据量难以满足海量数据需求 1.Redis的持久化 1.1RDB持久化 RDB全称Redis Database Backup file（Redis数据备份文件），也被叫做Redis数据快照。 简单来说就是把内存中的所有数据都记录到磁盘中，不是只写更新的数据。 当Redis实例故障重启后，从磁盘读取快照文件，恢复数据。 快照文件称为RDB文件，默认是保存在当前运行目录。 1.1.1执行时机 RDB持久化在四种情况下会执行： 执行save命令 执行bgsave命令 Redis停机时 触发RDB条件时 1）save命令 执行下面的命令，可以立即执行一次RDB： save命令会导致主进程执行RDB，这个过程中其它所有命令都会被阻塞。只有在数据迁移时可能用到。 2）bgsave命令 下面的命令可以异步执行RDB： 这个命令执行后会开启独立进程完成RDB，主进程可以持续处理用户请求，不受影响。 3）停机时 Redis停机时会执行一次save命令，实现RDB持久化。 4）触发RDB条件 Redis内部有触发RDB的机制，可以在redis.conf文件中找到，格式如下： # 900秒内，如果至少有1个key被修改，则执行bgsave ， 如果是save \"\" 则表示禁用RDB save 900 1 save 300 10 save 60 10000 RDB的其它配置也可以在redis.conf文件中设置： # 是否压缩 ,建议不开启，压缩也会消耗cpu，磁盘的话不值钱 rdbcompression yes # RDB文件名称 dbfilename dump.rdb # 文件保存的路径目录 dir ./ 1.1.2.RDB原理 bgsave开始时会fork主进程得到子进程，子进程共享主进程的内存数据。完成fork后读取内存数据并写入 RDB 文件。 fork采用的是copy-on-write技术： 当主进程执行读操作时，访问共享内存； 当主进程执行写操作时，则会拷贝一份数据，执行写操作，极限情况内存数据会翻倍。 1.1.3.小结 RDB方式bgsave的基本流程？ fork主进程得到一个子进程，共享内存空间 子进程读取内存数据并写入新的RDB文件 用新RDB文件替换旧的RDB文件 RDB会在什么时候执行？save 60 1000代表什么含义？ 默认是服务停止时 代表60秒内至少执行1000次修改则触发RDB RDB的缺点？ RDB执行间隔时间长，两次RDB之间写入数据有丢失的风险 fork子进程、压缩、写出RDB文件都比较耗时 1.2.AOF持久化 1.2.1.AOF原理 AOF全称为Append Only File（追加文件）。Redis处理的每一个写命令都会记录在AOF文件，可以看做是命令日志文件。 1.2.2.AOF配置 AOF默认是关闭的，需要修改redis.conf配置文件来开启AOF： # 是否开启AOF功能，默认是no appendonly yes # AOF文件的名称 appendfilename \"appendonly.aof\" AOF的命令记录的频率也可以通过redis.conf文件来配： # 表示每执行一次写命令，立即记录到AOF文件 appendfsync always # 写命令执行完先放入AOF缓冲区，然后表示每隔1秒将缓冲区数据写到AOF文件，是默认方案 appendfsync everysec # 写命令执行完先放入AOF缓冲区，由操作系统决定何时将缓冲区内容写回磁盘 appendfsync no 三种策略对比： 1.2.3.AOF文件重写 因为是记录命令，AOF文件会比RDB文件大的多。而且AOF会记录对同一个key的多次写操作，但只有最后一次写操作才有意义。通过执行bgrewriteaof命令，可以让AOF文件执行重写功能，用最少的命令达到相同效果。 Redis也会在触发阈值时自动去重写AOF文件。阈值也可以在redis.conf中配置： # AOF文件比上次文件 增长超过多少百分比则触发重写 auto-aof-rewrite-percentage 100 # AOF文件体积最小多大以上才触发重写 auto-aof-rewrite-min-size 64mb 1.2.4.文件校验 AOF 校验机制是 Redis 在启动时对 AOF 文件进行检查，以判断文件是否完整，是否有损坏或者丢失的数据。这个机制的原理其实非常简单，就是通过使用一种叫做 校验和（checksum） 的数字来验证 AOF 文件。这个校验和是通过对整个 AOF 文件内容进行 CRC64 算法计算得出的数字。如果文件内容发生了变化，那么校验和也会随之改变。因此，Redis 在启动时会比较计算出的校验和与文件末尾保存的校验和（计算的时候会把最后一行保存校验和的内容给忽略点），从而判断 AOF 文件是否完整。如果发现文件有问题，Redis 就会拒绝启动并提供相应的错误信息。AOF 校验机制十分简单有效，可以提高 Redis 数据的可靠性。 类似地，RDB 文件也有类似的校验机制来保证 RDB 文件的正确性，这里就不重复进行介绍了。 1.3.RDB与AOF对比 RDB和AOF各有自己的优缺点，如果对数据安全性要求较高，在实际开发中往往会结合两者来使用。 如何选择? Redis 保存的数据丢失一些也没什么影响的话，可以选择使用 RDB。 不建议单独使用 AOF，因为时不时地创建一个 RDB 快照可以进行数据库备份、更快的重启以及解决 AOF 引擎错误。 如果保存的数据要求安全性比较高的话，建议同时开启 RDB 和 AOF 持久化或者开启 RDB 和 AOF 混合持久化。 Redis 在重新启动时会根据持久化的策略读取 RDB 和 AOF 文件来恢复内存中的数据。 RDB 文件恢复： 如果 Redis 检测到存在 RDB 文件（通常是 dump.rdb），它会在启动时优先加载这个快照文件。 加载 RDB 文件时，Redis 会将快照中的所有数据一次性加载到内存中，使得内存的状态恢复到生成快照时的状态。 AOF 文件恢复： 如果设置了 AOF 持久化（通常称为 appendonly.aof），Redis 在重启时也会检查 AOF 文件。 如果 AOF 文件存在且 appendfsync 配置为 everysec 或者其他值，Redis 会使用 AOF 文件中的命令逐条重放，来恢复内存中的数据。 恢复过程 优先顺序 Redis 在启动时会先尝试加载 RDB 文件，如果 RDB 文件存在且有效，Redis 会加载它。 如果没有 RDB 文件，或者 RDB 文件加载失败，Redis 将尝试加载 AOF 文件。 数据一致性 使用 RDB 文件重启时，可能会丢失自最后一次快照以来的一些数据（例如在最后一次 RDB 持久化之后的写操作）。 使用 AOF 文件时，如果 AOF 文件是完整的，所有的写操作都将被回放，通常能更好地保证数据一致性，但相对 RDB 可能会导致更长的恢复时间。 2.Redis主从 2.1.搭建主从架构 单节点Redis的并发能力是有上限的，要进一步提高Redis的并发能力，就需要搭建主从集群，实现读写分离。 主节点负责写（也可以读），从节点只能负责读取（不能写）。 假设有A、B两个Redis实例，如何让B作为A的slave节点? 在B节点执行命令：slaveof A的IP A的port 具体搭建流程参考课前资料《Redis集群.md》： 2.2.主从数据同步原理 2.2.1.全量同步 主从第一次建立连接时，会执行全量同步，将master节点的所有数据都拷贝给slave节点，流程： 这里有一个问题，master如何得知salve是第一次来连接呢？？ 有几个概念，可以作为判断依据： Replication Id：简称replid，是数据集的标记，id一致则说明是同一数据集。每一个master都有唯一的replid，slave则会继承master节点的replid offset：偏移量，随着记录在repl_baklog中的数据增多而逐渐增大。slave完成同步时也会记录当前同步的offset。如果slave的offset小于master的offset，说明slave数据落后于master，需要更新。 因此slave做数据同步，必须向master声明自己的replication id 和offset，master才可以判断到底需要同步哪些数据。 因为slave原本也是一个master，有自己的replid和offset，当第一次变成slave，与master建立连接时，发送的replid和offset是自己的replid和offset。 master判断发现slave发送来的replid与自己的不一致，说明这是一个全新的slave，就知道要做全量同步了。 master会将自己的replid和offset都发送给这个slave，slave保存这些信息。以后slave的replid就与master一致了。 因此，master判断一个节点是否是第一次同步的依据，就是看replid是否一致。 如图： 完整流程描述： slave节点请求增量同步 master节点判断replid，发现不一致，拒绝增量同步 master将完整内存数据生成RDB，发送RDB到slave slave清空本地数据，加载master的RDB master将RDB期间的命令记录在repl_baklog，并持续将log中的命令发送给slave slave执行接收到的命令，保持与master之间的同步 2.2.2.增量同步 全量同步需要先做RDB，然后将RDB文件通过网络传输个slave，成本太高了。 因此除了第一次做全量同步，其它大多数时候slave与master都是做增量同步。 什么是增量同步？就是只更新slave与master存在差异的部分数据。如图： 2.2.3.repl_backlog原理 master怎么知道slave与自己的数据差异在哪里呢? 这就要说到全量同步时的repl_baklog文件了。 这个文件是一个固定大小的环形数组，也就是说角标到达数组末尾后，会再次从0开始读写，这样数组头部的数据就会被覆盖。 repl_baklog中会记录Redis处理过的命令日志及offset，包括master当前的offset，和slave已经拷贝到的offset： slave与master的offset之间的差异，就是salve需要增量拷贝的数据了。 随着不断有数据写入，master的offset逐渐变大，slave也不断的拷贝，追赶master的offset： 此时，如果有新的数据写入，就会覆盖数组中的旧数据。不过，旧的数据只要是绿色的，说明是已经被同步到slave的数据，即便被覆盖了也没什么影响。因为未同步的仅仅是红色部分。 但是，如果slave出现网络阻塞，导致master的offset远远超过了slave的offset： 如果master继续写入新数据，其offset就会覆盖旧的数据，直到将slave现在的offset也覆盖： 棕色框中的红色部分，就是尚未同步，但是却已经被覆盖的数据。此时如果slave恢复，需要同步，却发现自己的offset都没有了，无法完成增量同步了。只能做全量同步。 2.3.主从同步优化 主从同步可以保证主从数据的一致性，非常重要。 可以从以下几个方面来优化Redis主从就集群： 在master中配置repl-diskless-sync yes启用无磁盘复制，不写磁盘，直接网络传，这样要求网速要快。 Redis单节点上的内存占用不要太大，减少RDB导致的过多磁盘IO 适当提高repl_baklog的大小，发现slave宕机时尽快实现故障恢复，尽可能避免全量同步 限制一个master上的slave节点数量，如果实在是太多slave，则可以采用主-从-从链式结构，减少master压力 主从从架构图： 2.4.小结 简述全量同步和增量同步区别？ 全量同步：master将完整内存数据生成RDB，发送RDB到slave。后续命令则记录在repl_baklog，逐个发送给slave。 增量同步：slave提交自己的offset到master，master获取repl_baklog中从offset之后的命令给slave 什么时候执行全量同步？ slave节点第一次连接master节点时 slave节点断开时间太久，repl_baklog中的offset已经被覆盖时 什么时候执行增量同步？ slave节点断开又恢复，并且在repl_baklog中能找到offset时 3.Redis哨兵 Redis提供了哨兵（Sentinel）机制来实现主从集群的自动故障恢复。 3.1.哨兵原理 3.1.1.集群结构和作用 哨兵的结构如图： 哨兵的作用如下： 监控：Sentinel 会不断检查您的master和slave是否按预期工作 自动故障恢复：如果master故障，Sentinel会将一个slave提升为master。当故障实例恢复后也以新的master为主 通知：Sentinel充当Redis客户端的服务发现来源，当集群发生故障转移时，会将最新信息推送给Redis的客户端 3.1.2.集群监控原理 Sentinel基于心跳机制监测服务状态，每隔1秒向集群的每个实例发送ping命令： •主观下线：如果某sentinel节点发现某实例未在规定时间响应，则认为该实例主观下线。 •客观下线：若超过指定数量（quorum）的sentinel都认为该实例主观下线，则该实例客观下线。quorum值最好超过Sentinel实例数量的一半。 3.1.3.集群故障恢复原理 一旦发现master故障，sentinel需要在salve中选择一个作为新的master，选择依据是这样的： 首先会判断slave节点与master节点断开时间长短，如果超过指定值（down-after-milliseconds * 10）则会排除该slave节点 然后判断slave节点的slave-priority值，越小优先级越高，如果是0则永不参与选举 如果slave-prority一样，则判断slave节点的offset值，越大说明数据越新，优先级越高 最后是判断slave节点的运行id大小，越小优先级越高。 当选出一个新的master后，该如何实现切换呢？ 流程如下： sentinel给备选的slave1节点发送slaveof no one命令，让该节点成为master sentinel给所有其它slave发送slaveof 192.168.150.101 7002 命令，让这些slave成为新master的从节点，开始从新的master上同步数据。 最后，sentinel将故障节点标记为slave，当故障节点恢复后会自动成为新的master的slave节点 3.1.4.小结 Sentinel的三个作用是什么？ 监控 故障转移 通知 Sentinel如何判断一个redis实例是否健康？ 每隔1秒发送一次ping命令，如果超过一定时间没有相向则认为是主观下线 如果大多数sentinel都认为实例主观下线，则判定服务下线 故障转移步骤有哪些？ 首先选定一个slave作为新的master，执行slaveof no one 然后让所有节点都执行slaveof 新master 修改故障节点配置，添加slaveof 新master 3.2.搭建哨兵集群 具体搭建流程参考课前资料《Redis集群.md》： 也和redis一样，需要启动，注意仅需要连接主节点就行，因为主节点制动从节点信息，并且给主节点起个名字，比如mymaster，后序配置redisTemplate需要用到。 3.3.RedisTemplate ​ 在Sentinel集群监管下的Redis主从集群，其节点会因为自动故障转移而发生变化，Redis的客户端必须感知这种变化，及时更新连接信息。Spring的RedisTemplate底层利用lettuce实现了节点的感知和自动切换。 下面，我们通过一个测试来实现RedisTemplate集成哨兵机制。 3.3.1.导入Demo工程 首先，我们引入课前资料提供的Demo工程： 3.3.2.引入依赖 在项目的pom文件中引入依赖： org.springframework.boot spring-boot-starter-data-redis 3.3.3.配置Redis地址 然后在配置文件application.yml中指定redis的sentinel相关信息： 因为主节点是动态变化的，我们需要哨兵告诉我主节点和从节点，完成读写分离。 spring: redis: sentinel: master: mymaster nodes: - 192.168.150.101:27001 - 192.168.150.101:27002 - 192.168.150.101:27003 3.3.4.配置读写分离 在项目的启动类中，添加一个新的bean： @Bean public LettuceClientConfigurationBuilderCustomizer clientConfigurationBuilderCustomizer(){ return clientConfigurationBuilder -> clientConfigurationBuilder.readFrom(ReadFrom.REPLICA_PREFERRED); } 这个bean中配置的就是读写策略，包括四种： MASTER：从主节点读取 MASTER_PREFERRED：优先从master节点读取，master不可用才读取replica REPLICA：从slave（replica）节点读取 REPLICA _PREFERRED：优先从slave（replica）节点读取，所有的slave都不可用才读取master 4.Redis分片集群 4.1.搭建分片集群 主从和哨兵可以解决高可用、高并发读的问题。但是依然有两个问题没有解决： 海量数据存储问题 高并发写的问题 使用分片集群可以解决上述问题，如图: 分片集群特征： 集群中有多个master，每个master保存不同数据 每个master都可以有多个slave节点 master之间通过ping监测彼此健康状态 客户端请求可以访问集群任意节点，最终都会被转发到正确节点 具体搭建流程参考课前资料《Redis集群.md》 4.2.散列插槽 4.2.1.插槽原理 Redis会把每一个master节点映射到0~16383共16384个插槽（hash slot）上，查看集群信息时就能看到： 数据key不是与节点绑定，而是与插槽绑定。redis会根据key的有效部分计算插槽值，分两种情况： key中包含\"{}\"，且“{}”中至少包含1个字符，“{}”中的部分是有效部分 key中不包含“{}”，整个key都是有效部分 例如：key是num，那么就根据num计算，如果是{itcast}num，则根据itcast计算。计算方式是利用CRC16算法得到一个hash值，然后对16384取余，得到的结果就是slot值。 如图，在7001这个节点执行set a 1时，对a做hash运算，对16384取余，得到的结果是15495，因此要存储到103节点。 到了7003后，执行get num时，对num做hash运算，对16384取余，得到的结果是2765，因此需要切换到7001节点 4.2.1.小结 Redis如何判断某个key应该在哪个实例？ 将16384个插槽分配到不同的实例 根据key的有效部分计算哈希值，对16384取余 余数作为插槽，寻找插槽所在实例即可 如何将同一类数据固定的保存在同一个Redis实例？ 这一类数据使用相同的有效部分，例如key都以{typeId}为前缀 4.3.集群伸缩 redis-cli --cluster提供了很多操作集群的命令，可以通过下面方式查看： 比如，添加节点的命令： 4.3.1.需求分析 需求：向集群中添加一个新的master节点，并向其中存储 num = 10 启动一个新的redis实例，端口为7004 添加7004到之前的集群，并作为一个master节点 给7004节点分配插槽，使得num这个key可以存储到7004实例 这里需要两个新的功能： 添加一个节点到集群中 将部分插槽分配到新插槽 4.3.2.创建新的redis实例 创建一个文件夹： mkdir 7004 拷贝配置文件： cp redis.conf /7004 修改配置文件： sed /s/6379/7004/g 7004/redis.conf 启动 redis-server 7004/redis.conf 4.3.3.添加新节点到redis 添加节点的语法如下： 执行命令： redis-cli --cluster add-node 192.168.150.101:7004 192.168.150.101:7001 通过命令查看集群状态： redis-cli -p 7001 cluster nodes 如图，7004加入了集群，并且默认是一个master节点： 但是，可以看到7004节点的插槽数量为0，因此没有任何数据可以存储到7004上 4.3.4.转移插槽 我们要将num存储到7004节点，因此需要先看看num的插槽是多少： 如上图所示，num的插槽为2765. 我们可以将0~3000的插槽从7001转移到7004，命令格式如下： 具体命令如下： 建立连接： 得到下面的反馈： 询问要移动多少个插槽，我们计划是3000个： 新的问题来了： 那个node来接收这些插槽？？ 显然是7004，那么7004节点的id是多少呢？ 复制这个id，然后拷贝到刚才的控制台后： 这里询问，你的插槽是从哪里移动过来的？ all：代表全部，也就是三个节点各转移一部分 具体的id：目标节点的id done：没有了 这里我们要从7001获取，因此填写7001的id： 填完后，点击done，这样插槽转移就准备好了： 确认要转移吗？输入yes： 然后，通过命令查看结果： 可以看到： 目的达成。 4.4.故障转移 集群初识状态是这样的： 其中7001、7002、7003都是master，我们计划让7002宕机。 4.4.1.自动故障转移 当集群中有一个master宕机会发生什么呢？ 直接停止一个redis实例，例如7002： redis-cli -p 7002 shutdown 1）首先是该实例与其它实例失去连接 2）然后是疑似宕机： 3）最后是确定下线，自动提升一个slave为新的master： 4）当7002再次启动，就会变为一个slave节点了： 4.4.2.手动故障转移 利用cluster failover命令可以手动让集群中的某个master宕机，切换到执行cluster failover命令的这个slave节点，实现无感知的数据迁移。其流程如下： 这种failover命令可以指定三种模式： 缺省：默认的流程，如图1~6歩 force：省略了对offset的一致性校验 takeover：直接执行第5歩，忽略数据一致性、忽略master状态和其它master的意见 案例需求：在7002这个slave节点执行手动故障转移，重新夺回master地位 步骤如下： 1）利用redis-cli连接7002这个节点 2）执行cluster failover命令 如图： 效果： 4.5.RedisTemplate访问分片集群 RedisTemplate底层同样基于lettuce实现了分片集群的支持，而使用的步骤与哨兵模式基本一致： 1）引入redis的starter依赖 2）配置分片集群地址 3）配置读写分离 与哨兵模式相比，其中只有分片集群的配置方式略有差异，如下： spring: redis: cluster: nodes: - 192.168.150.101:7001 - 192.168.150.101:7002 - 192.168.150.101:7003 - 192.168.150.101:8001 - 192.168.150.101:8002 - 192.168.150.101:8003 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-02-03 16:24:33 "},"Chapter3/Spring.html":{"url":"Chapter3/Spring.html","title":"Spring","keywords":"","body":"Spring 基础 1. 什么是 Spring 框架? Spring 是一款开源的轻量级 Java 开发框架，支持 IoC（Inversion of Control:控制反转） 和 AOP(Aspect-Oriented Programming:面向切面编程)、可以很方便地对数据库进行访问、可以很方便地集成第三方组件（电子邮件，任务，调度，缓存等等）、对单元测试支持比较好、支持 RESTful Java 应用程序的开发。 2. Spring 包含的模块有哪些？ Core Container spring-core：Spring 框架基本的核心工具类。 spring-beans：提供对 bean 的创建、配置和管理等功能的支持。 spring-context：提供对国际化、事件传播、资源加载等功能的支持。 spring-expression：提供对表达式语言（Spring Expression Language） SpEL 的支持，只依赖于 core 模块，不依赖于其他模块，可以单独使用。 AOP Data Access/Integration Spring Web Spring Test 3. Spring,Spring MVC,Spring Boot 之间什么关系? Spring MVC 是 Spring 中的一个很重要的模块，基于 Spring 的 IoC 容器，主要赋予 Spring 快速构建 MVC 架构的 Web 程序的能力。MVC 是模型(Model)、视图(View)、控制器(Controller)的简写，其核心思想是通过将业务逻辑、数据、显示分离来组织代码。 Spring Boot 旨在简化 Spring 开发（减少配置文件，开箱即用！）。 4. Spring 框架中用到了哪些设计模式？ 工厂设计模式 : Spring 使用工厂模式通过 BeanFactory、ApplicationContext 创建 bean 对象。 代理设计模式 : Spring AOP 功能的实现。 单例设计模式 : Spring 中的 Bean 默认都是单例的。 Spring IoC 1. 谈谈自己对于 Spring IoC 的了解 IoC 的思想就是将原本在程序中手动创建对象的控制权，交由 Spring 框架来管理。 为什么叫控制反转？ 控制：指的是对象创建（实例化、管理）的权力 反转：控制权交给外部环境（Spring 框架、IoC 容器） 在实际项目中一个 Service 类可能依赖了很多其他的类，假如我们需要实例化这个 Service，你可能要每次都要搞清这个 Service 所有底层类的构造函数，这可能会把人逼疯。如果利用 IoC 的话，你只需要配置好，然后在需要的地方引用就行了，这大大增加了项目的可维护性且降低了开发难度。 2. IoC 和 DI 有区别吗？ IoC 最常见以及最合理的实现方式叫做依赖注入（Dependency Injection，简称 DI）。 2. 什么是 Spring Bean？ Bean 代指的就是那些被 IoC 容器所管理的对象。 3. 将一个类声明为 Bean 的注解有哪些? @Component：通用的注解，可标注任意类为 Spring 组件。如果一个 Bean 不知道属于哪个层，可以使用@Component 注解标注。 @Repository : 对应持久层即 Dao 层，主要用于数据库相关操作。 @Service : 对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao 层。 @Controller : 对应 Spring MVC 控制层，主要用于接受用户请求并调用 Service 层返回数据给前端页面。 4. @Component 和 @Bean 的区别是什么？ @Component 注解作用于类，而@Bean注解作用于方法。 @Component通常是通过类路径扫描来自动侦测以及自动装配到 Spring 容器中（我们可以使用 @ComponentScan 注解定义要扫描的路径从中找出标识了需要装配的类自动装配到 Spring 的 bean 容器中）。@Bean 注解通常是我们在标有该注解的方法中定义产生这个 bean,@Bean告诉了 Spring 这是某个类的实例，当我需要用它的时候还给我。 @Bean 注解比 @Component 注解的自定义性更强，而且很多地方我们只能通过 @Bean 注解来注册 bean。比如当我们引用第三方库中的类需要装配到 Spring容器时，则只能通过 @Bean来实现。 下面这个例子是通过 @Component 无法实现的。 @Bean public OneService getService(status) { case (status) { when 1: return new serviceImpl1(); when 2: return new serviceImpl2(); when 3: return new serviceImpl3(); } } 5. 注入 Bean 的注解有哪些？ Spring 内置的 @Autowired 以及 JDK 内置的 @Resource 和 @Inject 都可以用于注入 Bean。 6. @Autowired 和 @Resource 的区别是什么？ @Autowired 是 Spring 提供的注解，@Resource 是 JDK 提供的注解。 Autowired 默认的注入方式为byType（根据类型进行匹配，类型不行的话按名称），@Resource默认注入方式为 byName（根据名称进行匹配，名称不行按类型）。 当一个接口存在多个实现类的情况下，@Autowired 和@Resource都需要通过名称才能正确匹配到对应的 Bean。Autowired 可以通过 @Qualifier 注解来显式指定名称，@Resource可以通过 name 属性来显式指定名称。 @Autowired 支持在构造函数、方法、字段和参数上使用。@Resource 主要用于字段和方法上的注入，不支持在构造函数或参数上使用。 // 报错，byName 和 byType 都无法匹配到 bean @Autowired private SmsService smsService; // 正确注入 SmsServiceImpl1 对象对应的 bean @Autowired private SmsService smsServiceImpl1; // 正确注入 SmsServiceImpl1 对象对应的 bean // smsServiceImpl1 就是我们上面所说的名称 @Autowired @Qualifier(value = \"smsServiceImpl1\") private SmsService smsService; ---- // 报错，byName 和 byType 都无法匹配到 bean @Resource private SmsService smsService; // 正确注入 SmsServiceImpl1 对象对应的 bean @Resource private SmsService smsServiceImpl1; // 正确注入 SmsServiceImpl1 对象对应的 bean（比较推荐这种方式） @Resource(name = \"smsServiceImpl1\") private SmsService smsService; 7. 注入 Bean 的方式有哪些（依赖注入）？ 构造函数注入示例： @Service public class UserService { //依赖项 private final UserRepository userRepository; public UserService(UserRepository userRepository) { this.userRepository = userRepository; } //... } Setter 注入示例： @Service public class UserService { private UserRepository userRepository; // 在 Spring 4.3 及以后的版本，特定情况下 @Autowired 可以省略不写 @Autowired public void setUserRepository(UserRepository userRepository) { this.userRepository = userRepository; } //... } Field 注入示例： @Service public class UserService { @Autowired private UserRepository userRepository; //... } 8. 构造函数注入还是 Setter 注入？ Spring 官方推荐构造函数注入，这种注入方式的优势如下： 依赖完整性：确保所有必需依赖在对象创建时就被注入，避免了空指针异常的风险。 不可变性：有助于创建不可变对象，提高了线程安全性。 构造函数注入适合处理必需的依赖项，而 Setter 注入 则更适合可选的依赖项，这些依赖项可以有默认值或在对象生命周期中动态设置。虽然 @Autowired 可以用于 Setter 方法来处理必需的依赖项，但构造函数注入仍然是更好的选择。 9. Bean 的作用域有哪些?如何配置？ singleton : IoC 容器中只有唯一的 bean 实例。Spring 中的 bean 默认都是单例的，是对单例设计模式的应用。 prototype : 每次获取都会创建一个新的 bean 实例。也就是说，连续 getBean() 两次，得到的是不同的 Bean 实例。 request （仅 Web 应用可用）: 每一次 HTTP 请求都会产生一个新的 bean（请求 bean），该 bean 仅在当前 HTTP request 内有效。 session （仅 Web 应用可用） : 每一次来自新 session 的 HTTP 请求都会产生一个新的 bean（会话 bean），该 bean 仅在当前 HTTP session 内有效。 application/global-session （仅 Web 应用可用）：每个 Web 应用在启动时创建一个 Bean（应用 Bean），该 bean 仅在当前应用启动时间内有效。 websocket （仅 Web 应用可用）：每一次 WebSocket 会话产生一个新的 bean。 xml方式 注解方式： @Bean @Scope(value = ConfigurableBeanFactory.SCOPE_PROTOTYPE) public Person personPrototype() { return new Person(); } 10. Bean 是线程安全的吗？如果不安全如何解决？ singleton 作用域下，IoC 容器中只有唯一的 bean 实例，可能会存在资源竞争问题（取决于 Bean 是否有状态）。如果这个 bean 是有状态的话，那就存在线程安全问题（有状态 Bean 是指包含可变的成员变量的对象）。 // 定义了一个购物车类，其中包含一个保存用户的购物车里商品的 List @Component public class ShoppingCart { private List items = new ArrayList<>(); public void addItem(String item) { items.add(item); } public List getItems() { return items; } } 不过，大部分 Bean 实际都是无状态（没有定义可变的成员变量）的（比如 Dao、Service），这种情况下， Bean 是线程安全的。 prototype 作用域下，每次获取都会创建一个新的 bean 实例，不存在资源竞争问题，所以不存在线程安全问题。 对于有状态单例 Bean 的线程安全问题，常见的三种解决办法是： 避免可变成员变量: 尽量设计 Bean 为无状态。 使用ThreadLocal: 将可变成员变量保存在 ThreadLocal 中，确保线程独立。 使用同步机制: 利用 synchronized 或 ReentrantLock 来进行同步控制，确保线程安全。 11. Bean 的生命周期了解么? 创建 Bean 的实例：Bean 容器首先会找到配置文件中的 Bean 定义，然后使用 Java 反射 API 来创建 Bean 的实例。 Bean 属性赋值/填充：为 Bean 设置相关属性和依赖，例如@Autowired 等注解注入的对象、@Value 注入的值、setter方法或构造函数注入依赖和值、@Resource注入的各种资源。 Bean 初始化 如果 Bean 实现了 BeanNameAware 接口，调用 setBeanName()方法，传入 Bean 的名字。 如果 Bean 实现了 BeanClassLoaderAware 接口，调用 setBeanClassLoader()方法，传入 ClassLoader对象的实例。 如果 Bean 实现了 BeanFactoryAware 接口，调用 setBeanFactory()方法，传入 BeanFactory对象的实例。 与上面的类似，如果实现了其他 *.Aware接口，就调用相应的方法。 如果有和加载这个 Bean 的 Spring 容器相关的 BeanPostProcessor 对象，执行postProcessBeforeInitialization() 方法 如果 Bean 实现了InitializingBean接口，执行afterPropertiesSet()方法。 如果 Bean 在配置文件中的定义包含 init-method 属性，执行指定的方法。 如果有和加载这个 Bean 的 Spring 容器相关的 BeanPostProcessor 对象，执行postProcessAfterInitialization() 方法。 销毁 Bean ：销毁并不是说要立马把 Bean 给销毁掉，而是把 Bean 的销毁方法先记录下来，将来需要销毁 Bean 或者销毁容器的时候，就调用这些方法去释放 Bean 所持有的资源。 如果 Bean 实现了 DisposableBean 接口，执行 destroy() 方法。 如果 Bean 在配置文件中的定义包含 destroy-method 属性，执行指定的 Bean 销毁方法。或者，也可以直接通过@PreDestroy 注解标记 Bean 销毁之前执行的方法。 整体上可以简单分为四步：实例化 —> 属性赋值 —> 初始化 —> 销毁。 初始化这一步涉及到的步骤比较多，包含 Aware 接口的依赖注入、BeanPostProcessor 在初始化前后的处理以及 InitializingBean 和 init-method 的初始化操作。 销毁这一步会注册相关销毁回调接口，最后通过DisposableBean 和 destory-method 进行销毁。 Spring AOP 1. 谈谈自己对于 AOP 的了解 AOP能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。 Spring AOP 就是基于动态代理的。 术语 含义 目标(Target) 被通知的对象 代理(Proxy) 向目标对象应用通知之后创建的代理对象 连接点(JoinPoint) 目标对象的所属类中，定义的所有方法均为连接点 切入点(Pointcut) 被切面拦截 / 增强的连接点（切入点一定是连接点，连接点不一定是切入点）。切点可以通过注解、正则表达式、逻辑运算等方式来定义 通知(Advice) 增强的逻辑 / 代码，也即拦截到目标对象的连接点之后要做的事情 切面(Aspect) 对横切关注点进行封装的类，一个切面是一个类，等于切入点(Pointcut)+通知(Advice) Weaving(织入) 将通知应用到目标对象，进而生成代理对象的过程动作 import org.aspectj.lang.JoinPoint; import org.aspectj.lang.annotation.After; import org.aspectj.lang.annotation.AfterReturning; import org.aspectj.lang.annotation.Aspect; import org.aspectj.lang.annotation.Before; import org.springframework.stereotype.Component; @Aspect @Component public class LoggingAspect { // @Before 通知，在目标方法执行前执行 @Before(\"execution(* com.example.demo.service.*.*(..))\") public void beforeAdvice(JoinPoint joinPoint) { System.out.println(\"Before method: \" + joinPoint.getSignature().getName()); } // @After 通知，无论目标方法是否抛出异常都会执行 @After(\"execution(* com.example.demo.service.*.*(..))\") public void afterAdvice(JoinPoint joinPoint) { System.out.println(\"After method: \" + joinPoint.getSignature().getName()); } // @AfterReturning 通知，只有目标方法正常返回时才会执行 @AfterReturning(pointcut = \"execution(* com.example.demo.service.*.*(..))\", returning = \"result\") public void afterReturningAdvice(JoinPoint joinPoint, Object result) { System.out.println(\"After returning method: \" + joinPoint.getSignature().getName() + \", Result: \" + result); } } 2. Spring AOP 和 AspectJ AOP 有什么区别？ Spring AOP 属于运行时增强，而 AspectJ 是编译时增强。 Spring AOP 基于代理(Proxying)，而 AspectJ 基于字节码操作(Bytecode Manipulation)。 Spring AOP 已经集成了 AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。AspectJ 相比于 Spring AOP 功能更加强大，但是 Spring AOP 相对来说更简单， 3. AOP 常见的通知类型有哪些？ Before（前置通知）：目标对象的方法调用之前触发 After （后置通知）：目标对象的方法调用之后触发 AfterReturning（返回通知）：目标对象的方法调用完成，在返回结果值之后触发 AfterThrowing（异常通知）：目标对象的方法运行中抛出 / 触发异常后触发。AfterReturning 和 AfterThrowing 两者互斥。如果方法调用成功无异常，则会有返回值；如果方法抛出了异常，则不会有返回值。 Around （环绕通知）：编程式控制目标对象的方法调用。环绕通知是所有通知类型中可操作范围最大的一种，因为它可以直接拿到目标对象，以及要执行的方法，所以环绕通知可以任意的在目标对象的方法调用前后搞事，甚至不调用目标对象的方法 @After：不管连接点（目标方法）是正常返回还是抛出异常，它都会在目标方法执行之后执行。可以把它理解成一个 “最终通知”，类似于 Java 中 try-catch-finally 结构里的 finally 块，无论目标方法的执行结果如何，@After 标注的通知方法都会被调用。 @AfterReturning：只有在目标方法正常返回（没有抛出异常）时才会执行。如果目标方法执行过程中抛出了异常，那么 @AfterReturning 标注的通知方法不会被调用。 4. 多个切面的执行顺序如何控制？ 通常使用@Order 注解直接定义切面顺序。 实现Ordered 接口重写 getOrder 方法。 Spring MVC 1. 说说自己对于 Spring MVC 了解? MVC 是模型(Model)、视图(View)、控制器(Controller)的简写，其核心思想是通过将业务逻辑、数据、显示分离来组织代码。 2. Spring MVC 的核心组件有哪些？ DispatcherServlet：核心的中央处理器，负责接收请求、分发，并给予客户端响应。 HandlerMapping：处理器映射器，根据 URL 去匹配查找能处理的 Handler ，并会将请求涉及到的拦截器和 Handler 一起封装。 HandlerAdapter：处理器适配器，根据 HandlerMapping 找到的 Handler ，适配执行对应的 Handler； Handler：请求处理器，处理实际请求的处理器。 ViewResolver：视图解析器，根据 Handler 返回的逻辑视图 / 视图，解析并渲染真正的视图，并传递给 DispatcherServlet 响应客户端 3. SpringMVC 工作原理了解吗? 客户端（浏览器）发送请求， DispatcherServlet拦截请求。 DispatcherServlet 根据请求信息调用 HandlerMapping 。HandlerMapping 根据 URL 去匹配查找能处理的 Handler（也就是我们平常说的 Controller 控制器） ，并会将请求涉及到的拦截器和 Handler 一起封装。 DispatcherServlet 调用 HandlerAdapter适配器执行 Handler 。 Handler 完成对用户请求的处理后，会返回一个 ModelAndView 对象给DispatcherServlet，ModelAndView 顾名思义，包含了数据模型以及相应的视图的信息。Model 是返回的数据对象，View 是个逻辑上的 View。 ViewResolver 会根据逻辑 View 查找实际的 View。 DispaterServlet 把返回的 Model 传给 View（视图渲染）。 把 View 返回给请求者（浏览器） 上述流程是传统开发模式（JSP，Thymeleaf 等）的工作原理。 对于前后端分离时，后端通常不再返回具体的视图，而是返回纯数据（通常是 JSON 格式，Spring 会自动将其转换为 JSON 格式），由前端负责渲染和展示。 4. 统一异常处理怎么做？ 见javaguide。 Spring 的循环依赖 1. Spring 循环依赖了解吗，怎么解决？ 两个或多个 Bean 之间相互持有对方的引用。 @Component public class CircularDependencyA { @Autowired private CircularDependencyB circB; } @Component public class CircularDependencyB { @Autowired private CircularDependencyA circA; } 有三级缓存： 一级缓存（singletonObjects）：存放最终形态的 Bean（已经实例化、属性填充、初始化），单例池，为“Spring 的单例属性”⽽⽣。一般情况我们获取 Bean 都是从这里获取的，但是并不是所有的 Bean 都在单例池里面，例如原型 Bean 就不在里面。 二级缓存（earlySingletonObjects）：存放过渡 Bean（半成品，尚未属性填充，未进行依赖注入），也就是三级缓存中ObjectFactory产生的对象，与三级缓存配合使用的，可以防止 AOP 的情况下，每次调用ObjectFactory#getObject()都是会产生新的代理对象的。 三级缓存（singletonFactories）：存放ObjectFactory，ObjectFactory的getObject()方法（最终调用的是getEarlyBeanReference()方法）可以生成原始 Bean 对象或者代理对象（如果 Bean 被 AOP 切面代理）。三级缓存只会对单例 Bean 生效。 先去 一级缓存 singletonObjects 中获取，存在就返回； 如果不存在或者对象正在创建中，于是去 二级缓存 earlySingletonObjects 中获取； 如果还没有获取到，就去 三级缓存 singletonFactories 中获取，通过执行 ObjectFacotry 的 getObject() 就可以获取该对象，获取成功之后，从三级缓存移除B，并将B对象加入到二级缓存中。 步骤 1：创建 Bean A Spring 容器开始创建 Bean A，首先将 Bean A 的创建状态标记为 “正在创建”。 实例化 Bean A，将 Bean A 的早期引用（一个 ObjectFactory 对象）放入三级缓存 singletonFactories 中。这个 ObjectFactory 对象可以在需要时返回 Bean A 的早期实例。 开始对 Bean A 进行属性注入，发现 Bean A 依赖于 Bean B。 步骤 2：创建 Bean B Spring 容器开始创建 Bean B，同样将 Bean B 的创建状态标记为 “正在创建”。 实例化 Bean B，将 Bean B 的早期引用放入三级缓存 singletonFactories 中。 开始对 Bean B 进行属性注入，发现 Bean B 依赖于 Bean A。 步骤 3：解决 Bean B 对 Bean A 的依赖 当 Bean B 需要注入 Bean A 时，Spring 容器首先从一级缓存 singletonObjects 中查找 Bean A，发现没有找到。 接着从二级缓存 earlySingletonObjects 中查找，也没有找到。 然后从三级缓存 singletonFactories 中查找，找到了 Bean A 的早期引用（ObjectFactory 对象）。 调用 ObjectFactory 的 getObject() 方法，获取 Bean A 的早期实例。如果 Bean A 需要进行 AOP 代理，会在这里创建代理对象，并将代理对象放入二级缓存 earlySingletonObjects 中，同时从三级缓存 singletonFactories 中移除该引用。 将获取到的 Bean A 的早期实例注入到 Bean B 中。 步骤 4：完成 Bean B 的创建 Bean B 完成属性注入后，进行初始化操作。 将完全创建好的 Bean B 实例放入一级缓存 singletonObjects 中，并从二级缓存 earlySingletonObjects 和三级缓存 singletonFactories 中移除相关引用。 步骤 5：完成 Bean A 的创建 由于 Bean B 已经创建完成，将 Bean B 实例注入到 Bean A 中。 Bean A 完成属性注入后，进行初始化操作。 将完全创建好的 Bean A 实例放入一级缓存 singletonObjects 中，并从二级缓存 earlySingletonObjects 和三级缓存 singletonFactories 中移除相关引用。 2. @Lazy 能解决循环依赖吗？ 如果一个 Bean 没有被标记为懒加载，那么它会在 Spring IoC 容器启动的过程中被创建和初始化。 如果一个 Bean 被标记为懒加载，那么它不会在 Spring IoC 容器启动时立即实例化，而是在第一次被请求时才创建。这可以帮助减少应用启动时的初始化时间，也可以用来解决循环依赖问题。 没有 @Lazy 的情况下：在 Spring 容器初始化 A 时会立即尝试创建 B，而在创建 B 的过程中又会尝试创建 A，最终导致循环依赖（即无限递归，最终抛出异常）。 使用 @Lazy 的情况下：Spring 不会立即创建 B，而是会注入一个 B 的代理对象。由于此时 B 仍未被真正初始化，A 的初始化可以顺利完成。等到 A 实例实际调用 B 的方法时，代理对象才会触发 B 的真正初始化。 @Lazy 能够在一定程度上打破循环依赖链，允许 Spring 容器顺利地完成 Bean 的创建和注入。但这并不是一个根本性的解决方案，尤其是在构造函数注入、复杂的多级依赖等场景中。 3. SpringBoot 允许循环依赖发生么？ 允许，但是不推荐。 Spring 事务 SpringBoot 1. 什么是 Spring Boot Starters? Spring Boot Starters 是一组便捷的依赖描述符，它们预先打包了常用的库和配置。当我们开发 Spring 应用时，只需添加一个 Starter 依赖项，即可自动引入所有必要的库和配置，快速引入相关功能。 在没有 Spring Boot Starters 之前，开发一个 RESTful 服务或 Web 应用程序通常需要手动添加多个依赖，比如 Spring MVC、Tomcat、Jackson 等。这不仅繁琐，还容易导致版本不兼容的问题。而有了 Spring Boot Starters，我们只需添加一个依赖，如 spring-boot-starter-web，即可包含所有开发 REST 服务所需的库和依赖。 这个 spring-boot-starter-web 依赖包含了 Spring MVC（用于处理 Web 请求）、Tomcat（默认嵌入式服务器）、Jackson（用于 JSON 处理）等依赖项。这种方式极大地简化了开发过程，让我们可以更加专注于业务逻辑的实现。 2. 介绍一下@SpringBootApplication 注解 @EnableAutoConfiguration: 启用 Spring Boot 的自动配置机制。它是自动配置的核心，允许 Spring Boot 根据项目的依赖和配置自动配置 Spring 应用的各个部分。 @ComponentScan: 启用组件扫描，扫描被 @Component（以及 @Service、@Controller 等）注解的类，并将这些类注册为 Spring 容器中的 Bean。默认情况下，它会扫描该类所在包及其子包下的所有类。 @Configuration: 允许在上下文中注册额外的 Bean 或导入其他配置类。它相当于一个具有 @Bean 方法的 Spring 配置类。 3. Spring Boot 的自动配置是如何实现的? Spring Boot 通过@EnableAutoConfiguration开启自动装配，通过 SpringFactoriesLoader 最终加载META-INF/spring.factories中的自动配置类实现自动装配. 4. 开发 RESTful Web 服务常用的注解有哪些？ @Autowired : 自动导入对象到类中，被注入进的类同样要被 Spring 容器管理。 @RestController : @RestController注解是@Controller和@ResponseBody的合集,表示这是个控制器 bean,并且是将函数的返回值直 接填入 HTTP 响应体中,是 REST 风格的控制器。RestController一般把 JSON 数据写入http的reponse中，而Controller返回的是页面。 @Component ：通用的注解，可标注任意类为 Spring 组件。如果一个 Bean 不知道属于哪个层，可以使用@Component 注解标注。 @Repository : 对应持久层即 Dao 层，主要用于数据库相关操作。 @Service : 对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao 层。 @Controller : 对应 Spring MVC 控制层，主要用于接受用户请求并调用 Service 层返回数据给前端页面。 @GetMapping : GET 请求 @PostMapping : POST 请求 @PutMapping : PUT 请求 @DeleteMapping : DELETE 请求 @RequestParam：用于获取查询参数 @Pathvariable：用于获取路径参数 @RequestBody：接收到数据之后会自动将数据绑定到 Java 对象上去 5.Spirng Boot 常用的两种配置文件，如何读取？ 我们可以通过 application.properties或者 application.yml 对 Spring Boot 程序进行简单的配置。 通过 @value 读取比较简单的配置信息 @Value(\"${wuhan2020}\") String wuhan2020; 通过@ConfigurationProperties读取并与 bean 绑定 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-02-09 13:57:55 "},"Chapter3/场景题.html":{"url":"Chapter3/场景题.html","title":"场景题","keywords":"","body":"亿级点赞系统的设计 数据库表： 用户表(id, username, created_time) 视频表(id, userID, created_time) 点赞表(id, userid, videoid, created_time) 点赞计数表(video_id, like_count) 索引优化：针对点赞表的用户id和视频id建立联合索引。 分库分表：对点赞表分库分表。 读写分离：写用于主库，读用于分库。 批量操作：将多个点赞请求合并为批量操作，减少数据库写入次数。 缓存优化：点赞数放在Redis中，对于热门视频可以定时去查询。 异步点赞：把点赞请求放入消息队列中，消费者异步写入数据库。 设计一个游戏排行榜功能，要求能够实时查看自己的排名和全服前几名。 需求分析 实时查看自己的排名：玩家可以随时查询自己在全服中的排名。 全服前几名：支持快速查询全服前几名玩家的信息。 高并发支持：排行榜可能被大量玩家频繁访问，需要保证性能。 数据一致性：玩家的分数更新后，排行榜需要及时更新。 使用 Redis 的 ZSet 存储排行榜数据，score 作为战力分数，player_id 作为成员。 更新战力分数的时候，直接ZADD添加元素。 查询前几名：zrevrange。 查询自己排名：zreverank Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-03-04 09:28:06 "},"Chapter3/操作系统.html":{"url":"Chapter3/操作系统.html","title":"操作系统","keywords":"","body":"操作系统基础 1. 什么是操作系统 ‌操作系统（Operating System，简称OS）是一种系统软件，用于管理和控制计算机的硬件和软件资源，提供给用户一个友好的界面和操作环境‌。 2. 操作系统主要有哪些功能 进程和线程的管理：进程的创建、撤销、阻塞、唤醒，进程间的通信等。 存储管理：内存的分配和管理、外存（磁盘等）的分配和管理等。 文件管理：文件的读、写、创建及删除等。 设备管理：完成设备（输入输出设备和外部存储设备等）的请求或释放，以及设备启动等功能。 网络管理：操作系统负责管理计算机网络的使用。网络是计算机系统中连接不同计算机的方式，操作系统需要管理计算机网络的配置、连接、通信和安全等，以提供高效可靠的网络服务。 安全管理：用户的身份认证、访问控制、文件加密等，以防止非法用户对系统资源的访问和操作。 3. 常见的操作系统有哪些 Windows（独立开发）、Unix、Linux(开源的类 Unix 操作系统)、MacOS（类Unix） 4. 用户态和内核态 4.1 二者定义 根据进程访问资源的特点，我们可以把进程在系统上的运行分为两个级别： 用户态(User Mode) : 用户态运行的进程可以直接读取用户程序的数据，拥有较低的权限，无法直接访问硬件资源。当应用程序需要执行某些需要特殊权限的操作，例如读写磁盘、网络通信等，就需要向操作系统发起系统调用请求，进入内核态。 内核态(Kernel Mode)：内核态运行的进程几乎可以访问计算机的任何资源包括系统的内存空间、设备、驱动程序等，不受限制，拥有非常高的权限。当操作系统接收到进程的系统调用请求时，就会从用户态切换到内核态，执行相应的系统调用，并将结果返回给进程，最后再从内核态切换回用户态。 内核态相比用户态拥有更高的特权级别，因此能够执行更底层、更敏感的操作。不过，由于进入内核态需要付出较高的开销（需要进行一系列的上下文切换和权限检查），应该尽量减少进入内核态的次数，以提高系统的性能和稳定性。 4.2 为什么要有内核态和用户态？只有一个内核态不行吗？ 安全性：通过对权限的划分，用户程序无法直接访问硬件资源，从而避免了恶意程序对系统资源的破坏。 稳定性：用户态程序出现问题时，不会影响到整个系统，避免了程序故障导致系统崩溃的风险。 隔离性：内核态和用户态的划分使得操作系统内核与用户程序之间有了明确的边界，有利于系统的模块化和维护。 4.3 用户态和内核态是如何切换的？ 系统调用：当用户程序需要请求操作系统服务（如文件操作、网络通信等）时，会通过系统调用切换到内核态。 中断：硬件中断（如定时器中断、I/O设备中断）会导致CPU从用户态切换到内核态，以便操作系统处理这些事件。 异常：程序运行过程中发生异常（如除零错误、非法内存访问）时，系统会切换到内核态进行处理。 4.4 系统调用 我们运行的程序基本都是运行在用户态，如果我们调用操作系统提供的内核态级别的子功能咋办呢？那就需要系统调用了！ 也就是说在我们运行的用户程序中，凡是与系统态级别的资源有关的操作（如文件管理、进程控制、内存管理等)，都必须通过系统调用方式向操作系统提出服务请求，并由操作系统代为完成。 进程和线程 1. 什么是进程和线程 进程（Process） 是指计算机中正在运行的一个程序实例。举例：你打开的微信就是一个进程。 线程（Thread） 也被称为轻量级进程，更加轻量。多个线程可以在同一个进程中同时执行，并且共享进程的资源比如内存空间、文件句柄、网络连接等。举例：你打开的微信里就有一个线程专门用来拉取别人发你的最新的消息。 2. 进程和线程区别 本质区别：进程是操作系统资源分配的基本单位，而线程是任务调度和执行的基本单位 并发关系：一个进程在其执行的过程中可以产生多个线程，多个线程可以并发处理不同的任务，更有效地利用了多处理器和多核计算机（系统动不动就要求百万级甚至千万级的并发量）。 独立关系上：基本上各进程是独立的，每个进程都有自己独立的内存空间，它们之间不会共享资源，如文件、网络连接等。而各线程则不一定，因为同一进程中的线程极有可能会相互影响。因此进程崩溃不会对其他进程产生很大影响。 在开销方面：每个进程都有独立的代码和数据空间（程序上下文），进程之间的切换会有较大的开销；多个线程之间共享进程的堆和方法区，每个线程都有自己独立的运行栈和程序计数器（PC），线程之间切换的开销小 3. 进程是分配资源的基本单位，那么这个资源指的是什么？ 虚拟内存、文件句柄、信号量等资源 4. 线程切换为什么比进程切换快，节省了什么资源？ 线程切换比进程切换快是因为线程共享同一进程的地址空间和资源，线程切换时只需切换堆栈和程序计数器等少量信息，而不需要切换地址空间，避免了进程切换时需要切换内存映射表等大量资源的开销，从而节省了时间和系统资源。 5. 线程间的通信（同步）的方式有哪些？ 互斥锁(Mutex) ：采用互斥对象机制，只有拥有互斥对象的线程才有访问公共资源的权限。因为互斥对象只有一个，所以可以保证公共资源不会被多个线程同时访问。比如 Java 中的 synchronized 关键词和各种 Lock 都是这种机制。 读写锁（Read-Write Lock） ：允许多个线程同时读取共享资源，但只有一个线程可以对共享资源进行写操作。 信号量(Semaphore) ：它允许同一时刻多个线程访问同一资源，但是需要控制同一时刻访问此资源的最大线程数量。 屏障（Barrier） ：屏障是一种同步原语，用于等待多个线程到达某个点再一起继续执行。当一个线程到达屏障时，它会停止执行并等待其他线程到达屏障，直到所有线程都到达屏障后，它们才会一起继续执行。比如 Java 中的 CyclicBarrier 是这种机制。 事件(Event) :Wait/Notify：通过通知操作的方式来保持多线程同步，还可以方便的实现多线程优先级的比较操作。 6. PCB是什么？包含哪些信息？ PCB（Process Control Block） 即进程控制块，是操作系统中用来管理和跟踪进程的数据结构，每个进程都对应着一个独立的 PCB。 你可以将 PCB 视为进程的大脑。 当操作系统创建一个新进程时，会为该进程分配一个唯一的进程 ID，并且为该进程创建一个对应的进程控制块。当进程执行时，PCB 中的信息会不断变化，操作系统会根据这些信息来管理和调度进程。 进程的描述信息，包括进程的名称、标识符等等； 进程的调度信息，包括进程阻塞原因、进程状态（就绪、运行、阻塞等）、进程优先级（标识进程的重要程度）等等； 进程对资源的需求情况，包括 CPU 时间、内存空间、I/O 设备等等。 进程打开的文件信息，包括文件描述符、文件类型、打开模式等等。 处理机的状态信息（由处理机的各种寄存器中的内容组成的），包括通用寄存器、指令计数器、程序状态字 PSW、用户栈指针。 7. 进程有哪几种状态？ 8. 进程间的通信方式有哪些？ 管道/匿名管道(Pipes) ：用于父子进程间或者兄弟进程之间的通信，一种单向通信方式，只存在于内存中的文件，允许一个进程将数据写入管道，另一个进程从管道中读取数据。 有名管道(Named Pipes)：允许无亲缘关系的进程之间进行通信。有名管道严格遵循 先进先出(First In First Out) 。有名管道以磁盘文件的方式存在，可以实现本机任意两个进程通信。 消息队列(Message Queuing) ：克服了管道通信的数据是无格式的字节流的问题，消息队列实际上是保存在内核的「消息链表」，消息队列的消息体是可以用户自定义的数据类型。消息队列支持异步通信，消息可以按照优先级进行处理。 信号(Signal) ：信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生； 信号量(Semaphores) ：信号量是一个计数器，用于多进程对共享数据的访问，信号量的意图在于进程间同步。这种通信方式主要用于解决与同步相关的问题并避免竞争条件。 共享内存(Shared memory) ：使得多个进程可以访问同一块内存空间，不同进程可以及时看到对方进程中对共享内存中数据的更新。这种方式需要依靠某种同步操作，如互斥锁和信号量等。可以说这是最有用的进程间通信方式。 套接字(Sockets) : 此方法主要用于在客户端和服务器之间通过网络进行通信。套接字是支持 TCP/IP 的网络通信的基本操作单元，可以看做是不同主机之间的进程进行双向通信的端点，简单的说就是通信的两方的一种约定，用套接字中的相关函数来完成通信过程。 9. 进程调度算法 先到先服务调度算法(FCFS，First Come, First Served) : 从就绪队列中选择一个最先进入该队列的进程为之分配资源，使它立即执行并一直执行到完成或发生某事件而被阻塞放弃占用 CPU 时再重新调度。 短作业优先的调度算法(SJF，Shortest Job First) : 从就绪队列中选出一个估计运行时间最短的进程为之分配资源，使它立即执行并一直执行到完成或发生某事件而被阻塞放弃占用 CPU 时再重新调度。 时间片轮转调度算法（RR，Round-Robin） : 时间片轮转调度是一种最古老，最简单，最公平且使用最广的算法。每个进程被分配一个时间段，称作它的时间片，即该进程允许运行的时间。 优先级调度算法（Priority）：为每个流程分配优先级，首先执行具有最高优先级的进程，依此类推。具有相同优先级的进程以 FCFS 方式执行。可以根据内存要求，时间要求或任何其他资源要求来确定优先级。 多级反馈队列调度算法（MFQ，Multi-level Feedback Queue）：前面介绍的几种进程调度的算法都有一定的局限性。如短进程优先的调度算法，仅照顾了短进程而忽略了长进程 。多级反馈队列包含多个优先级不同的队列。每个队列都有自己的调度算法，通常高优先级队列采用短作业优先（SJF）或先来先服务（FCFS）等算法，而低优先级队列可能采用轮转调度（Round Robin）等算法。多级反馈队列调度算法既能使高优先级的作业得到响应又能使短作业（进程）迅速完成，因而它是目前被公认的一种较好的进程调度算法，UNIX 操作系统采取的便是这种调度算法。 10.共享内存怎么实现的？ 共享内存的机制，就是拿出一块虚拟地址空间来，映射到相同的物理内存中。这样这个进程写入的东西，另外一个进程马上就能看到了，都不需要拷贝来拷贝去，传来传去，大大提高了进程间通信的速度。 11.什么是僵尸进程和孤儿进程？ 僵尸进程：子进程已经终止，但是其父进程仍在运行，且父进程没有调用 wait()或 waitpid()等系统调用来获取子进程的状态信息，释放子进程占用的资源，导致子进程的 PCB 依然存在于系统中，但无法被进一步使用。这种情况下，子进程被称为“僵尸进程”。避免僵尸进程的产生，父进程需要及时调用 wait()或 waitpid()系统调用来回收子进程。 孤儿进程：一个进程的父进程已经终止或者不存在，但是该进程仍在运行。这种情况下，该进程就是孤儿进程。孤儿进程通常是由于父进程意外终止或未及时调用 wait()或 waitpid()等系统调用来回收子进程导致的。为了避免孤儿进程占用系统资源，操作系统会将孤儿进程的父进程设置为 init 进程（进程号为 1），由 init 进程来回收孤儿进程的资源。 死锁 1. 什么是死锁 死锁是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。 假设有两个进程 A 和 B，以及两个资源 X 和 Y，它们的分配情况如下： 进程 占用资源 需求资源 A X Y B Y X 此时，进程 A 占用资源 X 并且请求资源 Y，而进程 B 已经占用了资源 Y 并请求资源 X。两个进程都在等待对方释放资源，无法继续执行，陷入了死锁状态。 2. 产生死锁的四个必要条件是什么? 互斥：资源必须处于非共享模式，即一次只有一个进程可以使用。如果另一进程申请该资源，那么必须等待直到该资源被释放为止。 请求和保持：一个进程至少应该占有一个资源，但又提出了新的资源请求，而该资源已被其他进程占有，此时请求进程被阻塞，但又对自己已获得的其他资源保持不放。 不可抢占：已经分配给进程的资源在该进程释放之前，不会被其他进程抢占。 循环等待：有一组等待进程 {P0, P1,..., Pn}， P0 等待的资源被 P1 占有，P1 等待的资源被 P2 占有，……，Pn-1 等待的资源被 Pn 占有，Pn 等待的资源被 P0 占有。 这四个条件是产生死锁的 必要条件 ，也就是说只要系统发生死锁，这些条件必然成立，而只要上述条件之一不满足，就不会发生死锁。 3. 解决死锁的方法 3.1 预防死锁 破坏请求和保持：一个进程必须在执行前就申请到它所需要的全部资源，并且知道它所要的资源都得到满足之后才开始执行。进程要么占有所有的资源然后开始执行，要么不占有资源，不会出现占有一些资源等待一些资源的情况。严重地降低了资源利用率，因为在每个进程所占有的资源中，有些资源是在比较靠后的执行时间里采用的。 破坏循环等待：在层次分配策略下，所有的资源被分成了多个层次，一个进程得到某一次的一个资源后，它只能再申请较高一层的资源；当一个进程要释放某层的一个资源时，必须先释放所占用的较高层的资源，按这种策略，是不可能出现循环等待链的， 3.2 避免死锁 允许同时存在四个必要条件 ，只要掌握并发进程中与每个进程有关的资源动态申请情况，做出 明智和合理的选择 ，仍然可以避免死锁。 我们将系统的状态分为 安全状态 和 不安全状态 ，每当在为申请者分配资源前先测试系统状态，若把系统资源分配给申请者会产生死锁，则拒绝分配，否则接受申请，并为它分配资源。 银行家算法：先 试探 分配给该进程资源，然后通过 安全性算法 判断分配后系统是否处于安全状态，若不安全则试探分配作废，让该进程继续等待，若能够进入到安全的状态，则就 真的分配资源给该进程。 3.3 死锁的检测 这种方法对资源的分配不加以任何限制，也不采取死锁避免措施，但系统 定时地运行一个 “死锁检测” 的程序，判断系统内是否出现死锁，如果检测到系统发生了死锁，再采取措施去解除它。 3.4 死锁的解除 当死锁检测程序检测到存在死锁发生时，应设法让其解除，让系统从死锁状态中恢复过来，常用的解除死锁的方法有以下四种： 立即结束所有进程的执行，重新启动操作系统：这种方法简单，但以前所在的工作全部作废，损失很大。 撤销涉及死锁的所有进程，解除死锁后继续运行：这种方法能彻底打破死锁的循环等待条件，但将付出很大代价，例如有些进程可能已经计算了很长时间，由于被撤销而使产生的部分结果也被消除了，再重新执行时还要再次进行计算。 逐个撤销涉及死锁的进程，回收其资源直至死锁解除。 抢占资源：从涉及死锁的一个或几个进程中抢占资源，把夺得的资源再分配给涉及死锁的进程直至死锁解除。 内存管理 1. 内存管理主要做了什么？ 内存的分配与回收：对进程所需的内存进行分配和释放，malloc 函数：申请内存，free 函数：释放内存。 地址转换：将程序中的虚拟地址转换成内存中的物理地址。 内存扩充：当系统没有足够的内存时，利用虚拟内存技术或自动覆盖技术，从逻辑上扩充内存。 内存映射：将一个文件直接映射到进程的进程空间中，这样可以通过内存指针用读写内存的办法直接存取文件内容，速度更快。 内存优化：通过调整内存分配策略和回收算法来优化内存使用效率。 内存安全：保证进程之间使用内存互不干扰，避免一些恶意程序通过修改内存来破坏系统的安全性。 2. 虚拟内存 2.1 什么是虚拟内存和物理内存？ 虚拟内存：是操作系统提供给每个运行中程序的一种地址空间，每个程序在运行时认为自己拥有的内存空间就是虚拟内存，其大小可以远远大于物理内存的大小。虚拟内存通过将程序的地址空间划分成若干个固定大小的页或段，并将这些页或者段映射到物理内存中的不同位置，从而使得程序在运行时可以更高效地利用物理内存。 物理内存：物理内存是计算机实际存在的内存，是计算机中的实际硬件部件。 2.2 虚拟内存的作用 隔离进程：物理内存通过虚拟地址空间访问，虚拟地址空间与进程一一对应。每个进程都认为自己拥有了整个物理内存，进程之间彼此隔离，一个进程中的代码无法更改正在由另一进程或操作系统使用的物理内存。 提升物理内存利用率：有了虚拟地址空间后，操作系统只需要将进程当前正在使用的部分数据或指令加载入物理内存。 简化内存管理：进程都有一个一致且私有的虚拟地址空间，程序员不用和真正的物理内存打交道，而是借助虚拟地址空间访问物理内存，从而简化了内存管理。 多个进程共享物理内存：进程在运行过程中，会加载许多操作系统的动态库。这些库对于每个进程而言都是公用的，它们在内存中实际只会加载一份，这部分称为共享内存。 提高内存使用安全性：控制进程对物理内存的访问，隔离不同进程的访问权限，提高系统的安全性。 提供更大的可使用内存空间：可以让程序拥有超过系统物理内存大小的可用内存空间。这是因为当物理内存不够用时，可以利用磁盘充当，将物理内存页（通常大小为 4 KB）保存到磁盘文件（会影响读写速度），数据或代码页会根据需要在物理内存与磁盘之间移动。 2.3 没有虚拟内存有什么问题？ 用户程序可以访问任意物理内存，可能会不小心操作到系统运行必需的内存，进而造成操作系统崩溃，严重影响系统的安全。 同时运行多个程序容易崩溃。比如你想同时运行一个微信和一个 QQ 音乐，微信在运行的时候给内存地址 1xxx 赋值后，QQ 音乐也同样给内存地址 1xxx 赋值，那么 QQ 音乐对内存的赋值就会覆盖微信之前所赋的值，这就可能会造成微信这个程序会崩溃。 程序运行过程中使用的所有数据或指令都要载入物理内存，根据局部性原理，其中很大一部分可能都不会用到，白白占用了宝贵的物理内存资源。 2.4 什么是虚拟地址和物理地址？ 物理地址（Physical Address） 是真正的物理内存中地址，更具体点来说是内存地址寄存器中的地址。程序中访问的内存地址不是物理地址，而是 虚拟地址（Virtual Address） 。 也就是说，我们编程开发的时候实际就是在和虚拟地址打交道。比如在 C 语言中，指针里面存储的数值就可以理解成为内存里的一个地址，这个地址也就是我们说的虚拟地址。 操作系统一般通过 CPU 芯片中的一个重要组件 MMU(Memory Management Unit，内存管理单元) 将虚拟地址转换为物理地址，这个过程被称为 地址翻译/地址转换（Address Translation） 。 2.5 虚拟地址与物理内存地址是如何映射的？ MMU 将虚拟地址翻译为物理地址的主要机制有 3 种: 分段机制 分页机制 段页机制 其中，现代操作系统广泛采用分页机制，需要重点关注！ 3. 什么是内存碎片？ 4. 常见的内存管理方式有哪些？ 连续内存管理：为一个用户程序分配一个连续的内存空间，内存利用率一般不高。 非连续内存管理：允许一个程序使用的内存分布在离散或者说不相邻的内存中，相对更加灵活一些。 ​ 早期计算机操作系统的一种连续内存管理方式，存在严重的内存碎片问题。块式管理会将内存分为几个固定大小的块，每个块中只包含一个进程。如果程序运行需要内存的话，操作系统就分配给它一块，如果程序运行只需要很小的空间的话，分配的这块内存很大一部分几乎被浪费了 ​ 非连续内存管理存在下面 3 种方式： 段式管理：以段(一段连续的物理内存)的形式管理/分配物理内存。应用程序的虚拟地址空间被分为大小不等的段，段是有实际意义的，每个段定义了一组逻辑信息，例如有主程序段 MAIN、子程序段 X、数据段 D 及栈段 S 等。 页式管理：把物理内存分为连续等长的物理页，应用程序的虚拟地址空间也被划分为连续等长的虚拟页，是现代操作系统广泛使用的一种内存管理方式。 段页式管理机制：结合了段式管理和页式管理的一种内存管理机制，把物理内存先分成若干段，每个段又继续分成若干大小相等的页。 5. 分段机制 应用程序的虚拟地址空间被分为大小不等的段，段是有实际意义的，每个段定义了一组逻辑信息。 5.1 段表有什么用？地址翻译过程是怎样的？ 6. 分页机制 分页机制（Paging） 把主存（物理内存）分为连续等长的物理页，应用程序的虚拟地址空间划也被分为连续等长的虚拟页。现代操作系统广泛采用分页机制。这里的页是连续等长的，不同于分段机制下不同长度的段。 6.1 页表有什么用？地址翻译过程是怎样的？ 6.2 单级页表有什么问题？为什么需要多级页表？ 如果使用单机页表，页表项太占空间。 多级页表属于时间换空间的典型场景，利用增加页表查询的次数减少页表占用的空间。 6.3 快表（TLB） 有什么用？使用 TLB 之后的地址翻译流程是怎样的？ TLB在高速缓冲区，速度比较快。TLB 的设计思想非常简单，但命中率往往非常高，效果很好。这就是因为被频繁访问的页就是其中的很小一部分。 用虚拟地址中的虚拟页号作为 key 去 TLB 中查询； 如果能查到对应的物理页的话，就不用再查询页表了，这种情况称为 TLB 命中（TLB hit)。 如果不能查到对应的物理页的话，还是需要去查询主存中的页表，同时将页表中的该映射表项添加到 TLB 中，这种情况称为 TLB 未命中（TLB miss)。 当 TLB 填满后，又要登记新页时，就按照一定的淘汰策略淘汰掉快表中的一个页。 6.4 换页机制有什么用？ 换页机制的思想是当物理内存不够用的时候，操作系统选择将一些物理页的内容放到磁盘上去，等要用到的时候再将它们读取到物理内存中。也就是说，换页机制利用磁盘这种较低廉的存储设备扩展的物理内存。 6.5 常见的页面置换算法有哪些? 最佳页面置换算法（OPT，Optimal）：优先选择淘汰的页面是以后永不使用的，或者是在最长时间内不再被访问的页面，这样可以保证获得最低的缺页率。但由于人们目前无法预知进程在内存下的若干页面中哪个是未来最长时间内不再被访问的，因而该算法无法实现，只是理论最优的页面置换算法，可以作为衡量其他置换算法优劣的标准。 先进先出页面置换算法（FIFO，First In First Out） : 最简单的一种页面置换算法，总是淘汰最先进入内存的页面，即选择在内存中驻留时间最久的页面进行淘汰。该算法易于实现和理解，一般只需要通过一个 FIFO 队列即可满足需求。不过，它的性能并不是很好。 最近最久未使用页面置换算法（LRU ，Least Recently Used）：LRU 算法赋予每个页面一个访问字段，用来记录一个页面自上次被访问以来所经历的时间 T，当须淘汰一个页面时，选择现有页面中其 T 值最大的，即最近最久未使用的页面予以淘汰。LRU 算法是根据各页之前的访问情况来实现，因此是易于实现的。OPT 算法是根据各页未来的访问情况来实现，因此是不可实现的。 最少使用页面置换算法（LFU，Least Frequently Used） : 和 LRU 算法比较像，不过该置换算法选择的是之前一段时间内使用最少的页面作为淘汰页。 时钟页面置换算法（Clock）：可以认为是一种最近未使用算法，即逐出的页面都是最近没有使用的那个。 FIFO 页面置换算法性能为何不好？ 经常访问或者需要长期存在的页面会被频繁调入调出：较早调入的页往往是经常被访问或者需要长期存在的页，这些页会被反复调入和调出。 哪一种页面置换算法实际用的比较多？ LRU 算法是实际使用中应用的比较多，也被认为是最接近 OPT 的页面置换算法。 7. 分页机制和分段机制有哪些共同点和区别？ 共同点： 都是非连续内存管理的方式。 都采用了地址映射的方法，将虚拟地址映射到物理地址，以实现对内存的管理和保护。 区别： 分页机制以页面为单位进行内存管理，而分段机制以段为单位进行内存管理。页的大小是固定的，由操作系统决定，通常为 2 的幂次方。而段的大小不固定，取决于我们当前运行的程序。 页是物理单位，即操作系统将物理内存划分成固定大小的页面，每个页面的大小通常是 2 的幂次方，例如 4KB、8KB 等等。而段则是逻辑单位，是为了满足程序对内存空间的逻辑需求而设计的，通常根据程序中数据和代码的逻辑结构来划分。 分段机制容易出现外部内存碎片，即在段与段之间留下碎片空间(不足以映射给虚拟地址空间中的段)。分页机制解决了外部内存碎片的问题，但仍然可能会出现内部内存碎片。 分页机制采用了页表来完成虚拟地址到物理地址的映射，页表通过一级页表和二级页表来实现多级映射；而分段机制则采用了段表来完成虚拟地址到物理地址的映射，每个段表项中记录了该段的起始地址和长度信息。 分页机制对程序没有任何要求，程序只需要按照虚拟地址进行访问即可；而分段机制需要程序员将程序分为多个段，并且显式地使用段寄存器来访问不同的段。 8. 段页机制 结合了段式管理和页式管理的一种内存管理机制，把物理内存先分成若干段，每个段又继续分成若干大小相等的页。 9. 局部性原理 时间局部性：由于程序中存在一定的循环或者重复操作，因此会反复访问同一个页或一些特定的页，这就体现了时间局部性的特点。为了利用时间局部性，分页机制中通常采用缓存机制来提高页面的命中率，即将最近访问过的一些页放入缓存中，如果下一次访问的页已经在缓存中，就不需要再次访问内存，而是直接从缓存中读取。 空间局部性：由于程序中数据和指令的访问通常是具有一定的空间连续性的，因此当访问某个页时，往往会顺带访问其相邻的一些页。为了利用空间局部性，分页机制中通常采用预取技术来预先将相邻的一些页读入内存缓存中，以便在未来访问时能够直接使用，从而提高访问速度。 中断和异常 1. 什么是中断？ CPU停下当前的工作任务，去处理其他事情，处理完后回来继续执行刚才的任务，这一过程便是中断。 外部中断：来源于CPU外部，必然是某个硬件产生的，所以外部中断又被称为硬件中断。 内部中断：来自于处理器内部，其中软中断是由软件主动发起的中断，常被用于系统调用（system call）；而异常则是指令执行期间CPU内部产生的错误引起的 2. 中断的流程 ‌中断请求‌：当外部设备或内部事件产生中断请求信号时，CPU会检测这些信号并确定中断源。‌ ‌中断响应‌：CPU在接收到中断请求后，会根据中断优先级暂停当前任务，并准备执行中断服务程序（ISR）。在这个过程中，CPU会保护当前的执行环境，包括程序计数器、状态寄存器和通用寄存器的内容。 ‌保护现场‌：CPU将当前执行任务的状态保存到堆栈中，以便在中断处理完毕后能够恢复到原来的状态继续执行。 ‌执行中断服务程序‌：CPU跳转到中断向量表中查找的中断服务程序的入口地址，开始执行相应的代码。 ‌中断处理‌：在中断服务程序中，CPU会执行必要的操作来处理中断，如数据读写、状态更新等。 ‌恢复现场‌：中断服务程序执行完毕后，CPU会恢复之前保存的执行环境，包括程序计数器和寄存器的内容。 ‌中断返回‌：CPU返回到被中断的任务的下一条指令处，继续执行程序。 3. 中断的作用 中断使得计算机系统具备应对对处理突发事件的能力，提高了CPU的工作效率，如果没有中断系统，CPU就只能按照原来的程序编写的先后顺序，对各个外设进行查询和处理，即轮询工作方式，轮询方法貌似公平，但实际工作效率却很低，却不能及时响应紧急事件。 文件系统 1.常见的磁盘调度算法有哪些？ 先来先服务算法（First-Come First-Served，FCFS）：按照请求到达磁盘调度器的顺序进行处理，先到达的请求的先被服务。FCFS 算法实现起来比较简单，不存在算法开销。不过，由于没有考虑磁头移动的路径和方向，平均寻道时间较长。同时，该算法容易出现饥饿问题，即一些后到的磁盘请求可能需要等待很长时间才能得到服务。 最短寻道时间优先算法（Shortest Seek Time First，SSTF）：也被称为最佳服务优先（Shortest Service Time First，SSTF）算法，优先选择距离当前磁头位置最近的请求进行服务。SSTF 算法能够最小化磁头的寻道时间，但容易出现饥饿问题，即磁头附近的请求不断被服务，远离磁头的请求长时间得不到响应。实际应用中，需要优化一下该算法的实现，避免出现饥饿问题。 扫描算法（SCAN）：也被称为电梯（Elevator）算法，基本思想和电梯非常类似。磁头沿着一个方向扫描磁盘，如果经过的磁道有请求就处理，直到到达磁盘的边界，然后改变移动方向，依此往复。SCAN 算法能够保证所有的请求得到服务，解决了饥饿问题。但是，如果磁头从一个方向刚扫描完，请求才到的话。这个请求就需要等到磁头从相反方向过来之后才能得到处理。 循环扫描算法（Circular Scan，C-SCAN）：SCAN 算法的变体，只在磁盘的一侧进行扫描，并且只按照一个方向扫描，直到到达磁盘边界，然后回到磁盘起点，重新开始循环。 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-01-21 15:08:01 "},"Chapter3/数据结构.html":{"url":"Chapter3/数据结构.html","title":"数据结构","keywords":"","body":"十大排序算法 0. 总览 1.冒泡排序 (Bubble Sort) 稳定性：稳定（相等元素之间不会相互交换，从而保证了它们的相对位置不变，因此冒泡排序被认为是稳定的排序算法） 时间复杂度：最佳：$O(n)$ ，最差：$O(n^2)$， 平均：$O(n^2)$ 空间复杂度：$O(1)$ /** * 冒泡排序 * @param arr * @return arr */ public static int[] bubbleSort(int[] arr) { for (int i = 1; i arr[j + 1]) { int tmp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = tmp; flag = false; } } if (flag) { break; } } return arr; } 2.选择排序 (Selection Sort) 稳定性：不稳定（5, 8, 5, 2, 9，使用选择排序进行升序排序，第一趟会将第一个 5 和 2 交换位置） 时间复杂度：最佳：$O(n^2)$ ，最差：$O(n^2)$， 平均：$O(n^2)$ 空间复杂度：$O(1)$ 排序方式：In-place /** * 选择排序 * @param arr * @return arr */ public static int[] selectionSort(int[] arr) { for (int i = 0; i 3.插入排序 (Insertion Sort) 稳定性：稳定 时间复杂度：最佳：$O(n)$ ，最差：$O(n^2)$， 平均：$O(n^2)$ 空间复杂度：$O(1)$ /** * 插入排序 * @param arr * @return arr */ public static int[] insertionSort(int[] arr) { for (int i = 1; i = 0 && current 4.希尔排序 (Shell Sort) 希尔排序也是一种插入排序，它是简单插入排序经过改进之后的一个更高效的版本，也称为递减增量排序算法，同时该算法是冲破 $O(n^2)$ 的第一批算法之一。 选择一个增量序列 $\\lbrace t_1, t_2, \\dots, t_k \\rbrace$，其中 $t_i \\gt t_j, i \\lt j, t_k = 1$； 按增量序列个数 k，对序列进行 k 趟排序； 每趟排序，根据对应的增量 $t$，将待排序列分割成若干长度为 $m$ 的子序列，分别对各子表进行直接插入排序。仅增量因子为 1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 稳定性：不稳定 时间复杂度：最佳：$O(nlogn)$， 最差：$O(n^2)$ 平均：$O(nlogn)$ 空间复杂度：$O(1)$ /** * 希尔排序 * * @param arr * @return arr */ public static int[] shellSort(int[] arr) { int n = arr.length; int gap = n / 2; while (gap > 0) { for (int i = gap; i = 0 && arr[preIndex] > current) { arr[preIndex + gap] = arr[preIndex]; preIndex -= gap; } arr[preIndex + gap] = current; } gap /= 2; } return arr; } 5.归并排序 (Merge Sort) 如果输入内只有一个元素，则直接返回，否则将长度为 $n$ 的输入序列分成两个长度为 $n/2$ 的子序列； 分别对这两个子序列进行归并排序，使子序列变为有序状态； 设定两个指针，分别指向两个已经排序子序列的起始位置； 比较两个指针所指向的元素，选择相对小的元素放入到合并空间（用于存放排序结果），并移动指针到下一位置； 重复步骤 3 ~ 4 直到某一指针达到序列尾； 将另一序列剩下的所有元素直接复制到合并序列尾。 稳定性：稳定(对于相等的元素，归并排序会按照它们在原数组中的先后顺序进行合并，不会改变相同元素的相对顺序。) 时间复杂度：最佳：$O(nlogn)$， 最差：$O(nlogn)$， 平均：$O(nlogn)$ 空间复杂度：$O(n)$(需要借助一个额外的辅助数组来暂存合并后的有序元素。) void merge_sort(int q[], int l, int r) { if (l >= r) return; int mid = l + r >> 1; merge_sort(q, l, mid); merge_sort(q, mid + 1, r); int k = 0, i = l, j = mid + 1; while (i 6.快速排序 (Quick Sort) 稳定性：不稳定 时间复杂度：最佳：$O(nlogn)$， 最差：$O(n^2)$，平均：$O(nlogn)$ 空间复杂度：$O(logn)$ 快速排序是递归算法(运用了分治的思想，递归树是高度) 从序列中随机挑出一个元素，做为 “基准”(pivot)； 重新排列序列，将所有比基准值小的元素摆放在基准前面，所有比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个操作结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地把小于基准值元素的子序列和大于基准值元素的子序列进行快速排序。 void quick_sort(int q[], int l, int r) { if (l >= r) return; int i = l - 1, j = r + 1, x = q[ l + r >> 1]; while (i x); if (i 7.堆排序 (Heap Sort) 堆排序是指利用堆这种数据结构所设计的一种排序算法。堆是一个近似完全二叉树的结构，并同时满足堆的性质：即子结点的值总是小于（或者大于）它的父节点。 稳定性：不稳定 时间复杂度：最佳：$O(nlogn)$， 最差：$O(nlogn)$， 平均：$O(nlogn)$ 空间复杂度：$O(1)$ 8.计数排序 (Counting Sort) 不是比较排序。 当输入的元素是 n 个 0 到 k 之间的整数时 稳定性：稳定 时间复杂度：最佳：$O(n+k)$ 最差：$O(n+k)$ 平均：$O(n+k)$ 空间复杂度：$O(k)$ 找出数组中的最大值 max、最小值 min； 创建一个新数组 C，其长度是 max-min+1，其元素默认值都为 0； 遍历原数组 A 中的元素 A[i]，以 A[i] - min 作为 C 数组的索引，以 A[i] 的值在 A 中元素出现次数作为 C[A[i] - min] 的值； 对 C 数组变形，新元素的值是该元素与前一个元素值的和，即当 i>1 时 C[i] = C[i] + C[i-1]； 创建结果数组 R，长度和原始数组一样。 从后向前遍历原始数组 A 中的元素 A[i]，使用 A[i] 减去最小值 min 作为索引，在计数数组 C 中找到对应的值 C[A[i] - min]，C[A[i] - min] - 1 就是 A[i] 在结果数组 R 中的位置，做完上述这些操作，将 count[A[i] - min] 减小 1。 /** * Gets the maximum and minimum values in the array * * @param arr * @return */ private static int[] getMinAndMax(int[] arr) { int maxValue = arr[0]; int minValue = arr[0]; for (int i = 0; i maxValue) { maxValue = arr[i]; } else if (arr[i] = 0; i--) { int idx = countArr[arr[i] - minValue] - 1; result[idx] = arr[i]; countArr[arr[i] - minValue] -= 1; } return result; } 9.桶排序 (Bucket Sort) 稳定性：稳定 时间复杂度：最佳：$O(n+k)$ 最差：$O(n^2)$ 平均：$O(n+k)$ 空间复杂度：$O(n+k)$ 桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。为了使桶排序更加高效，我们需要做到这两点： 计数排序适用于范围较小且离散的数据，而桶排序适用于范围较大且均匀分布的数据（包括浮点数）。 设置一个 BucketSize，作为每个桶所能放置多少个不同数值； 遍历输入数据，并且把数据依次映射到对应的桶里去； 对每个非空的桶进行排序，可以使用其它排序方法(插入排序等)，也可以递归使用桶排序； 从非空桶里把排好序的数据拼接起来。 假设我们要对以下数组进行桶排序：[0.42, 0.32, 0.23, 0.75, 0.56, 0.12] 假设将数据分配到 5 个桶，每个桶的范围为 [0, 0.2), [0.2, 0.4), [0.4, 0.6), [0.6, 0.8), [0.8, 1)。 将每个元素放入相应的桶： [0.42] → 桶 [0.4, 0.6) [0.32] → 桶 [0.2, 0.4) [0.23] → 桶 [0.2, 0.4) [0.75] → 桶 [0.6, 0.8) [0.56] → 桶 [0.4, 0.6) [0.12] → 桶 [0, 0.2) 每个桶内的元素进行排序： 桶 [0, 0.2) 排序后：[0.12] 桶 [0.2, 0.4) 排序后：[0.23, 0.32] 桶 [0.4, 0.6) 排序后：[0.42, 0.56] 桶 [0.6, 0.8) 排序后：[0.75] 合并所有桶中的元素，得到排序后的结果：[0.12, 0.23, 0.32, 0.42, 0.56, 0.75] 10.基数排序 (Radix Sort) 取得数组中的最大数，并取得位数，即为迭代次数 $N$（例如：数组中最大数值为 1000，则 $N=4$）； A 为原始数组，从最低位开始取每个位组成 radix 数组； 对 radix 进行计数排序（利用计数排序适用于小范围数的特点）； 将 radix 依次赋值给原数组； 重复 2~4 步骤 $N$ 次 稳定性：稳定 时间复杂度：最佳：$O(n×k)$ 最差：$O(n×k)$ 平均：$O(n×k)$ 空间复杂度：$O(n+k)$ 基数排序 vs 计数排序 vs 桶排序 这三种排序算法都利用了桶的概念，但对桶的使用方法上有明显差异： 基数排序：根据键值的每位数字来分配桶 计数排序：每个桶只存储单一键值 桶排序：每个桶存储一定范围的数值 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-02-13 19:01:05 "},"Chapter3/核心八股.html":{"url":"Chapter3/核心八股.html","title":"核心八股","keywords":"","body":"11. java创建对象有哪些方式？ 4. 获取 Class 对象的四种方式 3. 动态代理 2. I/O 流为什么要分为字节流和字符流呢? 为什么ArrayList不是线程安全的，具体来说是哪里不安全？ 2. 比较 HashSet、LinkedHashSet 和 TreeSet 三者的异同 1. HashMap 和 Hashtable 的区别 HashMap 的底层实现 HashMap 的长度为什么是 2 的幂次方 列举HashMap在多线程下可能会出现的问题？ 6. 列举HashMap在多线程下可能会出现的问题？ 7. ConcurrentHashMap 和 Hashtable 的区别 ConcurrentHashMap已经用了synchronized，为什么还要用CAS（乐观锁）呢 为什么HashMap要用红黑树而不是平衡二叉树？ HashMap的扩容机制介绍一下 请简要描述线程与进程的关系,区别及优缺点？ 4. 如何创建线程？ 说说线程的生命周期和状态? 7. Thread#sleep() 方法和 Object#wait() 方法对比 可以直接调用 Thread 类的 run 方法吗？ 3. 并发编程三个重要特性 4. volatile 关键字 4. 如何实现乐观锁？ 4. synchronized 和 volatile 有什么区别？ synchronized 和 ReentrantLock 有什么区别？ ThreadLocal 内存泄露问题是怎么导致的？如何避免？ 5. 如何跨线程传递 ThreadLocal 的值？ 3. 如何创建线程池？常用方法 start启动线程，线程池：submit对于callable，execute对于runable。 7. 线程池的拒绝策略有哪些？ 12. 线程池中线程异常后，销毁还是复用？ 3. 一个任务需要依赖另外两个任务执行完之后再执行，怎么设计？ 1. 对象的创建过程 内存分配和回收原则 死亡对象判断方法 4. 如何判断一个常量是废弃常量？ 如何判断一个类是无用的类？ 垃圾收集算法 垃圾收集器 类加载过程 JVM 中内置了三个重要的 ClassLoader 类加载器和双亲委派机制。 1. 说说自己对于 Spring MVC 了解? MVC 是模型(Model)、视图(View)、控制器(Controller)的简写，其核心思想是通过将业务逻辑、数据、显示分离来组织代码。 3. 将一个类声明为 Bean 的注解有哪些? @Component：通用的注解，可标注任意类为 Spring 组件。如果一个 Bean 不知道属于哪个层，可以使用@Component 注解标注。 @Repository : 对应持久层即 Dao 层，主要用于数据库相关操作。 @Service : 对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao 层。 @Controller : 对应 Spring MVC 控制层，主要用于接受用户请求并调用 Service 层返回数据给前端页面。 11. Bean 的生命周期了解么? 创建 Bean 的实例：Bean 容器首先会找到配置文件中的 Bean 定义，然后使用 Java 反射 API 来创建 Bean 的实例。 Bean 属性赋值/填充：为 Bean 设置相关属性和依赖，例如@Autowired 等注解注入的对象、@Value 注入的值、setter方法或构造函数注入依赖和值、@Resource注入的各种资源。 Bean 初始化 如果 Bean 实现了 BeanNameAware 接口，调用 setBeanName()方法，传入 Bean 的名字。 如果 Bean 实现了 BeanClassLoaderAware 接口，调用 setBeanClassLoader()方法，传入 ClassLoader对象的实例。 如果 Bean 实现了 BeanFactoryAware 接口，调用 setBeanFactory()方法，传入 BeanFactory对象的实例。 与上面的类似，如果实现了其他 *.Aware接口，就调用相应的方法。 如果有和加载这个 Bean 的 Spring 容器相关的 BeanPostProcessor 对象，执行postProcessBeforeInitialization() 方法 如果 Bean 实现了InitializingBean接口，执行afterPropertiesSet()方法。 如果 Bean 在配置文件中的定义包含 init-method 属性，执行指定的方法。 如果有和加载这个 Bean 的 Spring 容器相关的 BeanPostProcessor 对象，执行postProcessAfterInitialization() 方法。 销毁 Bean ：销毁并不是说要立马把 Bean 给销毁掉，而是把 Bean 的销毁方法先记录下来，将来需要销毁 Bean 或者销毁容器的时候，就调用这些方法去释放 Bean 所持有的资源。 如果 Bean 实现了 DisposableBean 接口，执行 destroy() 方法。 如果 Bean 在配置文件中的定义包含 destroy-method 属性，执行指定的 Bean 销毁方法。或者，也可以直接通过@PreDestroy 注解标记 Bean 销毁之前执行的方法。 整体上可以简单分为四步：实例化 —> 属性赋值 —> 初始化 —> 销毁。 初始化这一步涉及到的步骤比较多，包含 Aware 接口的依赖注入、BeanPostProcessor 在初始化前后的处理以及 InitializingBean 和 init-method 的初始化操作。 销毁这一步会注册相关销毁回调接口，最后通过DisposableBean 和 destory-method 进行销毁。 3. SpringMVC 工作原理了解吗? 客户端（浏览器）发送请求， DispatcherServlet拦截请求。 DispatcherServlet 根据请求信息调用 HandlerMapping 。HandlerMapping 根据 URL 去匹配查找能处理的 Handler（也就是我们平常说的 Controller 控制器） ，并会将请求涉及到的拦截器和 Handler 一起封装。 DispatcherServlet 调用 HandlerAdapter适配器执行 Handler 。 Handler 完成对用户请求的处理后，会返回一个 ModelAndView 对象给DispatcherServlet，ModelAndView 顾名思义，包含了数据模型以及相应的视图的信息。Model 是返回的数据对象，View 是个逻辑上的 View。 ViewResolver 会根据逻辑 View 查找实际的 View。 DispaterServlet 把返回的 Model 传给 View（视图渲染）。 把 View 返回给请求者（浏览器） 上述流程是传统开发模式（JSP，Thymeleaf 等）的工作原理。 对于前后端分离时，后端通常不再返回具体的视图，而是返回纯数据（通常是 JSON 格式，Spring 会自动将其转换为 JSON 格式），由前端负责渲染和展示。 1. Spring 循环依赖了解吗，怎么解决？ 两个或多个 Bean 之间相互持有对方的引用。 @Component public class CircularDependencyA { @Autowired private CircularDependencyB circB; } @Component public class CircularDependencyB { @Autowired private CircularDependencyA circA; } 有三级缓存： 一级缓存（singletonObjects）：存放最终形态的 Bean（已经实例化、属性填充、初始化），单例池，为“Spring 的单例属性”⽽⽣。一般情况我们获取 Bean 都是从这里获取的，但是并不是所有的 Bean 都在单例池里面，例如原型 Bean 就不在里面。 二级缓存（earlySingletonObjects）：存放过渡 Bean（半成品，尚未属性填充，未进行依赖注入），也就是三级缓存中ObjectFactory产生的对象，与三级缓存配合使用的，可以防止 AOP 的情况下，每次调用ObjectFactory#getObject()都是会产生新的代理对象的。 三级缓存（singletonFactories）：存放ObjectFactory，ObjectFactory的getObject()方法（最终调用的是getEarlyBeanReference()方法）可以生成原始 Bean 对象或者代理对象（如果 Bean 被 AOP 切面代理）。三级缓存只会对单例 Bean 生效。 先去 一级缓存 singletonObjects 中获取，存在就返回； 如果不存在或者对象正在创建中，于是去 二级缓存 earlySingletonObjects 中获取； 如果还没有获取到，就去 三级缓存 singletonFactories 中获取，通过执行 ObjectFacotry 的 getObject() 就可以获取该对象，获取成功之后，从三级缓存移除B，并将B对象加入到二级缓存中。 步骤 1：创建 Bean A Spring 容器开始创建 Bean A，首先将 Bean A 的创建状态标记为 “正在创建”。 实例化 Bean A，将 Bean A 的早期引用（一个 ObjectFactory 对象）放入三级缓存 singletonFactories 中。这个 ObjectFactory 对象可以在需要时返回 Bean A 的早期实例。 开始对 Bean A 进行属性注入，发现 Bean A 依赖于 Bean B。 步骤 2：创建 Bean B Spring 容器开始创建 Bean B，同样将 Bean B 的创建状态标记为 “正在创建”。 实例化 Bean B，将 Bean B 的早期引用放入三级缓存 singletonFactories 中。 开始对 Bean B 进行属性注入，发现 Bean B 依赖于 Bean A。 步骤 3：解决 Bean B 对 Bean A 的依赖 当 Bean B 需要注入 Bean A 时，Spring 容器首先从一级缓存 singletonObjects 中查找 Bean A，发现没有找到。 接着从二级缓存 earlySingletonObjects 中查找，也没有找到。 然后从三级缓存 singletonFactories 中查找，找到了 Bean A 的早期引用（ObjectFactory 对象）。 调用 ObjectFactory 的 getObject() 方法，获取 Bean A 的早期实例。如果 Bean A 需要进行 AOP 代理，会在这里创建代理对象，并将代理对象放入二级缓存 earlySingletonObjects 中，同时从三级缓存 singletonFactories 中移除该引用。 将获取到的 Bean A 的早期实例注入到 Bean B 中。 步骤 4：完成 Bean B 的创建 Bean B 完成属性注入后，进行初始化操作。 将完全创建好的 Bean B 实例放入一级缓存 singletonObjects 中，并从二级缓存 earlySingletonObjects 和三级缓存 singletonFactories 中移除相关引用。 步骤 5：完成 Bean A 的创建 由于 Bean B 已经创建完成，将 Bean B 实例注入到 Bean A 中。 Bean A 完成属性注入后，进行初始化操作。 将完全创建好的 Bean A 实例放入一级缓存 singletonObjects 中，并从二级缓存 earlySingletonObjects 和三级缓存 singletonFactories 中移除相关引用。 1. 什么是 Spring Boot Starters? Spring Boot Starters 是一组便捷的依赖描述符，它们预先打包了常用的库和配置。当我们开发 Spring 应用时，只需添加一个 Starter 依赖项，即可自动引入所有必要的库和配置，快速引入相关功能。 在没有 Spring Boot Starters 之前，开发一个 RESTful 服务或 Web 应用程序通常需要手动添加多个依赖，比如 Spring MVC、Tomcat、Jackson 等。这不仅繁琐，还容易导致版本不兼容的问题。而有了 Spring Boot Starters，我们只需添加一个依赖，如 spring-boot-starter-web，即可包含所有开发 REST 服务所需的库和依赖。 这个 spring-boot-starter-web 依赖包含了 Spring MVC（用于处理 Web 请求）、Tomcat（默认嵌入式服务器）、Jackson（用于 JSON 处理）等依赖项。这种方式极大地简化了开发过程，让我们可以更加专注于业务逻辑的实现。 2. 介绍一下@SpringBootApplication 注解 @EnableAutoConfiguration: 启用 Spring Boot 的自动配置机制。它是自动配置的核心，允许 Spring Boot 根据项目的依赖和配置自动配置 Spring 应用的各个部分。 @ComponentScan: 启用组件扫描，扫描被 @Component（以及 @Service、@Controller 等）注解的类，并将这些类注册为 Spring 容器中的 Bean。默认情况下，它会扫描该类所在包及其子包下的所有类。 @Configuration: 允许在上下文中注册额外的 Bean 或导入其他配置类。它相当于一个具有 @Bean 方法的 Spring 配置类。 3. Spring Boot 的自动配置是如何实现的? Spring Boot 通过@EnableAutoConfiguration开启自动装配，通过 SpringFactoriesLoader 最终加载META-INF/spring.factories中的自动配置类实现自动装配. HTTP和HTTPS二者区别 HTTP 是超文本传输协议，信息是明文传输，存在安全风险的问题。HTTPS 则解决 HTTP 不安全的缺陷，在 TCP 和 HTTP 网络层之间加入了 SSL/TLS 安全协议，使得报文能够加密传输。 HTTP 连接建立相对简单， TCP 三次握手之后便可进行 HTTP 的报文传输。而 HTTPS 在 TCP 三次握手之后，还需进行 SSL/TLS 的握手过程，才可进入加密报文传输。 两者的默认端口不一样，HTTP 默认端口号是 80，HTTPS 默认端口号是 443。 HTTPS 协议需要向 CA（证书权威机构）申请数字证书，来保证服务器的身份是可信的。 对称+非对称、摘要算法+数字签名、身份证书 TCP、UDP区别、应用场景 MySQL 基础架构 1. 结构 连接器： 身份认证和权限相关(登录 MySQL 的时候)。 查询缓存： 执行查询语句的时候，会先查询缓存（MySQL 8.0 版本后移除，因为这个功能不太实用）。 分析器： 没有命中缓存的话，SQL 语句就会经过分析器，分析器说白了就是要先看你的 SQL 语句要干嘛，再检查你的 SQL 语句语法是否正确。 优化器： 按照 MySQL 认为最优的方案去执行。 执行器： 执行语句，然后从存储引擎返回数据。 执行语句之前会先判断是否有权限，如果没有权限的话，就会报错。 插件式存储引擎：主要负责数据的存储和读取，采用的是插件式架构，支持 InnoDB、MyISAM、Memory 等多种存储引擎。InnoDB 是 MySQL 的默认存储引擎，绝大部分场景使用 InnoDB 就是最好的选择。 2. SQL语句在MySQL中的执行过程 查询语句的执行流程如下：权限校验（如果命中缓存）--->查询缓存--->分析器--->优化器--->权限校验--->执行器--->引擎 更新语句执行流程如下：分析器---->权限校验---->执行器--->引擎---redo log(prepare 状态)--->binlog--->redo log(commit 状态) Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-02-24 21:04:16 "},"Chapter3/深入八股.html":{"url":"Chapter3/深入八股.html","title":"深入八股","keywords":"","body":"对java序列化有了解吗？ Java 序列化是将对象转换为字节流的过程，以便可以将其保存到文件、通过网络传输或在内存中缓存。 JDK自带的序列化，要让一个类支持序列化，需要实现 java.io.Serializable 接口。 缺点： 生成的字节流体积大。可读性查。 Java 特有的，其他语言无法直接解析其生成的字节流。 JDK 反序列化机制存在安全漏洞，攻击者可以通过构造恶意字节流执行任意代码。 Jackson 库序列化，广泛用于Spring框架。我项目使用jackson的objectmapper序列化。 对java反序列框架有了解吗？ 使用jackson进行反序列化。 讲一下java的常用gc算法以及各自的侧重点 标记-清除：先标记不被回收的，在清除没有被标记的。会产生较多碎片。适用于老年代。 标记-整理：先标记不被回收的，将存活的对象向内存一端移动，然后清理边界外的内存。不会产生较多碎片。适用于老年代。 复制算法：将内存分为两块，每次只使用其中一块。当一块内存用满时，将存活的对象复制到另一块内存，然后清空当前内存。内存利用率较低。适用于新生代。 新生代中的对象大多数是临时对象，生命周期很短，很快就会被回收。 复制算法只需要复制少量存活对象，效率高。新生代的内存区域通常较小（如几十MB到几百MB），复制算法的内存开销（需要一半内存作为空闲区域）可以接受。 cms和g1各自优缺点和适用场景，对比一下这两个 CMS（Concurrent Mark Sweep）收集器是 HotSpot 虚拟机第一款真正意义上的并发收集器。已经过时。 优点：并发收集所以低停顿。 缺点： 标记清除算法产生较多碎片。 在并发清理阶段，应用程序可能产生新的垃圾（浮动垃圾），这些垃圾只能等到下一次GC时回收。 使用场景：适用于老年代 G1,目前在使用的垃圾收集器。 G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region(这也就是它的名字 Garbage-First 的由来) ，保证了 G1 收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。 并行与并发：G1 能充分利用 CPU、多核环境下的硬件优势，使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间。部分其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行。 空间整合：与 CMS 的“标记-清除”算法不同，G1 从整体来看是基于“标记-整理”算法实现的收集器；从局部上来看是基于“标记-复制”算法实现的，产生的内存碎片少 可预测的停顿：这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在垃圾收集上的时间不得超过 N 毫秒。 分代收集：虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。 缺点： 实现复杂：G1的实现较为复杂，可能导致额外的性能开销。 使用场景： 适用于大内存和低延迟场景中，整堆收集。 讲一下有哪些场景打破双亲委派机制 Java 的JDBC允许第三方提供实现类。 DriverManager位于rt.jar` 中，由启动类加载器加载，而数据库驱动类由第三方提供，由应用类加载器加载。 启动类加载器无法直接加载应用类路径下的类，因此需要通过线程上下文类加载器（Thread Context ClassLoader）打破双亲委派机制。 动态代理：动态代理（如 JDK 动态代理、CGLIB）需要动态生成代理类并加载。动态代理生成的类通常需要由自定义类加载器加载，而不委托父类加载器。 对springboot自动化配置有了解吗？ @SpringBootApplication 注解中包含了 @EnableAutoConfiguration，用通过 SpringFactoriesLoader 最终加载META-INF/spring.factories中的自动配置类实现自动装配. 慢sql如何排查？可能有哪些原因导致慢sql 开启慢查询日志， 缺少索引，索引失效。 mysql的索引是什么？b+树，相较于hash有什么优点 范围查询+有序 讲一下tcp报文格式 调表的时间复杂度空间复杂度 O(logn), 跳表通过多层链表存储数据，每层的节点数逐渐减少，空间复杂度为线性级别。 n+n/2+n/4+⋯≈2n，因此空间复杂度为 O(n)。 如果一个进程创建了过多的线程（比如几万个），可能会导致以下问题： 每个线程都需要分配独立的栈空间。如果创建了几万个线程，内存消耗会非常巨大，可能导致内存耗尽（Out of Memory）。 线程切换时，操作系统需要保存当前线程的状态（如寄存器、程序计数器等）并恢复下一个线程的状态。过多的线程会导致频繁的上下文切换，消耗大量 CPU 资源，降低系统整体性能。 操作系统对单个进程可以创建的线程数有限制（如 Linux 的 ulimit -u 或 /proc/sys/kernel/threads-max）。超过限制会导致线程创建失败。 http post如何实现幂等性？ 服务端记录多个历史版本，请求的时候携带版本号。 Mysql事务出现死锁怎么处理？除了顺序分配资源以外？ MySQL 内置了死锁检测机制，当检测到死锁时，会自动选择一个事务作为“牺牲者”，回滚该事务以解除死锁。 了解过缓存行或者伪共享吗？ 缓存行是 CPU 缓存中的最小数据单元，通常大小为 64 字节（具体大小取决于 CPU 架构）。当 CPU 从内存中读取数据时，会一次性加载一个缓存行，而不是单个字节或字。 伪共享是指多个线程同时修改位于同一个缓存行中的不同变量，导致缓存行在 CPU 核心之间频繁无效化，从而降低性能。 对应Redis的热点key有什么解决方案？ 推荐： Key 分片 思路：将热点 Key 拆分为多个子 Key，分散到不同的 Redis 节点。 实现例如，将 hot_key 拆分为 hot_key_1、hot_key_2、hot_key_3，分别存储在不同的节点 优点：分散热点 Key 的访问压力。 缺点：数据一致性需要额外处理。 本地缓存 思路：使用本地缓存工具（如 Guava Cache、Caffeine）缓存热点 Key 的值。 优点：减少对 Redis 的访问压力。 缺点：本地缓存占用应用服务器的内存。 保底： 缓存降级 在 Redis 无法承受压力时，降级到其他存储或直接返回默认值。 优点：保证系统的可用性。 缺点：数据可能不准确。 限流 对热点 Key 的访问进行限流，避免单个节点过载。 实现：使用限流工具（如 Redis 的 INCR 命令或分布式限流框架）限制访问频率。超出限流阈值时，直接返回错误或默认值。 优点：保护 Redis 节点不被压垮。 缺点：可能影响用户体验。 新技术： 使用分布式缓存 将热点 Key 存储到多个分布式缓存节点。通过哈希算法决定访问哪个节点。 优点：分散热点 Key 的访问压力。 缺点：需要引入新的技术栈，增加运维成本。 如何解决缓存击穿问题？ 解决方案 逻辑过期：只允许一个线程去重建缓存，其他线程返回脏数据。 互斥锁：只允许一个线程去获得锁，其他线程等待重建完成。 预防方案： 提前预热缓存。 保底方案： 在缓存失效时，通过限流或熔断机制控制请求量，避免数据库被压垮。 为什么数据库采用b+树，不是b树？ 树的高度低，查询效率高，查询速度稳定：因为非叶子节点，只存索引，不存数据，树的高度更低。 适合范围查找和排序操作：所有数据都存储在叶子节点，且叶子节点通过指针连接成链表，便于范围查询。 更高效的插入和删除：插入和删除操作主要集中在叶子节点，且叶子节点通过指针连接，调整更简单。而b树插入和删除可能涉及内部节点，调整更复杂。 缓存一致性 什么是TCP粘包和拆包？如何实现？ TCP粘包： 发生在应用层。用户发送的多条数据包，比如hello和world，使用一个TCP来发送，接收方无法区分两条消息的边界。 使用固定长度的数据包、使用特殊字符分割、使用标志位+长度。 TCP拆包： 发生在传输层。用户发生的一条数据包，由于MSS的限制，被用多条TCP发生。 解决方案： 使用HTTP，使用消息头加消息体（长度）。 Netty 供了多种内置的解码器来解决粘包和拆包问题。 字节的文化 创业、多元、务实、坦诚、极致、共同成长 说一下你对redis的理解 高性能的键值对存储系统，常用作缓存中间件。 基于内存，NoSQL，优化过的数据结构，支持持久化、集群。 MySQL最多可以存多少数据？为什么是2000W而不是2亿？ 从理论上讲，MySQL 单表可以存储 数十亿甚至上百亿 条数据。 但是维护成本显著增高：单表数据量超过 2000W 时，B+ 树的深度可能从 3 层增加到 4 层，查询时需要多一次磁盘 I/O，性能明显下降。当单表数据量超过 2000W 时，通常建议使用 分区表 或 分表 来优化性能。 UV和DAU怎么统计？ UV 是指在 一定时间范围内，访问网站或应用的 独立用户数量。同一个用户无论访问多少次，都只计为 1 个 UV。 DAU 是指在 一天内，访问网站或应用的 独立用户数量。同一个用户无论访问多少次，都只计为 1 个 DAU。 Redis的HyperLogLog ， 占用内存极小，且 计算速度快，非常适合处理大规模数据的去重统计。 HyperLogLog 是 Redis 提供的一种 基数统计 算法，用于高效地估计一个集合中 唯一元素的数量（即基数）。它的特点是 占用内存极小，且 计算速度快，非常适合处理大规模数据的去重统计。 优点 内存占用低：每个 HyperLogLog 仅占用 12KB 内存。 计算速度快：时间复杂度为 O(1)，适合实时统计。 支持大规模数据：可以统计多达 2^64 个唯一元素。 缺点 近似统计：结果存在一定误差，不适合需要精确统计的场景。 不支持元素查询：无法判断某个元素是否已存在于 HyperLogLog 中。 MySQL的主从同步是如何实现的 使用binLog日志。 static关键字 volatile 和 AtomicInteger的区别 AtomicIntege原子类使用了volatile关键字修饰，保证了可见性，但是不能保证原子性，如 i++ 是非原子的。 AtomicIntege原子类提供了一些复合操作函数，使用CAS（底层是unsafe类的nativ方法）保证原子性。 进程间可以资源共享吗？ 其实就是进程之间通信。 分布式锁 实现思路： 利用set nx ex获取锁，并设置过期时间，保存线程标示 释放锁时先判断线程标示是否与自己一致，一致则删除锁。（Lua脚本保持原子性） Redis分布式锁特性： 利用set nx满足互斥性 利用set ex保证故障时锁依然能释放，避免死锁，提高安全性 利用Redis集群保证高可用和高并发特性 如果要实现可重入，可以把string改成hash结构。 redis.call('hset', key, 'threadId', threadId) redis.call('hset', key, 'count', 1) redis.call('expire', key, expireTime) 但是还是有缺点，没有超市续约功能。 Redisson分布式锁原理 可重入：利用hash结构记录线程id和重入次数 可重试：利用信号量和PubSub功能实现等待、唤醒，获取锁失败的重试机制，不浪费cpu资源 超时续约：利用watchD0g，每隔一段时间(releaseTime/3)，重置超时时间，也就是重新expire。 缺点：redis单个节点崩溃就会失效， 解决方案：联锁， 通过在多个 Redis 实例（通常是 5 个）上同时获取锁，来确保即使某个 Redis 实例宕机，锁依然安全有效。 单例模式有哪些 ⼀个单例类在任何情况下都只存在⼀个实例。 构造⽅法必须是私有的。 由⾃⼰创建⼀个私有的静态变量存储实例。 对外提供⼀个静态公有⽅法获取实例。 饿汉式：在类加载时就创建实例，线程安全。 public class Singleton { private static final Singleton INSTANCE = new Singleton(); private Singleton() {} public static Singleton getInstance() { return INSTANCE; } } 懒汉式，在第一次调用 getInstance 时创建实例。线程不安全，需要额外处理多线程问题。 public class Singleton { private static Singleton instance; private Singleton() {} public static Singleton getInstance() { if (instance == null) { instance = new Singleton(); } return instance; } } 线程安全的double-check的懒汉式 //线程安全的单例模式 public class Singleton { // 使用 volatile 关键字确保 instance 的可见性 private static volatile Singleton instance; // 私有构造函数，防止外部实例化 private Singleton() { // 初始化代码 } // 获取单例实例的静态方法 public static Singleton getInstance() { // 第一次检查，避免不必要的同步 if (instance == null) { // 加锁，确保线程安全 synchronized (Singleton.class) { // 第二次检查，防止多个线程同时通过第一次检查 if (instance == null) { instance = new Singleton(); } } } return instance; } } MySQL查询语句执行顺序 FROM → 2. WHERE → 3. GROUP BY → 4. HAVING → 5. SELECT → 6. DISTINCT → 7. ORDER BY → 8. LIMIT。 HTTP1.1,2.0,3.0 HTTP1.1 队头阻塞：http会先请求html网页，然后请求css，图片等东西，如果css不收到，后面的请求也不会发出。 基于文本协议，明文传输 ，不安全 不支持服务端推送 HTTP2 多路复用解决队头堵塞：多个请求和响应可以在同一个 TCP 连接上并行传输，互不干扰。但是HTTP/2 仍然依赖 TCP 作为传输层协议，而 TCP 是面向连接的、可靠的协议。如果 TCP 数据包在传输过程中丢失，TCP 会重传丢失的包，导致后续数据包被阻塞（即使它们已经到达接收端）。 二进制协议，性能高，一定程度加密。 头部压缩（HPACK）：减少了 HTTP 头部的冗余数据。 服务器推送（Server Push）：服务器可以主动向客户端推送资源，减少请求次数。 HTTP/3 基于 QUIC 传输层协议，运行在 UDP 之上，完全解决了 TCP 的队头阻塞问题。 UDP 本身不提供可靠性，因此可以在应用层（如 QUIC）实现自定义的可靠性机制 默认加密（使用 TLS 1.3）。 读写穿透 读/写穿透模式 通过将缓存与数据库整合为一个服务，由服务维护一致性，简化了调用者的操作。实现该模式的关键在于： 读操作：先查缓存，未命中时加载数据库数据并写入缓存。 写操作：先更新数据库，再更新缓存。 服务封装：提供统一的接口，隐藏缓存与数据库的细节。 通过这种模式，可以显著提升系统性能，同时保证数据的一致性。希望以上内容能帮助你实现读/写穿透模式！ 异步缓存写入 读操作 主要依赖缓存，缓存命中时直接返回数据，缓存未命中时从数据库加载数据并写入缓存。 写操作 只操作缓存，由后台线程异步将缓存数据持久化到数据库，缓存和数据库之间可能存在短暂的不一致，但最终会保持一致。 Redis内存是有上限的，如何进行淘汰？ redis.conf` 中设置 `maxmemory-policy 默认策略：当内存不足时，新写入操作会返回错误，不会淘汰任何数据。 allkeys-lru：从所有键中淘汰最近最少使用（Least Recently Used, LRU）的键。 volatile-lru 从设置了过期时间的键中淘汰最近最少使用的键。 volatile-ttl：设置了过期时间的键中淘汰剩余生存时间（Time To Live, TTL）最短的键。 MySQl执行查询语句后的日志如何记录的 通用查询日志、慢查询日志、错误查询日志。 数据库错误恢复如何实现的 RedoLog（已提交但未写入磁盘），UndoLog（回滚未提交的事务） 检查点： 数据库定期将内存中已修改的数据页写入磁盘。 写入完成后，记录一个检查点，表示在此之前的日志不再需要重放。 什么是分布式事务 分布式事务是指跨越多个分布式系统或数据库的事务操作，需要保证这些操作要么全部成功，要么全部失败，以满足事务的 ACID 特性（原子性、一致性、隔离性、持久性）。 两阶段提交（2PC，Two-Phase Commit） 阶段一（准备阶段） 事务协调者向所有参与者发送准备请求。 参与者执行事务操作，但不提交，返回准备结果（成功或失败）。 阶段二（提交阶段） 如果所有参与者都准备成功，协调者发送提交请求，参与者提交事务。 如果有参与者准备失败，协调者发送回滚请求，参与者回滚事务。 优点：强一致性。 缺点：同步阻塞、性能低、单点故障。 线程之间如何通信，举例子 为什么Redis如此快 基于内存 优化过的数据结构 单线程模型，没有线程切换，也不会存在锁竞争 非阻塞IO，比如epoll模式 网络协议简单高效 http和websorcket区别 HTTP: HTTP 是一种请求-响应协议。客户端（如浏览器）向服务器发送请求，服务器处理请求后返回响应。 WebSocket: WebSocket 是一种全双工通信协议。客户端和服务器之间建立一个持久的连接，双方可以随时发送数据，而不需要等待请求。WebSocket 连接一旦建立，就可以持续通信，直到其中一方主动关闭连接。 HTTP: 每次请求和响应都需要携带完整的 HTTP 头部信息，包括方法、路径、状态码、Cookie 等。 WebSocket: 在连接建立后，数据传输时只需要携带少量的控制信息，头部开销较小。 HTTP: HTTP 本身不支持实时通信。适用于传统的 Web 应用，如网页浏览、文件下载。 WebSocket: WebSocket 是专门为实时通信设计的。它允许服务器主动向客户端推送数据，非常适合需要实时交互的应用场景，如在线聊天、实时游戏、股票行情等。 对于Java的OOM(Out of Memory)如何解决 堆内存不足：对象过多，堆内存耗尽。 栈内存不足：线程栈内存耗尽（通常是递归调用过深或线程过多）。 元空间（Metaspace）不足：加载的类过多，元空间内存耗尽。 解决方案：对于大内存应用，使用 G1 或 ZGC 垃圾回收器。增加栈和堆空间。及时释放资源 DNS为什么使用UDP而不是TCP 低开销 UDP无需建立和断开连接，减少了通信开销，适合DNS查询这种短小的请求。 速度快 UDP没有握手过程，响应更快，适合对实时性要求高的DNS查询。 小数据包 DNS查询和响应通常很小，UDP的512字节限制在大多数情况下足够使用。 尽管UDP是首选，但在以下情况下DNS会使用TCP： 响应数据超过512字节（启用EDNS0时）。 区域传输（AXFR/IXFR）需要可靠传输。 某些DNSSEC查询需要TCP的可靠性。 TIME_WAIT 堆积是什么原因如何解决 短连接过多：如果客户端频繁创建和关闭 TCP 连接（如 HTTP 短连接），会导致大量连接进入 TIME_WAIT 状态。 主动关闭连接的一方：TCP 连接中，主动关闭连接的一方会进入 TIME_WAIT 状态。如果服务器或客户端频繁主动关闭连接，会导致 TIME_WAIT 堆积。 使用长连接：尽量避免频繁创建和关闭连接，改用长连接（如 HTTP 的 Keep-Alive） 确保连接关闭由合适的一方发起。例如，尽量让客户端主动关闭连接，减少服务器端的TIME_WAIT 状态。 如果是服务器端问题，可以通过增加服务器或使用负载均衡分散连接压力。 DNS有什么安全问题，什么是DNSSEC DNS欺骗、劫持，重定向到攻击者的钓鱼网站。 DNSSEC（DNS Security Extensions） 是一组用于增强 DNS 安全性的扩展协议。它通过数字签名和公钥加密技术，解决了 DNS 的安全问题。 项目频繁出现fullgc如何排查？ 看看堆内存设置是否过小。 避免频繁创建和销毁对象，尽量复用对象。 检查是否有大对象频繁创建。 Redis 某个节点失效，大量的请求打过来，怎么办。 使用Redis Sentinel 或 Redis Cluster 实现高可用。 手动故障转移。 如果 Redis 不可用，可以暂时降级到本地缓存或数据库，确保服务基本可用。 MyBatis的缓存机制 一级缓存（Local Cache） 特点 作用范围：默认开启，作用范围是 SqlSession 级别。 生命周期：与 SqlSession 绑定，当 SqlSession 关闭或清空时，缓存也会被清空。 共享性：一级缓存是 SqlSession 私有的，不同 SqlSession 之间无法共享。 工作原理 在同一个 SqlSession 中，如果多次执行相同的 SQL 查询（相同的 SQL 和参数），MyBatis 会从一级缓存中直接返回结果，而不会再次查询数据库。 如果执行了 INSERT、UPDATE、DELETE 操作，MyBatis 会自动清空一级缓存，以保证数据一致性。 二级缓存（Global Cache） 作用范围：默认关闭，需要手动开启，作用范围是 Mapper 级别。 生命周期：与应用程序的生命周期一致，只有当应用程序关闭时，缓存才会被清空。 共享性：二级缓存是跨 SqlSession 的，多个 SqlSession 可以共享同一个二级缓存。 工作原理 当 SqlSession 提交或关闭时，MyBatis 会将一级缓存中的数据存入二级缓存。 后续的 SqlSession 查询时，如果二级缓存中存在数据，则直接从缓存中返回结果，而不会访问数据库。 如果执行了 INSERT、UPDATE、DELETE 操作，MyBatis 会自动清空二级缓存，以保证数据一致性。 二者区别 一级缓存：适合单次会话中重复查询的场景。 二级缓存：适合跨会话共享数据且数据更新频率较低的场景。 二者都可能出现脏读。 Spring的bean为什么默认是单例？ Spring 容器只会创建一个 Bean 实例，并在整个应用程序中共享。这避免了频繁创建和销毁对象的开销，提高了性能。 在大多数场景中，Bean 是无状态的，线程安全。 常见linux命令 ls：列出目录内容 cd: 切换目录 ping：测试链接情况 cp：复制文件 -r递归复制 tar：压缩文件。 mv：移动文件 rm：删除文件 -r递归删除 -f强制删除 vim：文本编辑器 :wq保存 i编辑模式 AOP的原理 Spring AOP使用的是动态代理。所谓的动态代理，就是说AOP框架不会去修改字节码，而是在内存中临时为方法生成一个AOP对象，这个AOP对象包含了目标对象的全部方法，并且在特定的切点做了增强处理，并回调原对象的方法。 动态代理原理 动态代理通过反射机制调用目标对象的方法，用于在运行时动态获取和操作类的信息。 两种实习方式：JDK动态代理、CGLIB动态代理。 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-03-04 10:08:33 "},"Chapter3/计算机网络.html":{"url":"Chapter3/计算机网络.html","title":"计算机网络","keywords":"","body":"计算机网络基础 1. TCP/IP网络模型 1.1 应用层 为用户提供应用功能，比如 HTTP、DNS等。 1.2 传输层 为应用层提供网络支持,TCP 和 UDP。 TCP TCP：流量控制、超时重传、拥塞控制等，这些都是为了保证数据包能可靠地传输给对方。 分段：应用需要传输的数据可能会非常大，如果直接传输就不好控制，因此当传输层的数据包大小超过 MSS（TCP 最大报文段长度） ，就要将数据包分块，这样即使中途有一个分块丢失或损坏了，只需要重新发送这一个分块，而不用重新发送整个数据包。在 TCP 协议中，我们把每个分块称为一个 TCP 段。 传输层的报文中会携带端口号，因此接收方可以识别出该报文是发送给哪个进程。 UDP UDP：简单到只负责发送数据包，不保证数据包是否能抵达对方，但它实时性相对更好，传输效率也高。 1.3 网络层 网络层主要负责将数据从源节点传输到目标节点（IP地址）。 IP 协议会将传输层的报文作为数据部分，再加上 IP 包头组装成 IP 报文，如果 IP 报文大小超过 MTU（以太网中一般为 1500 字节）就会再次进行分片，得到一个即将发送到网络的 IP 报文。 IPV4（32位）,前面是网络号，后面是主机号，通过子网掩码区分。 网络层核心功能 转发：根据数据包的目标IP地址查找其转发表（Forwarding Table），确定将数据包发送到哪个接口。 路由：路由是指确定数据包从源地址到目的地址的最佳路径的过程。用路由协议（如RIP、OSPF、BGP等）与其他路由器交换信息，以构建和维护路由表（Routing Table）。 1.4 网络接口层（传输层/物理层） 网络接口层主要为网络层提供「链路级别」传输的服务，负责在底层网络上发送原始数据包，工作在网卡这个层次，使用 MAC 地址来标识网络上的设备。 把IP报文加上MAC地址，封装成数据帧（Data frame）发送到网络上。 MAC 头部是以太网使用的头部，它包含了接收方和发送方的 MAC 地址等信息，我们可以通过 ARP 协议获取对方的 MAC 地址。 ARP协议 在发送数据包时，如果目标主机不是本地局域网，填入的MAC地址是路由器，也就是把数据包转发给路由器，路由器一直转发下一个路由器，直到转发到目标主机的路由器，发现 IP 地址是自己局域网内的主机，就会 arp 请求获取目标主机的 MAC 地址，从而转发到这个服务器主机。网络传输中，源IP地址和目标IP地址是不会变的，源 MAC 地址和目标 MAC 地址是会变化的。 1.5 总结 2. URL URL（Uniform Resource Locators），即统一资源定位器。 3. DNS DNS主要使用了UDP，有些情况也使用TCP。 如果Type=A，则Name是主机名信息，Value 是该主机名对应的 IP 地址。这样的 RR 记录了一条主机名到 IP 地址的映射。 如果Type=CNAME (Canonical Name Record,真实名称记录) ，则Value是别名为Name的主机对应的规范主机名。Value值才是规范主机名。CNAME 记录将一个主机名映射到另一个主机名。CNAME 记录用于为现有的 A 记录创建别名。下文有示例。 NAME TYPE VALUE -------------------------------------------------- bar.example.com. CNAME foo.example.com. foo.example.com. A 192.0.2.23 浏览器会先看自身有没有对这个域名的缓存，如果有，就直接返回，如果没有，就去问操作系统，操作系统也会去看自己的缓存，如果有，就直接返回，如果没有，再去 hosts 文件看，也没有，才会去问「本地 DNS 服务器」。 4. 键入网址到网页显示 1.在浏览器中输入指定网页的 URL。 2.浏览器通过 DNS 协议，获取域名对应的 IP 地址。 3.浏览器根据 IP 地址和端口号，向目标服务器发起一个 TCP 连接请求。 4.浏览器在 TCP 连接上，向服务器发送一个 HTTP 请求报文，请求获取网页的内容。 5.服务器收到 HTTP 请求报文后，处理请求，并返回 HTTP 响应报文给浏览器。 6.浏览器收到 HTTP 响应报文后，解析响应体中的 HTML 代码，渲染网页的结构和样式，同时根据 HTML 中的其他资源的 URL（如图片、CSS、JS 等），再次发起 HTTP 请求，获取这些资源的内容，直到网页完全加载显示。 7.浏览器在不需要和服务器通信时，可以主动关闭 TCP 连接，或者等待服务器的关闭请求。 5.OSI七层 OSI 模型将表示层和会话层独立出来，而 TCP/IP 模型将它们合并到应用层。 OSI 模型是理论模型，分层更细致，适合教学和标准化。 TCP/IP 模型是实际模型，分层更简洁，广泛用于互联网通信。 HTTP 1. 基本概念 HTTP 是超文本传输协议。 详细解释的话：协议、传输、超文本。 HTTP 是一个在计算机世界里专门在「两点」之间「传输」文字、图片、音频、视频等「超文本」数据的「约定和规范」。 常见状态码： 2xx 类状态码表示服务器成功处理了客户端的请求，也是我们最愿意看到的状态。 3xx 类状态码表示客户端请求的资源发生了变动，需要客户端用新的 URL 重新发送请求获取资源，也就是重定向。 4xx 类状态码表示客户端发送的报文有误 403 Forbidden表示服务器禁止访问资源，并不是客户端的请求出错。 404 Not Found表示请求的资源在服务器上不存在或未找到，所以无法提供给客户端。 常见字段： Host 字段：域名 Content-Length 字段：本次回应的数据长度。 Connection: Keep-Alive：可以使用同一个 TCP 连接来发送和接收多个 HTTP 请求/应答，避免了连接建立和释放的开销，这个方法称为 HTTP 长连接。 Content-Type 字段：本次数据是什么格式。 Content-Encoding 字段：使用了什么压缩格式 技术： 长连接：使用同一个 TCP 连接来发送和接收多个 HTTP 请求/应答，避免了连接建立和释放的开销，这个方法称为 HTTP 长连接。 流水线：客户端可以先一次性发送多个请求，而在发送过程中不需先等待服务器的回应 2. Get和Post区别 ​ GET 方法就是安全且幂等的，因为它是「只读」操作，无论操作多少次，服务器上的数据都是安全的，且每次的结果都是相同的。所以，可以对 GET 请求的数据做缓存，这个缓存可以做到浏览器本身上（彻底避免浏览器发请求），也可以做到代理上（如nginx），而且在浏览器中 GET 请求可以保存为书签。 ​ POST因为是「新增或提交数据」的操作，会修改服务器上的资源，所以是不安全的，且多次提交数据就会创建多个资源，所以不是幂等的。所以，浏览器一般不会缓存 POST 请求，也不能把 POST 请求保存为书签。 3. HTTP 与 HTTPS 3.1 二者区别 HTTP 是超文本传输协议，信息是明文传输，存在安全风险的问题。HTTPS 则解决 HTTP 不安全的缺陷，在 TCP 和 HTTP 网络层之间加入了 SSL/TLS 安全协议，使得报文能够加密传输。 HTTP 连接建立相对简单， TCP 三次握手之后便可进行 HTTP 的报文传输。而 HTTPS 在 TCP 三次握手之后，还需进行 SSL/TLS 的握手过程，才可进入加密报文传输。 两者的默认端口不一样，HTTP 默认端口号是 80，HTTPS 默认端口号是 443。 HTTPS 协议需要向 CA（证书权威机构）申请数字证书，来保证服务器的身份是可信的。 3.2 HTTPS 解决了 HTTP 的哪些问题？ 窃听风险，比如通信链路上可以获取通信内容，用户号容易没。 篡改风险，比如强制植入垃圾广告，视觉污染，用户眼容易瞎。 冒充风险，比如冒充淘宝网站，用户钱容易没。 信息加密：交互信息无法被窃取，但你的号会因为「自身忘记」账号而没。 校验机制：无法篡改通信内容，篡改了就不能正常显示，但百度「竞价排名」依然可以搜索垃圾广告。 身份证书：证明淘宝是真的淘宝网，但你的钱还是会因为「剁手」而没。 3.3 如何解决？ 摘要算法的方式来实现完整性，它能够为数据生成独一无二的「指纹」，指纹用于校验数据的完整性，解决了篡改的风险。 将服务器公钥放入到数字证书中，解决了冒充的风险。 1.混合加密：实现信息的机密性，解决了窃听的风险 HTTPS 采用的是对称加密和非对称加密结合的「混合加密」方式： 在通信建立前采用非对称加密的方式得到「会话秘钥」，后续就不再使用非对称加密。 在通信过程中全部使用对称加密的「会话秘钥」的方式加密明文数据。 采用「混合加密」的方式的原因： 对称加密只使用一个密钥，运算速度快，密钥必须保密，无法做到安全的密钥交换。 非对称加密使用两个密钥：公钥和私钥，公钥可以任意分发而私钥保密，解决了密钥交换问题但速度慢。 2.摘要算法+数字签名：无法篡改通信内容 用摘要算法（哈希函数）来计算出内容的哈希值，也就是内容的「指纹」，这个哈希值是唯一的，且无法通过哈希值推导出内容。 3.身份证书：解决了冒充的风险 总结： TCP TCP和UDP都使用IP协议。 1. TCP基本概念 TCP 是面向连接的、可靠的、基于字节流的传输层通信协议。 TCP工作在传输层，确保网络包是无损坏、无间隔、非冗余和按序的。 TCP有一个首部长度，TCP的数据长度=IP总长度-IP首部长度-TCP首部长度，其中 IP 总长度 和 IP 首部长度，在 IP 首部格式是已知的。TCP 首部长度，则是在 TCP 首部格式已知的，所以就可以求得 TCP 数据的长度。 序列号：在建立连接时由计算机生成的随机数作为其初始值，通过 SYN 包传给接收端主机，每发送一次数据，就「累加」一次该「数据字节数」的大小。用来解决网络包乱序问题。 确认应答号：指下一次「期望」收到的数据的序列号，发送端收到这个确认应答以后可以认为在这个序号以前的数据都已经被正常接收。用来解决丢包的问题。 公式一：序列号 = 上一次发送的序列号 + len（数据长度）。特殊情况，如果上一次发送的报文是 SYN 报文或者 FIN 报文，则改为 上一次发送的序列号 + 1。 公式二：确认号 = 上一次收到的报文中的序列号 + len（数据长度）。特殊情况，如果收到的是 SYN 报文或者 FIN 报文，则改为上一次收到的报文中的序列号 + 1。 标志位：ACK.SYN.FIN.RST(RST标志位用于强制关闭一个TCP连接。它的主要作用是通知对方当前的连接不再有效) TCP四元组： 2. UDP基本概念 UDP 不提供复杂的控制机制，利用 IP 提供面向「无连接」的通信服务。 UDP 协议真的非常简，头部固定只有 8 个字节（64 位），UDP 的头部格式如下： UDP数据长度=包长度-8。 3. TCP和UDP 3.1 二者区别 1. 连接 TCP 是面向连接的传输层协议，传输数据前先要建立连接。 UDP 是不需要连接，即刻传输数据。 2. 服务对象 TCP 是一对一的两点服务，即一条连接只有两个端点。 UDP 支持一对一、一对多、多对多的交互通信 3. 可靠性 TCP 是可靠交付数据的，数据可以无差错、不丢失、不重复、按序到达。 UDP 是尽最大努力交付，不保证可靠交付数据。 4. 拥塞控制、流量控制 TCP 有拥塞控制和流量控制机制，保证数据传输的安全性。 UDP 则没有，即使网络非常拥堵了，也不会影响 UDP 的发送速率。 5. 首部开销 TCP 首部长度较长，会有一定的开销，首部在没有使用「选项」字段时是 20 个字节，如果使用了「选项」字段则会变长的。 UDP 首部只有 8 个字节，并且是固定不变的，开销较小。 6. 传输方式 TCP 是流式传输，没有边界，但保证顺序和可靠。 UDP 是一个包一个包的发送，是有边界的，但可能会丢包和乱序。 7. 分片不同 TCP 的数据大小如果大于 MSS 大小，则会在传输层进行分片，目标主机收到后，也同样在传输层组装 TCP 数据包，如果中途丢失了一个分片，只需要传输丢失的这个分片。 UDP 的数据大小如果大于 MTU 大小，则会在 IP 层进行分片，目标主机收到后，在 IP 层组装完数据，接着再传给传输层。 3.2 TCP和UDP的应用场景 由于 TCP 是面向连接，能保证数据的可靠性交付，因此经常用于：FTP 文件传输；HTTP / HTTPS； 由于 UDP 面向无连接，它可以随时发送数据，再加上 UDP 本身的处理既简单又高效，因此经常用于：视频、音频等多媒体通信；包总量较少的通信，如 DNS 、SNMP 等。 3.3 可以使用同一个端口吗 可以，传输层有两个传输协议分别是 TCP 和 UDP，在内核中是两个完全独立的软件模块。 当主机收到数据包后，可以在 IP 包头的「协议号」字段知道该数据包是 TCP/UDP，所以可以根据这个信息确定送给哪个模块（TCP/UDP）处理，送给 TCP/UDP 模块的报文根据「端口号」确定送给哪个应用程序处理。 因此，TCP/UDP 各自的端口号也相互独立，如 TCP 有一个 80 号端口，UDP 也可以有一个 80 号端口，二者并不冲突。 4. TCP连接建立 4.1 易错点 ACK 报文是不会有重传的，当 ACK 丢失了，就由对方重传对应的报文。 4.2 建立流程 SYN和ACK是标志位，Seq Num是序列号，ACK num是确认应答号。 第三次握手是可以携带数据的，前两次握手是不可以携带数据的 4.3 为什么是三次握手？不是两次、四次？ 三次握手最主要的目的就是双方确认自己与对方的发送与接收是正常的。 第一次握手：Client 什么都不能确认；Server 确认了对方发送正常，自己接收正常 第二次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：对方发送正常，自己接收正常 第三次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己发送、接收正常，对方发送、接收正常 三次握手就能确认双方收发功能都正常，缺一不可。 除此之外一个别的观点：详细看小林coding：https://xiaolincoding.com/network/3_tcp/tcp_interview.html#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%98%AF%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B-%E4%B8%8D%E6%98%AF%E4%B8%A4%E6%AC%A1%E3%80%81%E5%9B%9B%E6%AC%A1 「两次握手」：无法防止历史连接的建立，会造成双方资源的浪费，也无法可靠的同步双方序列号； 「四次握手」：三次握手就已经理论上最少可靠连接建立，所以不需要使用更多的通信次数。 4.4 为什么每次建立 TCP 连接时，初始化的序列号都要求不一样呢？ 很大程度上能够避免历史报文被下一个相同四元组的连接接收 4.5 既然 IP 层会分片，为什么 TCP 层还需要 MSS 呢？ 因为 IP 层本身没有超时重传机制，它由传输层的 TCP 来负责超时和重传。 当某一个 IP 分片丢失后，接收方的 IP 层就无法组装成一个完整的 TCP 报文（头部 + 数据），也就无法将数据报文送到 TCP 层，所以接收方不会响应 ACK 给发送方，因为发送方迟迟收不到 ACK 确认报文，所以会触发超时重传，就会重发「整个 TCP 报文（头部 + 数据）」。 因此，可以得知由 IP 层进行分片传输，是非常没有效率的。 经过 TCP 层分片后，如果一个 TCP 分片丢失后，进行重发时也是以 MSS 为单位，而不用重传所有的分片，大大增加了重传的效率。 4.6 三次握手丢失会发生什么 超时重传（尝试几次，不行就断开连接），看小林coding 4.7 什么是 SYN 攻击？如何避免 SYN 攻击？ SYN 攻击方式最直接的表现就会把 TCP 半连接队列打满，这样当 TCP 半连接队列满了，后续再在收到 SYN 报文就会丢弃，导致客户端无法和服务端建立连接。 增大半连接队列，减少 SYN+ACK 重传次数。 5. TCP连接断开 5.1 建立流程 客户端打算关闭连接，此时会发送一个 TCP 首部 FIN 标志位被置为 1 的报文，也即 FIN 报文，之后客户端进入 FIN_WAIT_1 状态。 服务端收到该报文后，就向客户端发送 ACK 应答报文，接着服务端进入 CLOSE_WAIT 状态。 客户端收到服务端的 ACK 应答报文后，之后进入 FIN_WAIT_2 状态。 等待服务端处理完数据后，也向客户端发送 FIN 报文，之后服务端进入 LAST_ACK 状态。 客户端收到服务端的 FIN 报文后，回一个 ACK 应答报文，之后进入 TIME_WAIT 状态 服务端收到了 ACK 应答报文后，就进入了 CLOSE 状态，至此服务端已经完成连接的关闭。 客户端在经过 2MSL 一段时间后，自动进入 CLOSE 状态，至此客户端也完成连接的关闭。 你可以看到，每个方向都需要一个 FIN 和一个 ACK，因此通常被称为四次挥手。 这里一点需要注意是：主动关闭连接的，才有 TIME_WAIT 状态。 5.2 为什么是四次挥手？ 关闭连接时，客户端向服务端发送 FIN 时，仅仅表示客户端不再发送数据了但是还能接收数据。 服务端收到客户端的 FIN 报文时，先回一个 ACK 应答报文，而服务端可能还有数据需要处理和发送，等服务端不再发送数据时，才发送 FIN 报文给客户端来表示同意现在关闭连接。 第一次挥手：A 说“我没啥要说的了” 第二次挥手：B 回答“我知道了”，但是 B 可能还会有要说的话，A 不能要求 B 跟着自己的节奏结束通话 第三次挥手：于是 B 可能又巴拉巴拉说了一通，最后 B 说“我说完了” 第四次挥手：A 回答“知道了”，这样通话才算结束。 为什么不能把服务端发送的 ACK 和 FIN 合并起来，变成三次挥手？ 因为服务端收到客户端断开连接的请求时，可能还有一些数据没有发完，这时先回复 ACK，表示接收到了断开连接的请求。等到数据发完之后再发 FIN，断开服务端到客户端的数据传送。 5.3 四次挥手丢失会发生什么？ 看小林coding 5.4 为什么第四次挥手客户端需要等待 2*MSL时间后才进入 CLOSED 状态? 第四次挥手时，客户端发送给服务端的 ACK 有可能丢失，如果服务端因为某些原因而没有收到 ACK 的话，服务端就会重发 FIN，如果客户端在 2*MSL 的时间内收到了 FIN，就会重新发送 ACK 并再次等待 2MSL，防止 Server 没有收到 ACK 而不断重发 FIN。 5.5 为什么 TIME_WAIT 等待的时间是 2MSL？ 可以看到 2MSL时长 这其实是相当于至少允许报文丢失一次。比如，若 ACK 在一个 MSL 内丢失，这样被动方重发的 FIN 会在第 2 个 MSL 内到达，TIME_WAIT 状态的连接可以应对。 5.6 如果已经建立了连接，但是客户端突然出现故障了怎么办？ 通过心跳机制（保活机制）、超时设置等方法来管理的。服务器可能会定期发送数据包（如心跳包）来检查客户端是否仍然可用。 如果服务器在一定时间内没有收到来自客户端的数据包，它可能会认为客户端出现故障。服务器会根据其配置的超时设置来主动关闭连接。 如果没有开启keep-alive（不是http中的），呢么TCP连接永远不会断。 6. TCP是如何保障可靠的 6.1 概述 TCP 是通过序列号、确认应答、重发控制、连接管理以及窗口控制等机制实现可靠性传输的。 6.2 重传机制 6.2.1 超时重传 重传机制的其中一个方式，就是在发送数据时，设定一个定时器，当超过指定的时间后，没有收到对方的 ACK 确认应答报文，就会重发该数据，也就是我们常说的超时重传。 超时重传时间 RTO 的值应该略大于报文往返 RTT 的值。 实际上RTO是随着网络情况动态变化的。 每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送。 6.2.2 快速重传 当收到三个相同的 ACK 报文时，会在定时器过期之前，重传丢失的报文段。 注意：下图使用了发送窗口，发送方一下子发送了连续五个（而不是发一个等一个ack） 面临的问题，重传什么？ 如果只选择重传 Seq2 一个报文，那么重传的效率很低。因为对于丢失的 Seq3 报文，还得在后续收到三个重复的 ACK3 才能触发重传。 如果选择重传 Seq2 之后已发送的所有报文，虽然能同时重传已丢失的 Seq2 和 Seq3 报文，但是 Seq4、Seq5、Seq6 的报文是已经被接收过了，对于重传 Seq4 ～Seq6 折部分数据相当于做了一次无用功，浪费资源。 6.2.3 SACK 方法 选择性确认：这种方式需要在 TCP 头部「选项」字段里加一个 SACK 的东西，它可以将已收到的数据的信息发送给「发送方」，这样发送方就可以知道哪些数据收到了，哪些数据没收到，知道了这些信息，就可以只重传丢失的数据。 6.2.4 Duplicate SACK 6.3 滑动窗口 不使用滑动窗口，效率低，发一下等一下ack， 6.3.1 累计确认 通常窗口的大小是由接收方的窗口大小来决定的，接收窗口的大小是约等于发送窗口的大小的，但不完全相等。 发送窗口的值是swnd = min(cwnd, rwnd)，也就是拥塞窗口和接收窗口中的最小值。 实际上发送和接收窗口都是放在操作系统内存缓冲区中的，而操作系统的缓冲区，会被操作系统调整。 6.3.2 发送方窗口 6.3.3 接收方窗口 6.4 流量控制 发送方不能无脑的发数据给接收方，要考虑接收方处理能力。 TCP 提供一种机制可以让「发送方」根据「接收方」的实际接收能力控制发送的数据量，这就是所谓的流量控制。 接收方 向 发送方 通告窗口大小时，是通过 ACK 报文来通告的。 窗口探测的次数一般为 3 次，每次大约 30-60 秒（不同的实现可能会不一样）。如果 3 次过后接收窗口还是 0 的话，有的 TCP 实现就会发 RST 报文来中断连接。 6.5 拥塞控制 前面的流量控制是避免「发送方」的数据填满「接收方」的缓存，但是并不知道网络的中发生了什么。 一般来说，计算机网络都处在一个共享的环境。因此也有可能会因为其他主机之间的通信使得网络拥堵。TCP是无私的协议。 目的是：避免「发送方」的数据填满整个网络 那么怎么知道当前网络是否出现了拥塞呢？ 其实只要「发送方」没有在规定时间内接收到 ACK 应答报文，也就是发生了超时重传，就会认为网络出现了拥塞。 6.5.1 慢启动 当发送方每收到一个 ACK，拥塞窗口 cwnd 的大小就会加 1。 连接建立完成后，一开始初始化 cwnd = 1，表示可以传一个 MSS 大小的数据。 当收到一个 ACK 确认应答后，cwnd 增加 1，于是一次能够发送 2 个 当收到 2 个的 ACK 确认应答后， cwnd 增加 2，于是就可以比之前多发2 个，所以这一次能够发送 4 个 当这 4 个的 ACK 确认到来的时候，每个确认 cwnd 增加 1， 4 个确认 cwnd 增加 4，于是就可以比之前多发 4 个，所以这一次能够发送 8 个。 有一个叫慢启动门限 ssthresh （slow start threshold）状态变量。 当 cwnd ssthresh 时，使用慢启动算法。 当 cwnd >= ssthresh 时，就会使用「拥塞避免算法」 6.5.2 拥塞避免算法 拥塞窗口 cwnd 「超过」慢启动门限 ssthresh 就会进入拥塞避免算法。 那么进入拥塞避免算法后，它的规则是：每当收到一个 ACK 时，cwnd 增加 1/cwnd。 当 8 个 ACK 应答确认到来时，每个确认增加 1/8，8 个 ACK 确认 cwnd 一共增加 1，于是这一次能够发送 9 个 MSS 大小的数据，变成了线性增长。 6.5.3 超时重传 出现了超时重传，说明网络拥堵，减小拥塞窗口可以降低发送速率，减轻网络负担，帮助网络恢复到更稳定的状态。 ssthresh 设为 cwnd/2， cwnd 重置为 1 （是恢复为 cwnd 初始化值，我这里假定 cwnd 初始化值 1） 6.5.4 快速恢复 快速重传: 当发送端连续收到3个重复的确认报文端段的时候，tcp就认为拥塞发生了。然后会立即重传丢失的报文段。 快速恢复机制一般和快速重传机制同时使用。快速恢复机制如下： 快速恢复算法是认为，你还能收到 3 个重复 ACK 说明网络也不那么糟糕，所以没有必要像 RTO 超时那么强烈。 ssthresh = cwnd/2 cwnd = ssthresh + 3, 加 3 代表快速重传时已经确认接收到了 3 个重复的数据包； 如果再收到重复的 ACK，那么 cwnd 增加 1；说明网络的状况并没有恶化，允许发送方在拥塞恢复期间更有效地利用网络带宽，而不必完全回到慢开始状态，提高了效率。 如果收到新数据的 ACK 后，把 cwnd 设置为第一步中的 ssthresh 的值，原因是该 ACK 确认了新的数据，说明从 duplicated ACK 时的数据都已收到，该恢复过程已经结束，可以回到恢复之前的状态了，也即再次进入拥塞避免状态； 注意：在旧的tcp拥塞控制算法中，快速重传之后会进入慢启动阶段，而新的tcp拥塞控制算法在快速重传之后进入快速恢复阶段。 7.杂项 7.1如何理解是 TCP 面向字节流协议？ UDP 是面向报文的协议： 当用户消息通过 UDP 协议传输时，操作系统不会对消息进行拆分，在组装好 UDP 头部后就交给网络层来处理，所以发出去的 UDP 报文中的数据部分就是完整的用户消息，也就是每个 UDP 报文就是一个用户消息的边界，这样接收方在接收到 UDP 报文后，读一个 UDP 报文就能读取到完整的用户消息。 TCP 是面向字节流的协议： 发送方准备发送 「Hi.」和「I am Xiaolin」这两个消息。 我们不知道 「Hi.」和 「I am Xiaolin」 这两个用户消息是如何进行 TCP 分组传输的。 因此，我们不能认为一个用户消息对应一个 TCP 报文，正因为这样，所以 TCP 是面向字节流的协议。 当两个消息的某个部分内容被分到同一个 TCP 报文时，就是我们常说的 TCP 粘包问题，这时接收方不知道消息的边界的话，是无法读出有效的消息。 要解决这个问题，要交给应用程序。 如何解决粘包？ 特殊字符作为边界：我们可以在两个用户消息之间插入一个特殊的字符串，这样接收方在接收数据时，读到了这个特殊字符，就把认为已经读完一个完整的消息。HTTP 通过设置回车符、换行符作为 HTTP 报文协议的边界。 7.2 已建立连接的TCP，收到SYN会发生什么？ 7.3 四次挥手中收到乱序的 FIN 包会如何处理？ 若服务端在二三次挥手间或四次挥手前发送的数据包因网络阻塞，导致第三次挥手的 FIN 包比数据包先到主动关闭方，主动关闭方收到 FIN 是否会进入 timewait 状态以及延迟数据包能否正常接收处理? 在 FIN_WAIT_2 状态下收到乱序的 FIN 报文，会被加入 “乱序队列” 。等收到之前被网络延迟的数据包时，若乱序队列中有与当前报文序列号连续的报文且该报文有 FIN 标志，这时才会进入 TIME_WAIT 状态。 https://xiaolincoding.com/network/3_tcp/out_of_order_fin.html#tcp-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90 7.4 在 TIME_WAIT 状态的 TCP 连接，收到 SYN 后会发生什么？ 如果客户端的 SYN 的「序列号」比服务端「期望下一个收到的序列号」要大，并且SYN 的「时间戳」比服务端「最后收到的报文的时间戳」要大。那么就会重用该四元组连接，跳过 2MSL 而转变为 SYN_RECV 状态，接着就能进行建立连接过程。 如果客户端的 SYN 的「序列号」比服务端「期望下一个收到的序列号」要小，或者SYN 的「时间戳」比服务端「最后收到的报文的时间戳」要小。那么就会再回复一个第四次挥手的 ACK 报文，客户端收到后，发现并不是自己期望收到确认号，就回 RST 报文给服务端。 7.5 TCP 连接，一端断电和进程崩溃有什么区别？ 7.6 拔掉网线后， 原本的 TCP 连接还存在吗？ 等价于=客户端宕机 客户端拔掉网线后，并不会直接影响 TCP 连接状态。所以，拔掉网线后，TCP 连接是否还会存在，关键要看拔掉网线之后，有没有进行数据传输。 https://xiaolincoding.com/network/3_tcp/tcp_unplug_the_network_cable.html#%E6%80%BB%E7%BB%93 7.7 TCP Keepalive 和 HTTP Keep-Alive 是一个东西吗？ 这两个完全是两样不同东西 HTTP 的 Keep-Alive 也叫 HTTP 长连接，该功能是由「应用程序」实现的，可以使得用同一个 TCP 连接来发送和接收多个 HTTP 请求/应答，减少了 HTTP 短连接带来的多次 TCP 连接建立和释放的开销。 TCP 的 Keepalive 也叫 TCP 保活机制，该功能是由「内核」实现的，当客户端和服务端长达一定时间没有进行数据交互时，内核为了确保该连接是否还有效，就会发送探测报文，来检测对方是否还在线，然后来决定是否要关闭该连接。 7.8 TCP 四次挥手，可以变成三次吗？ TCP 四次挥手中，能不能把第二次的 ACK 报文， 放到第三次 FIN 报文一起发送？ 不太好，有点暴力，因为服务端收到客户端断开连接的请求时，可能还有一些数据没有发完，这时先回复 ACK，表示接收到了断开连接的请求。等到数据发完之后再发 FIN，断开服务端到客户端的数据传送。 IP 0. 基本知识 IP 包中有一个字段叫做 TTL （Time To Live，生存周期），它的值随着每经过一次路由器就会减 1，直到减到 0 时该 IP 包会被丢弃。 此时，路由器将会发送一个 ICMP 超时消息给发送端主机，并通知该包已被丢弃。 1. IP和MAC关系 源IP地址和目标IP地址在传输过程中是不会变化的（前提：没有使用 NAT 网络），只有源 MAC 地址和目标 MAC 一直在变化。 2. IP 地址的分类 目前不用了。 主机号全为 1 指定某个网络下的所有主机，用于广播 主机号全为 0 指定某个网络 广播分为本地广播（只在局域网内，路由器不转发），和直接广播（有安全性问题） IP分类的优点：简单明了、选路（基于网络地址）简单 缺点：不能很好的与现实网络匹配。 C 类地址能包含的最大主机数量实在太少了，只有 254 个，估计一个网吧都不够用。 而 B 类地址能包含的最大主机数量又太多了，6 万多台机器放在一个网络下面，一般的企业基本达不到这个规模，闲着的地址就是浪费。 3. 无分类地址 CIDR 3.1 基本内容 表示形式 a.b.c.d/x，其中 /x 表示前 x 位属于网络号， x 的范围是 0 ~ 32，这就使得 IP 地址更加具有灵活性。 比如 10.100.122.2/24，这种地址表示形式就是 CIDR，/24 表示前 24 位是网络号，剩余的 8 位是主机号。 注意主机号，全1是广播地址。全0也有用处。 也可以使用子网掩码（掩盖掉主机号）。 3.2 为什么要分离网络号和主机号？ 因为两台计算机要通讯，首先要判断是否处于同一个广播域内，即网络地址是否相同。如果网络地址相同，表明接受方在本网络上，那么可以把数据包直接发送到目标主机。 路由器寻址工作中，也就是通过这样的方式来找到对应的网络号的，进而把数据包转发给对应的网络内。 3.3 怎么进行子网划分？ 子网掩码还有一个作用，那就是划分子网。 子网划分实际上是将主机地址分为两个部分：子网网络地址和子网主机地址。形式如下 ​ 这样我们就可以根据ip地址和子网掩码，区分出网络地址、子网地址、主机地址 4. 公有 IP 地址与私有 IP 地址 平时我们办公室、家里、学校用的 IP 地址，一般都是私有 IP 地址。 因为这些地址允许组织内部的 IT 人员自己管理、自己分配，而且可以重复。因此，你学校的某个私有 IP 地址和我学校的可以是一样的。 5. IP 地址与路由控制 计算机使用一个特殊的 IP 地址 127.0.0.1 作为环回地址。与该地址具有相同意义的是一个叫做 localhost 的主机名。使用这个 IP 或主机名时，数据包不会流向网络。 6. IP 分片与重组 在分片传输中，一旦某个分片丢失，则会造成整个 IP 数据报作废，所以 TCP 引入了 MSS 也就是在 TCP 层进行分片不由 IP 层分片。 对于 UDP 我们尽量不要发送一个大于 MTU 的数据报文。 7. IPv6 基本认识 IPv4 的地址是 32 位的，大约可以提供 42 亿个地址，但是早在 2011 年 IPv4 地址就已经被分配完了。 但是 IPv6 的地址是 128 位的，这可分配的地址数量是大的惊人，说个段子 IPv6 可以保证地球上的每粒沙子都能被分配到一个 IP 地址。 IPv6 可自动配置，即使没有 DHCP 服务器也可以实现自动分配IP地址，真是便捷到即插即用啊。 IPv6 包头包首部长度采用固定的值 40 字节，去掉了包头校验和，简化了首部结构，减轻了路由器负荷，大大提高了传输的性能。 IPv6 有应对伪造 IP 地址的网络安全功能以及防止线路窃听的功能，大大提升了安全性。 IPv4 地址长度共 32 位，是以每 8 位作为一组，并用点分十进制的表示方式。 IPv6 地址长度是 128 位，是以每 16 位作为一组，每组用冒号 「:」 隔开。 8. IP 协议相关技术 8.1 DNS 前面写了 8.2 ARP 操作系统通常会把第一次通过 ARP 获取的 MAC 地址缓存起来，以便下次直接从缓存中找到对应 IP 地址的 MAC 地址。 同一个子网内：主机A首先比较目的IP地址与自己的IP地址是否在同一子网中，如果在同一子网，则向本网发送ARP广播，获得目标IP所对应的MAC地址； 不同子网： 检查路由表：主机A检查其路由表，发现目标IP地址不在同一子网内，因此需要通过默认网关进行通信。 ARP请求：主机A会查看是否已经缓存了网关的MAC地址。如果没有，它会发送一个ARP请求广播，询问“谁拥有网关的IP地址Y.Y.Y.Y？”。 网关响应：局域网内的所有主机都会接收到这个ARP请求，只有网关会回应其MAC地址。 发送数据包到网关：一旦主机A获得了网关的MAC地址，它就可以将数据包发送到网关。 数据包转发：网关接收到数据包后，会根据其路由表将数据包转发到目标子网 目标子网的ARP解析：在目标子网内，网关会使用ARP请求来找到主机C的MAC地址，然后将数据包转发给主机 8.3 DHCP DHCP 在生活中我们是很常见的了，我们的电脑通常都是通过 DHCP 动态获取 IP 地址，大大省去了配 IP 信息繁琐的过程。 客户端首先发起 DHCP 发现报文（DHCP DISCOVER） 的 IP 数据报，由于客户端没有 IP 地址，也不知道 DHCP 服务器的地址，所以使用的是 UDP 广播通信，其使用的广播目的地址是 255.255.255.255（端口 67） 并且使用 0.0.0.0（端口 68） 作为源 IP 地址。DHCP 客户端将该 IP 数据报传递给链路层，链路层然后将帧广播到所有的网络中设备。 DHCP 服务器收到 DHCP 发现报文时，用 DHCP 提供报文（DHCP OFFER） 向客户端做出响应。该报文仍然使用 IP 广播地址 255.255.255.255，该报文信息携带服务器提供可租约的 IP 地址、子网掩码、默认网关、DNS 服务器以及 IP 地址租用期。 客户端收到一个或多个服务器的 DHCP 提供报文后，从中选择一个服务器，并向选中的服务器发送 DHCP 请求报文（DHCP REQUEST进行响应，回显配置的参数。 最后，服务端用 DHCP ACK 报文对 DHCP 请求报文进行响应，应答所要求的参数。 一旦客户端收到 DHCP ACK 后，交互便完成了，并且客户端能够在租用期内使用 DHCP 服务器分配的 IP 地址。 如果租约的 DHCP IP 地址快期后，客户端会向服务器发送 DHCP 请求报文： 服务器如果同意继续租用，则用 DHCP ACK 报文进行应答，客户端就会延长租期。 服务器如果不同意继续租用，则用 DHCP NACK 报文，客户端就要停止使用租约的 IP 地址。 可以发现，DHCP 交互中，全程都是使用 UDP 广播通信。 咦，用的是广播，那如果 DHCP 服务器和客户端不是在同一个局域网内，路由器又不会转发广播包，那不是每个网络都要配一个 DHCP 服务器？ 8.4 NAT NAT 就是同个公司、家庭、教室内的主机对外部通信时，把私有 IP 地址转换成公有 IP 地址。 特殊的：可以解决IPv4短缺，但NAT在某种程度上确实影响了TCP的端到端原则，并且需要转换ip增加了开销。 8.5 ICMP Internet Control Message Protocol，也就是互联网控制报文协议。 当数据包在传输过程中出现问题时，ICMP可以向源主机发送错误消息。例如，如果目标主机不可达，路由器可以发送一个“目的不可达”消息给源主机 ICMP消息被封装在IP数据包中。工作在网络层 9. PING工作原理 ping 是基于 ICMP 协议工作的 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-02-26 19:31:48 "},"Chapter3/设计模式.html":{"url":"Chapter3/设计模式.html","title":"设计模式","keywords":"","body":"基础知识 1. 什么是设计模式？ 是指在软件开发过程中，针对反复出现的问题所总结归纳出的通用解决方案。 使⽤设计模式是为了重⽤代码、让代码更容易被他⼈理解、保证代码可靠性。 2. 设计模式的分类了解吗？ 创建型： 在创建对象的同时隐藏创建逻辑，不使⽤ new 直接实例化对象，程序在判断需要创建哪些对象时更灵活。 包括⼯⼚/抽象⼯⼚/单例/建造者/原型模式。 结构型： 通过类和接⼝间的继承和引⽤实现创建复杂结构的对象。 包括适配器/桥接模式/过滤器/组合/装饰器/外观/享元/代理模式。 ⾏为型： 通过类之间不同通信⽅式实现不同⾏为。 包括责任链/命名/解释器/迭代器/中介者/备忘录/观察者/状态/策略/模板/访问者模式。 3. 你知道哪些设计模式？ 创建型模式 1. 单例模式 ⼀个单例类在任何情况下都只存在⼀个实例。 构造⽅法必须是私有的。 由⾃⼰创建⼀个静态变量存储实例。 对外提供⼀个静态公有⽅法获取实例。 例子：Spring 框架里，单例 Bean 就遵循了单例模式的核心思想，在整个 Spring 应用上下文中，一个单例 Bean 只会有一个实例存在。 //线程安全的单例模式 public class Singleton { // 使用 volatile 关键字确保 instance 的可见性 private static volatile Singleton instance; // 私有构造函数，防止外部实例化 private Singleton() { // 初始化代码 } // 获取单例实例的静态方法 public static Singleton getInstance() { // 第一次检查，避免不必要的同步 if (instance == null) { // 加锁，确保线程安全 synchronized (Singleton.class) { // 第二次检查，防止多个线程同时通过第一次检查 if (instance == null) { instance = new Singleton(); } } } return instance; } } 2. 简单工厂模式 由⼀个⼯⼚对象来创建实例，客户端不需要关注创建逻辑，只需提供传⼊⼯⼚的参数。 缺点：适⽤于⼯⼚类负责创建对象较少的情况，缺点是如果要增加新产品，就需要修改⼯⼚类的判断逻辑，违背开闭原则，且产品多的话会使⼯⼚类⽐较复杂。 例子： Calendar 抽象类的 getInstance ⽅法，调⽤ createCalendar ⽅法根据不同的地区参数创建不同的⽇历对象。 Spring 中的 BeanFactory 使⽤简单⼯⼚模式，根据传⼊⼀个唯⼀的标识来获得 Bean 对象。 3. ⼯⼚⽅法模式 也就是定义⼀个抽象⼯⼚，其定义了产品的⽣产接⼝，但不负责具体的产品，将⽣产任务交给不同的具体⼯⼚。 这样不⽤通过指定类型来创建对象了，而是直接使用具体工厂。 注意：工厂方法模式主要关注单个产品的创建，每个具体工厂类只负责创建一种具体产品。 4. 抽象⼯⼚模式 简单⼯⼚模式和⼯⼚⽅法模式不管⼯⼚怎么拆分抽象，都只是针对⼀类产品，如果要⽣成另⼀种产品，就⽐较难办了！ 工厂方法模式主要关注单个产品的创建，每个具体工厂类只负责创建一种具体产品。 而抽象工厂模式关注的是创建一系列相关的产品，一个具体工厂类可以创建多种不同类型的产品。 5. 建造者模式 结构性模式 1. 适配器模式 所谓适配器模式就是将⼀个类的接⼝，转换成客户期望的另⼀个接⼝。它可以让原本两个不兼容的接⼝能够⽆缝完成对接。 作为中间件的适配器将⽬标类和适配者解耦，增加了类的透明性和可复⽤性。 应用：旧的库存管理系统提供了一个以 XML 格式返回库存信息的接口，而新的销售系统需要的是 JSON 格式的数据。为了让新系统能够使用旧系统的库存信息，可以创建一个适配器类，将旧系统返回的 XML 数据转换为 JSON 数据。 2. 代理模式 使⽤者通过代理间接的访问服务提供者，便于后者的封装和控制，主要⽬的是解耦合服务提供者和使⽤者。 例子：SpringAop的实现。 3. 装饰器模式 装饰器模式主要对现有的类对象进⾏包裹和封装，以期望在不改变类对象及其类定义的情况下，为对象添加额外功能。 如果你希望在⽆需修改代码的情况下即可使⽤对象， 且希望在运⾏时为对象新增额外的⾏为， 可以使⽤装饰模式。 在java的IO流中较为常用。 InputStream fileInputStream = new FileInputStream(\"test.txt\"); // 使用具体装饰器：BufferedInputStream 进行装饰 InputStream bufferedInputStream = new BufferedInputStream(fileInputStream); 行为型模式 1. 策略模式 其⽤意是针对⼀组算法，将每⼀个算法封装到具有共同接⼝的独⽴的类中，从⽽使得它们可以相互替换。 应用：线程次的拒绝策略，需要传入一个策略。 好处：可以灵活切换。 2. 观察者模式 也称为发布订阅模式。 观察者模式主要⽤于处理对象间的⼀对多的关系，当⼀个对象状态发⽣变化时，所有该对象的关注者均能收到状态变化通知，以进⾏相应的处理。 核心：添加观察者，通知观察者。 应用： Spring 的事件驱动模型：Spring 框架提供了一套事件驱动模型，本质上是基于观察者模式实现的。主要有三个核心元素：ApplicationEvent（事件）、ApplicationListener（事件监听器，即观察者）和ApplicationEventPublisher（事件发布者，即被观察者）。 例如，在一个 Web 应用中，当用户成功登录时，可以发布一个自定义的登录成功事件，然后有相关的监听器来处理这个事件，比如记录登录日志、更新用户在线状态等。 3. 责任链模型 ⼀个请求沿着⼀条“链”传递，直到该“链”上的某个处理者处理它为⽌。 请求者不关心谁去处理，处理者不需要知道请求的全貌。 例子： 请假审批：假设员工请假天数不同，审批人不同。如果员工请假天数小于等于 3 天，由直属主管审批；如果请假天数大于 3 天且小于等于 7 天，由部门经理审批；如果请假天数大于 7 天，由总经理审批。这里可以将直属主管、部门经理、总经理看作责任链上的节点。 web过滤器链： Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-02-17 19:29:40 "},"Chapter3/非常规代码题.html":{"url":"Chapter3/非常规代码题.html","title":"非常规代码题","keywords":"","body":"多线程打印ABC 三个线程分别打印 A，B，C，要求这三个线程一起运行，打印 n 次，输出形如“ABCABCABC....”的字符串。 在if (idx == num)条件不满足时，锁会被释放，但线程会立即重新获取锁，导致忙等待（Busy Waiting），浪费CPU资源。 package Thread; import java.util.concurrent.locks.ReentrantLock; //三个线程分别打印 A，B，C，要求这三个线程一起运行，打印 n 次，输出形如“ABCABCABC....”的字符串。 public class ABC { public static int count = 12; public static int idx = 0; static ReentrantLock reentrantLock = new ReentrantLock(); public static void printABC(char c, int num){ for(int i = 1; i printABC('A', 0)); Thread thread2 = new Thread(() -> printABC('B', 1)); Thread thread3 = new Thread(() -> printABC('C', 2)); thread1.start(); thread2.start(); thread3.start(); } } 改进版，使用通知进行线程之间的交互。 package Thread; import java.util.concurrent.locks.ReentrantLock; //三个线程分别打印 A，B，C，要求这三个线程一起运行，打印 n 次，输出形如“ABCABCABC....”的字符串。 public class ABC { public static int count = 12; public static int idx = 0; static Object lock = new Object();//对象锁 public static void printABC(char c, int num){ for(int i = 1; i printABC('A', 0)); Thread thread2 = new Thread(() -> printABC('B', 1)); Thread thread3 = new Thread(() -> printABC('C', 2)); thread1.start(); thread2.start(); thread3.start(); } } 两个线程交替奇偶打印1-100 package Thread; public class EvenAndOdd { public static int idx = 1; static Object lock = new Object(); public static void printEven(boolean flag){ int c = flag ? 1 : 0; while(true){ synchronized (lock){ while(idx % 2 != c){ try { lock.wait(); } catch (InterruptedException e) { throw new RuntimeException(e); } } if(idx > 100) return; System.out.println(Thread.currentThread().getName() + \" \" + idx); idx ++; lock.notifyAll(); } } } public static void main(String[] args){ Thread thread1 = new Thread(() -> printEven(true)); Thread thread2 = new Thread(() -> printEven(false)); thread1.setName(\"even\"); thread2.setName(\"odd\"); thread1.start(); thread2.start(); } } 生产者-消费者 例如一个厨子10s生产一个，一个客人4s消费一个。 package Thread; import java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.BlockingQueue; public class CustomerAndProducer { public static BlockingQueue blockQueue = new ArrayBlockingQueue<>(100); public static class Producer implements Runnable{ @Override public void run() { while(true){ try { Thread.sleep(10000); blockQueue.add(\"food\"); System.out.println(\"厨师放了一个餐品\"); } catch (InterruptedException e) { throw new RuntimeException(e); } } } } public static class Consumer implements Runnable{ @Override public void run() { while(true){ try { Thread.sleep(4000); String food = blockQueue.take(); System.out.println(\"消费者消费了一个餐品\"); } catch (InterruptedException e) { throw new RuntimeException(e); } } } } public static void main(String[] args){ Thread thread1 = new Thread(new Producer()); Thread thread2 = new Thread(new Consumer()); thread1.start(); thread2.start(); } } 单例模式：懒汉，饿汉，双重校验锁 懒汉 package Singleton; public class Singleton { /*恶汉 private static Singleton instance = new Singleton(); private Singleton(){} public static Singleton getInstance(){ return instance; } */ /*懒汉 private static Singleton instance; private Singleton(){} public static Singleton getInstance(){ if(instance == null){ instance = new Singleton(); } } */ // 使用 volatile 关键字确保 instance 的可见性 private static volatile Singleton instance; // 私有构造函数，防止外部实例化 private Singleton() { // 初始化代码 } // 获取单例实例的静态方法 public static Singleton getInstance() { // 第一次检查，避免不必要的同步 if (instance == null) { // 加锁，确保线程安全 synchronized (Singleton.class) { // 第二次检查，防止多个线程同时通过第一次检查 if (instance == null) { instance = new Singleton(); } } } return instance; } } 随机数 有一个0-4的随机器rand4，如何实现0-6的随机器rand6，概率相同。拓展：rand X = func(rand Y)，实现func函数 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-03-04 11:27:21 "},"Chapter5/":{"url":"Chapter5/","title":"算法题有关","keywords":"","body":"第四章 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-02-12 23:35:54 "},"Chapter5/ACM输入输出.html":{"url":"Chapter5/ACM输入输出.html","title":"ACM输入输出","keywords":"","body":"ACM输入输出模板 输入部分分为C++和JAVA语言 牛客网在线编程算法篇输入输出练习 lower_bound 返回指向第一个值不小于 val 的位置，也就是返回第一个大于等于val值的位置。 前提是有序的情况下，upper_bound 返回第一个大于--val值的位置。 定义链表 #include using namespace std; struct ListNode{ int val; ListNode* nxt; ListNode(int _val):val(_val),nxt(nullptr){} ListNode(int _val, ListNode* _nxt):val(_val),nxt(_nxt){} } int main() { // 创建一个单链表 ListNode* head = nullptr; ListNode* pre = head; ListNode* cur = nullptr; int num; while(cin >> num) { if(num == -1) { break; } cur = new ListNode(num); if(head == nullprt) { head = cur; pre = cur; } else { pre.nxt = cur; pre = cur; } } return 0; } 定义二叉树 #include #include #include using namespace std; //定义树节点 struct TreeNode { int val; TreeNode* left; TreeNode* right; TreeNode():val(0),left(nullptr),right(nullptr){} TreeNode(int _val):val(_val),left(nullptr),right(nullptr){} TreeNode(int _val,TreeNode* _left,TreeNode* _right):val(0),left(_left),right(_right){} }; //根据数组生成树 TreeNode* buildTree(const vector& v) { vector vTree(v.size(),nullptr); TreeNode* root = nullptr; for(int i = 0; i left = vTree[2 * i + 1]; vTree[i]->right = vTree[2 * i + 2]; } } return root; } int main() { // 验证 vector v = {4,1,6,0,2,5,7,-1,-1,-1,3,-1,-1,-1,8}; TreeNode* root = buildTree(v); return 0; } Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-02-26 20:48:53 "},"Chapter5/输入数据.html":{"url":"Chapter5/输入数据.html","title":"输入数据格式","keywords":"","body":"输入数据 实现栈 输入： 6 push 1 pop top push 2 push 3 pop 输出： 1 error 3 C++ #include using namespace std; int arr[100010]; int cnt = 0; void push(int val) { arr[cnt ++] = val; } string top(){ if(cnt > n; while(n --) { string op; int val; cin >> op; if(op == \"push\") { cin >> val; push(val); } if(op == \"top\") { cout JAVA(Scnner): import java.io.BufferedReader; import java.io.InputStreamReader; import java.util.Scanner; // 注意类名必须为 Main, 不要有任何 package xxx 信息 public class Main { public static int[] arr = new int[100010]; public static int cnt = 0; public static void push(int val) { arr[cnt ++] = val; } public static String pop() { if(cnt 0){ String op = in.next(); if(\"push\".equals(op)) { int val = in.nextInt(); push(val); } if(\"pop\".equals(op)) { System.out.println(pop()); } if(\"top\".equals(op)) { System.out.println(top()); } } in.close(); } } Java(br): br.read() 只能读一个字符，例如输入6，则int为54，也就是ASCII码 直接用readLine() import java.io.BufferedReader; import java.io.IOException; import java.io.InputStreamReader; import java.util.Scanner; // 注意类名必须为 Main, 不要有任何 package xxx 信息 public class Main { public static int[] arr = new int[100010]; public static int cnt = 0; public static void push(int val) { arr[cnt ++] = val; } public static String pop() { if(cnt 0){ String[] t = br.readLine().split(\" \"); String op = t[0]; if(\"push\".equals(op)) { int val = Integer.valueOf(t[1]); push(val); } if(\"pop\".equals(op)) { System.out.println(pop()); } if(\"top\".equals(op)) { System.out.println(top()); } } br.close(); // in.close(); } } Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-03-02 18:06:50 "},"Chapter5/非常规代码题.html":{"url":"Chapter5/非常规代码题.html","title":"非常规代码题","keywords":"","body":"多线程打印ABC 三个线程分别打印 A，B，C，要求这三个线程一起运行，打印 n 次，输出形如“ABCABCABC....”的字符串。 在if (idx == num)条件不满足时，锁会被释放，但线程会立即重新获取锁，导致忙等待（Busy Waiting），浪费CPU资源。 package Thread; import java.util.concurrent.locks.ReentrantLock; //三个线程分别打印 A，B，C，要求这三个线程一起运行，打印 n 次，输出形如“ABCABCABC....”的字符串。 public class ABC { public static int count = 12; public static int idx = 0; static ReentrantLock reentrantLock = new ReentrantLock(); public static void printABC(char c, int num){ for(int i = 1; i printABC('A', 0)); Thread thread2 = new Thread(() -> printABC('B', 1)); Thread thread3 = new Thread(() -> printABC('C', 2)); thread1.start(); thread2.start(); thread3.start(); } } 改进版，使用通知进行线程之间的交互。 package Thread; import java.util.concurrent.locks.ReentrantLock; //三个线程分别打印 A，B，C，要求这三个线程一起运行，打印 n 次，输出形如“ABCABCABC....”的字符串。 public class ABC { public static int count = 12; public static int idx = 0; static Object lock = new Object();//对象锁 public static void printABC(char c, int num){ for(int i = 1; i printABC('A', 0)); Thread thread2 = new Thread(() -> printABC('B', 1)); Thread thread3 = new Thread(() -> printABC('C', 2)); thread1.start(); thread2.start(); thread3.start(); } } 两个线程交替奇偶打印1-100 package Thread; public class EvenAndOdd { public static int idx = 1; static Object lock = new Object(); public static void printEven(boolean flag){ int c = flag ? 1 : 0; while(true){ synchronized (lock){ while(idx % 2 != c){ try { lock.wait(); } catch (InterruptedException e) { throw new RuntimeException(e); } } if(idx > 100) return; System.out.println(Thread.currentThread().getName() + \" \" + idx); idx ++; lock.notifyAll(); } } } public static void main(String[] args){ Thread thread1 = new Thread(() -> printEven(true)); Thread thread2 = new Thread(() -> printEven(false)); thread1.setName(\"even\"); thread2.setName(\"odd\"); thread1.start(); thread2.start(); } } 生产者-消费者 例如一个厨子10s生产一个，一个客人4s消费一个。 package Thread; import java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.BlockingQueue; public class CustomerAndProducer { public static BlockingQueue blockQueue = new ArrayBlockingQueue<>(100); public static class Producer implements Runnable{ @Override public void run() { while(true){ try { Thread.sleep(10000); blockQueue.add(\"food\"); System.out.println(\"厨师放了一个餐品\"); } catch (InterruptedException e) { throw new RuntimeException(e); } } } } public static class Consumer implements Runnable{ @Override public void run() { while(true){ try { Thread.sleep(4000); String food = blockQueue.take(); System.out.println(\"消费者消费了一个餐品\"); } catch (InterruptedException e) { throw new RuntimeException(e); } } } } public static void main(String[] args){ Thread thread1 = new Thread(new Producer()); Thread thread2 = new Thread(new Consumer()); thread1.start(); thread2.start(); } } 单例模式：懒汉，饿汉，双重校验锁 懒汉 package Singleton; public class Singleton { /*恶汉 private static Singleton instance = new Singleton(); private Singleton(){} public static Singleton getInstance(){ return instance; } */ /*懒汉 private static Singleton instance; private Singleton(){} public static Singleton getInstance(){ if(instance == null){ instance = new Singleton(); } } */ // 使用 volatile 关键字确保 instance 的可见性 private static volatile Singleton instance; // 私有构造函数，防止外部实例化 private Singleton() { // 初始化代码 } // 获取单例实例的静态方法 public static Singleton getInstance() { // 第一次检查，避免不必要的同步 if (instance == null) { // 加锁，确保线程安全 synchronized (Singleton.class) { // 第二次检查，防止多个线程同时通过第一次检查 if (instance == null) { instance = new Singleton(); } } } return instance; } } 随机数 有一个0-4的随机器rand4，如何实现0-6的随机器rand6，概率相同。拓展：rand X = func(rand Y)，实现func函数 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-03-02 18:06:49 "},"Chapter4/":{"url":"Chapter4/","title":"面试有关","keywords":"","body":"第四章 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-02-12 23:35:54 "},"Chapter4/算法学习.html":{"url":"Chapter4/算法学习.html","title":"算法模板","keywords":"","body":"[TOC] 基础算法 进制转化 int get(string s, int b) // 将b进制的数转化成十进制 { int res = 0; // 秦九韶算法 for (auto c: s) res = res * b + c - '0'; return res; } 排序 快速排序 void quick_sort(int q[], int l, int r) { if (l >= r) return; int i = l - 1, j = r + 1, x = q[ l + r >> 1]; while (i x); if (i 归并排序 void merge_sort(int q[], int l, int r) { if (l >= r) return; int mid = l + r >> 1; merge_sort(q, l, mid); merge_sort(q, mid + 1, r); int k = 0, i = l, j = mid + 1; while (i 二分 整数二分 bool check(int x) {/* ... */} // 检查x是否满足某种性质 // 区间[l, r]被划分成[l, mid - 1]和[mid, r]时使用： int bsearch_2(int l, int r) { while (l > 1; if (check(mid)) l = mid; else r = mid - 1; } return l; } // 区间[l, r]被划分成[l, mid]和[mid + 1, r]时使用： int bsearch_1(int l, int r) { while (l > 1; if (check(mid)) r = mid; // check()判断mid是否满足性质 else l = mid + 1; } return l; } 浮点数二分 bool check(double x) {/* ... */} // 检查x是否满足某种性质 double bsearch_3(double l, double r) { const double eps = 1e-6; // eps 表示精度，一般题目要求是1e-6时，eps是1e-8 while (r - l > eps) { double mid = (l + r) / 2; if (check(mid)) r = mid; else l = mid; } return l; } 前缀和与差分 一维前缀和 #include using namespace std; const int N = 100060; int n, m, l, r, a[N], b[N]; int main() { cin >> n >> m; for (int i = 1; i > a[i]; b[i] = b[i - 1] + a[i]; } while (m--) { cin >> l >> r; cout 二维前缀和 #include using namespace std; const int N = 1005; int a[N][N], b[N][N]; int n, m, q; int main() { cin >> n >> m >> q; for (int i = 1; i > a[i][j]; b[i][j] = b[i][j - 1] + b[i - 1][j] - b[i - 1][j - 1] + a[i][j]; } } while (q--) { int x1, y1, x2, y2; cin >> x1 >> y1 >> x2 >> y2; cout 一维差分 #include using namespace std; const int N = 100010; int n, m, a[N], b[N]; int main() { cin >> n >> m; for (int i = 1; i > a[i]; b[i] = a[i] - a[i - 1]; } while (m--) { int l, r, x; cin >> l >> r >> x; b[l] += x, b[r + 1] -= x; } for (int i = 1; i 二维差分 /* a[][]是差分矩阵，对差分矩阵求前缀和就得到了原数组，这和一维是完全一样的 不同于一维差分的是，二维差分进行修改区间需要修改四个点 a[x1][y1]+=d;进行求前缀和的时候，这会让所有x1,y1之后求的点都改变 所以要进行三个操作抵消 a[x2+1][y1]-=d,a[x1][y2+1]-=d,a[x2+1][y2+1]+=d; insert函数便诞生了 最终的数组对差分数组求二维前缀和就行 */ #include const int N = 1005; using namespace std; int n, m, q; int a[N][N], b[N][N]; void insert(int x1, int y1, int x2, int y2, int d) { a[x1][y1] += d; a[x2 + 1][y1] -= d; a[x1][y2 + 1] -= d; a[x2 + 1][y2 + 1] += d; } int main() { cin >> n >> m >> q; for (int i = 1; i > x; insert(i, j, i, j, x);//可以看成在一个小区间进行insert } } while (q--) { int x1, y1, x2, y2, d; cin >> x1 >> y1 >> x2 >> y2 >> d; insert(x1, y1, x2, y2, d); } for (int i = 1; i 高精度 加法 #include using namespace std; const int N = 1e6; vector add(vector A, vector B) { vector C; int t = 0;//进位 for (int i = 0; i > a >> b; vector A, B; for (int i = a.size() - 1; i >= 0; i--) A.push_back(a[i] - '0'); for (int i = b.size() - 1; i >= 0; i--) B.push_back(b[i] - '0'); vector C = add(A, B); for (int i = C.size() - 1; i >= 0; i--) cout 减法 #include using namespace std; const int N = 1e6; bool cmp(vector a, vector b)//比较大小 { if (a.size() != b.size()) return a.size() > b.size(); for (int i = a.size() - 1; i >= 0; i--) { if (a[i] != b[i]) return a[i] > b[i]; } return true; } vector sub(vector a, vector b) { vector c; int t = 0;//借位1 for (int i = 0; i 1 && c.back() == 0) c.pop_back();//去除前导0 return c; } int main() { vector A, B, C; string a, b; cin >> a >> b; for (int i = a.size() - 1; i >= 0; i--) A.push_back(a[i] - '0'); for (int i = b.size() - 1; i >= 0; i--) B.push_back(b[i] - '0'); if (cmp(A, B)) C = sub(A, B); else { cout = 0; i--) cout 乘法 大数乘小数 #include using namespace std; const int N = 1e6; vector mul(vector A, int b) { vector c; int t = 0;//进位 for (int i = 0; i 1 && c.back() == 0) c.pop_back(); return c; } int main() { string a; int b; cin >> a >> b; vector A; for (int i = a.size() - 1; i >= 0; i--) A.push_back(a[i] - '0'); vector c = mul(A, b); for (int i = c.size() - 1; i >= 0; i--) cout 除法 大数除小数，先输出商，在输出余数 #include using namespace std; vector div(vector &A, int B, int &r) {//r传入r的地址，便于直接对余数r进行修改 vector C; for (int i = 0; i 1 && C.back() == 0) C.pop_back(); return C; } int main() { string a; int B, r = 0; //代表余数 cin >> a >> B; vector A; for (int i = 0; i = 0; i--) cout 离散化 离散化其实就是排序+去重，然后把值映射成[0,n-1]或者[1,n]，注意此时的n是去重后的n。 映射其实就是把值放入数组（vector）中，下标就是数组下标。 alls表示所有待离散化的数。 注意处理离散化问题的时候，往往需要先把所有用到的坐标存起来并且离散化，之后在进行各种操作。 #include using namespace std; typedef pair PII; vector add, query; vector alls; int a[300010], b[300010];//最多这么多下标 int find(int x) { int l = 0, r = alls.size() - 1;//因为就是存在vector里面 while (l > 1; if (alls[mid] >= x) r = mid; else l = mid + 1; } return l + 1; } int main() { int n, m; cin >> n >> m; for (int i = 1; i > x >> c; add.push_back({x, c}); alls.push_back(x); } for (int i = 1; i > l >> r; query.push_back({l, r}); alls.push_back(l), alls.push_back(r); } sort(alls.begin(), alls.end()); alls.erase(unique(alls.begin(), alls.end()), alls.end()); for (int i = 0; i 读入 string s; getline(cin,s);//空格不断行输入 stringstream ss(s); int x;//也可以改为string,double,char; while(ss >> x) cout using namespace std; int main() { int a; cin >> a; getchar(); for (int i = 1; i using namespace std; int main() { int a; cin >> a; getchar(); for (int i = 1; i 矩阵 #include #define CLOSE ios::sync_with_stdio(false);cin.tie(0);cout.tie(0) #define endl \"\\n\" typedef long long LL; const int N = 1e5 + 10, M = N, mod = 1e9 + 7; using namespace std; struct Mat{ int a[105][105]; int r, c; Mat(int _r, int _c){ r = _r, c = _c; memset(a, 0, sizeof a); } //单位矩阵 void unit(){ memset(a, 0, sizeof a); for(int i = 1; i >= 1; } return ans; } void output(){ for(int i = 1; i > m1 >> n1 >> m2 >> n2; Mat A(n1, m1), B(n2, m2); for(int i = 1; i > A.a[i][j]; } } for(int i = 1; i > B.a[i][j]; } } Mat ans = A * B; ans.output(); return 0; } STL Set/multiset set和multiset会根据特定的排序原则将元素排序。两者不同之处在于，multisets允许元素重复，而set不允许重复。 s.insert() //log(n) s.erase() //log(n) s.begin() //返回第一个元素的地址 s.end() //返回最后一个元素下一个元素的地址 s.find(x) //log(n) 返回一个地址（迭代器），如果找不到当前元素x返回s.end() s.lower_bound(x)// log(n) 返回一个元素>=x的地址（迭代器），如果找不到当前元素x返回s.end() s.upper_bound(x)// log(n) 返回一个元素>x的地址（迭代器），如果找不到当前元素x返回s.end() 二分函数 lower_bound(首地址,尾地址,x) //左闭右开，查找>=x的第一个数，若查找不到，返回尾地址 upper_bound(首地址,尾地址,x) //左闭右开，查找>x的第一个数 ，若查找不到，返回尾地址 //注意此函数返回的是地址，若要变成下标，若是数组a只需要-a,若是vector只需要-v.begin() 数据结构 字符串哈希 不能把字符映射成0，否则$A,AA,AAA$都是一样的。经验值$P=131$，或者$P=13331$，Q取成$2^{64}$,这样冲突的可能性很小。 /* 全称字符串前缀哈希法，把字符串变成一个p进制数字（哈希值），实现不同的字符串映射到不同的数字。 对形如 X1X2X3⋯Xn−1XnX1X2X3⋯Xn−1Xn 的字符串,采用字符的ascii 码乘上 P 的次方来计算哈希值。 映射公式 (X1×Pn−1+X2×Pn−2+⋯+Xn−1×P1+Xn×P0)modQ Q一般qu公式 (X1×Pn−1+X2×Pn−2+⋯+Xn−1×P1+Xn×P0)modQ Q一般 注意 1. 任意字符不可以映射成0，否则会出现不同的字符串都映射成0的情况，比如A,AA,AAA皆为0 2. 冲突问题：通过巧妙设置P (131 或 13331) , Q (2的64次方)的值，一般可以理解为不产生冲突。 公式：前缀hash h[i]=h[i-1]*P+str[i]; 区间hash h[l,r]=h[r]−h[l−1]×P(r−l+1)次方 */ #include typedef unsigned long long ULL;//代替对2的64次方取模 const int N = 1e6, P = 131;//P一般取131或者13331， using namespace std; ULL h[N], p[N];//h放的是前缀hash值，p放的是P的多少次方； int l1, r1, l2, r2, n, m; char str[N]; ULL query(int l, int r)//类似于前缀和，只不过需要一个偏移量 { return h[r] - h[l - 1] * p[r - l + 1];//公式 } int main() { scanf(\"%d%d%s\", &n, &m, str + 1);//注意s从数组的第一位开始存，类似于前缀和 h[0] = 0, p[0] = 1;//初始化，P的0次方为1 for (int i = 1; i KMP 复杂度：$O(n)$ $p$是模板串，长度为$m$，$s$是要匹配的串，长度为$n$ $next[j]$的含义，以$p[j]$为结尾，最大的后缀和前缀相等的长度，当然长度要小于$j$，否则肯定是$k$ #include using namespace std; const int N = 1e6 + 10; int ne[N], n, m; char s[N], p[N]; int main() { cin >> m >> (p + 1) >> n >> (s + 1); ne[0] = ne[1] = 0; for (int i = 2, j = 0; i 单调栈和队列 给定一个长度为 N 的整数数列，输出每个数左边第一个比它小的数，如果不存在则输出 −1。 #include #define CLOSE ios::sync_with_stdio(false);cin.tie(0);cout.tie(0) using namespace std; typedef long long ll; typedef pairPII; const int N=1e6; stack stk; int main() { CLOSE; int n; cin>>n; for(int i=1;i>x; while(!stk.empty()&&stk.top()>=x) stk.pop();//当前这个点一定不会被用到，因为x更优 if(!stk.empty()) cout 单调队列 #include using namespace std; const int N = 1e6; deque q;//双端队列 int a[N]; int main() { int n, k; cin >> n >> k; for (int i = 1; i > a[i]; for (int i = 1; i = a[i]) q.pop_back();//制作队列 q.push_back(i); while (!q.empty() && i - q.front() + 1 > k) q.pop_front();//因为要求长度是k if (i >= k) cout k) q.pop_front(); if (i >= k) cout 并查集 #include using namespace std; const int N = 1e6; int p[N], siz[N]; int find(int x) { if (p[x] != x) p[x] = find(p[x]); return p[x]; } int merge(int x, int y) { int fx = find(x); int fy = find(y); if (fx != fy) { p[fx] = fy; siz[fy] += siz[fx]; } } int main() { int n, m; cin >> n >> m; for (int i = 1; i > op; if (op == \"C\") { cin >> a >> b; merge(a, b); } else if (op == \"Q1\") { cin >> a >> b; if (find(a) == find(b)) cout > a; cout DFS序 树上单点修改，子树求和问题。 dfs把子树转换成区间，子树对应的区间一定是连续的。 #include #define CLOSE ios::sync_with_stdio(false);cin.tie(0);cout.tie(0) #define endl \"\\n\" typedef long long LL; const int N = 1e6 + 10, M = 2 * N, mod = 1e9 + 7; using namespace std; int n, m, k, a[N], t[N]; int h[N], e[M], ne[M], idx; int l[N], r[N], timestamp; void add(int x, int y){ e[idx] = y, ne[idx] = h[x], h[x] = idx ++; } int lowbit(int x){return x & -x;} void addT(int x, int c){ for(int i = x; i > n >> m >> k; memset(h, -1, sizeof h); for(int i = 1; i > a[i]; for(int i = 1; i > x >> y; add(x, y), add(y, x); } dfs(k, -1); for(int i = 1; i > op >> x; if(op == 1){ cin >> y; addT(l[x], y); } else{ cout ST表 #include #define CLOSE ios::sync_with_stdio(false);cin.tie(0);cout.tie(0) #define endl \"\\n\" typedef long long LL; const int N = 2e5 + 10, M = 19, mod = 1e9 + 7; using namespace std; int f[N][M], a[N]; void init(int n){ int x = log2(n); for(int j = 0; j > n; for(int i = 1; i > a[i]; init(n); cin >> m; while(m --){ int l, r; cin >> l >> r; cout 树状数组 单点修改，区间求和 #include typedef long long LL; const int N = 5e5 + 10, M = N; using namespace std; int a[N], n, m; LL t[N]; int lowbit(int x){return x & -x;} void add(int x, int c){ for(int i = x; i > n >> m; for(int i = 1; i > a[i]; add(i, a[i]); } while(m --){ int op, x, y; cin >> op >> x >> y; if(op == 1) add(x, y); else cout 区间修改，单点查询，即维护差分数组 #include typedef long long LL; const int N = 5e5 + 10, M = N; using namespace std; int a[N], n, m; LL t[N]; int lowbit(int x){return x & -x;} void add(int x, int c){ for(int i = x; i > n >> m; for(int i = 1; i > a[i]; add(i, a[i] - a[i - 1]); } while(m --){ int op; cin >> op; if(op == 1) { int l, r, k; cin >> l >> r >> k; add(l, k), add(r + 1, -k); } else { int x; cin >> x; cout #include #define CLOSE ios::sync_with_stdio(false);cin.tie(0);cout.tie(0) #define endl \"\\n\" typedef long long LL; const int N = 1e6 + 10, M = N, mod = 1e9 + 7; using namespace std; int n, q; LL t1[N], t2[N], a[N], b[N]; int lowbit(int x) {return x & -x;} void add(int x, LL c, LL t[]){ for(int i = x; i > n >> q; for(int i = 1; i > a[i]; } for(int i = 1; i > op; if(op == 1){ int l, r; LL k; cin >> l >> r >> k; add(r + 1, -k, t1), add(l, k, t1);//改差分数组 add(r + 1, -k * (r + 1), t2), add(l, k * l, t2);//改i*差分数组 } else{ int l, r; cin >> l >> r; cout 线段树 建树复杂度 $O(n)$,其他操作$O(log(n))$ 父节点：x/2 int会自动下取整 x>>1; 左儿子：2x x 凡是只修改单点的，是不需要懒标记的，修改区间的需要懒标记。 pushup（）放在build和modify的下面 pushdown放在query和modify的上面，并且pushdown放在的是要分裂的地方； #include using namespace std; typedef long long LL; const int N = 1e5 + 100; struct node { LL l, r, sum, add; } t[N * 4];//开四倍空间 LL a[N]; void pushup(LL u)//用子节点的信息更新父亲节点 { t[u].sum = t[u > 1; build(u > 1;//这里一定不要错了，l,r是要修改的区间 if (l mid) modify(u > 1;//不要用l,r, l,r是要查询的区间左右端点 LL ans = 0; if (l mid) ans += query(u > n >> m; for (int i = 1; i > op >> l >> r; if (op == 'Q') { cout > d; modify(1, l, r, d); } } } 字典树 void insert(string s){ int p = 0; for(int i = 0; i 01字典树 cnt[p] ++，保证只用一次，用完了就不会跳到x！！ #include #define CLOSE ios::sync_with_stdio(false);cin.tie(0);cout.tie(0) #define endl \"\\n\" typedef long long LL; const int N = 3e5 + 10, M = N, mod = 1e9 + 7; using namespace std; int son[N][2], cnt[N], idx; int a[N]; void insert(int x){ int p = 0; for(int i = 30; i >= 0; i --){ int u = (x >> i) & 1; if(!son[p][u]) son[p][u] = ++ idx; p = son[p][u]; cnt[p] ++; } } int query(int x){ int p = 0, ans = 0; for(int i = 30; i >= 0; i --){ int u = (x >> i) & 1; if(cnt[son[p][u]]){ p = son[p][u]; } else { p = son[p][u ^ 1]; ans += 1 > n; for(int i = 1; i > a[i]; } for(int i = 1; i > x; insert(x); } for(int i = 1; i 搜索 BFS bfs中的队列有两个性质 1：两段性 距离： x x x x (x+1) (x+1) (x+1) 2：单调性 bfs每次取出的队头都是距离最近的点，类似于Dijisitla， 3：只要能时时刻刻满足两段性和单调性，呢么bfs就相当于Dijisitla，所以bfs是正确的； 双端队列广搜 1：双端队列广搜解决的问题是边权是0和边权是1的问题，普通的BFS是解决边权都是1（边权相等）的问题； 2：双端队列可以说是简化版的堆优化Dijisitla，但是复杂度更优为O(n)； 3：因为有两种权重，第一次搜到的点不一定就是最短距离，与堆优化Dijkstra 一样，必须在出队时才知道每个点最终的最小值，而和一般的bfs不一样，原因是如下图所示 4.与Dijisitla不同是，他是搜索的时候用，不需要建边，不需要建图，与普通的bfs题的不同是有两种边。 #include const int N = 1006; using namespace std; struct node { int x, y; }; int a[N][N], dist[N][N], st[N][N]; int wy[4][2] = {1, 0},{-1, 0},{0, 1},{0,-1}; int qdx, qdy; int bfs() { memset(dist, 0x3f, sizeof dist); deque q; q.push_front({qdx, qdy}); dist[qdx][qdy] = 0; while (!q.empty()) { node one = q.front(); q.pop_front(); if (st[one.x][one.y]) continue; else st[one.x][one.y] = 1; for (int i = 0; i = 0 && xx = 0 && yy dist[one.x][one.y] + a[xx][yy]) { dist[xx][yy] = dist[one.x][one.y] + a[xx][yy]; if (a[xx][yy] == 1)q.push_back({xx, yy}); else q.push_front({xx, yy}); } } } } return dist[0][0]; } int main() { int n; cin >> n >> qdx >> qdy; for (int i = 1; i > x >> y; a[x][y] = 1; } cout 双向广搜 一般用于最小步数模型，最短路模型一般用不到，因为本来就不会超时，比如我们要看一个字符串十步之内是不是能变到另一个字符串，如果有六个变化方式，直接BFS是6^10,如果是双向广搜的话，只有3*6的五次方，显然是一个很大的优化，而不是简单的/2。 //字符串函数 s.substr(qd,len); #include using namespace std; string A, B, x, y; string a[7], b[7]; unordered_map da, db; queue qa, qb; int idx = 0; int extend(queue &q, unordered_map &da, unordered_map &db, string a[], string b[])//别忘了加引用符号 { string s = q.front(); q.pop(); for (int i = 0; i B { int bs; if (qa.size() > A >> B; while (cin >> x >> y) { idx++; a[idx] = x, b[idx] = y; } int ans = bfs(); if (ans == -1) puts(\"NO ANSWER!\"); else cout 图和树 二叉树 先序+中序 #include #define CLOSE ios::sync_with_stdio(false);cin.tie(0);cout.tie(0) using namespace std; typedef long long LL; typedef pair PII; const int N = 1e3; map node; map mp; int n; int a[N], b[N]; vector v; int dfs(int l1, int r1, int l2, int r2)//先序+中序 { if (l1 > r1 || l2 > r2) return 0; int root = a[l1]; int k = mp[root]; int sum = k - l2; node[root].first = dfs(l1 + 1, l1 + sum, l2, k - 1); node[root].second = dfs(l1 + sum + 1, r1, k + 1, r2); return root; } int dfs(int l1, int r1, int l2, int r2)//后序+中序 { if (l1 > r1 || l2 > r2) return 0; int root = a[r1]; int k = mp[root]; int sum = k - l2; node[root].first = dfs(l1, l1 + sum - 1, l2, k - 1); node[root].second = dfs(l1 + sum, r1 - 1, k + 1, r2); return root; } void bfs()//层次 { queue q; q.push(a[1]); int cnt = 0; while (!q.empty()) { int t = q.front(); q.pop(); cnt++; if (cnt != n) cout > n; for (int i = 1; i > b[i]; mp[b[i]] = i; } for (int i = 1; i > a[i]; } dfs(1, n, 1, n); bfs(); return 0; } 树的直径和重心 直径 #include using namespace std; const int N = 10010, M = 20010; int h[N], ne[M], w[M], e[M], idx; int ans; // 保存最长路径 int t; // 保存找到的最远点 int n; void add(int a, int b, int c) { e[idx] = b, w[idx] = c, ne[idx] = h[a], h[a] = idx++; } void dfs(int u, int father, int dist) { for (int i = h[u]; ~i; i = ne[i]) { int j = e[i]; if (j == father) continue; dfs(j, u, dist + w[i]); } // 找到最大的dist用来更新答案ans和点t if (dist > ans) { ans = dist; t = u; } } int main() { memset(h, -1, sizeof h); scanf(\"%d\", &n); for (int i = 0; i 重心定义：重心是指树中的一个结点，如果将这个点删除后，剩余各个连通块中点数的最大值最小，那么这个节点被称为树的重心。树的重心不唯一。 定义函数$dfs(u)$，表示以$u$为根的子树的节点个数。 #include using namespace std; const int N = 100010, M = N * 2; int n; int h[N], e[M], ne[M], idx;//存图 int ans = 0x3f3f3f3f;//最终的答案 bool st[N];//表示哪些点已经被遍历过 //加边函数 void add(int a, int b) { e[idx] = b, ne[idx] = h[a], h[a] = idx++; } //dfs(u)返回的结果是以u为根的子树的节点个数 int dfs(int u) { st[u] = true;//u节点被访问过 int size = 0;//size存的是若删除u节点，剩余各个连通块中点数的最大值的最小是多少。 int sum = 1;// sum存的是以u为根的子树的节点个数，也就是dfs函数最后返回的值，初值是1 for (int i = h[u]; i != -1; i = ne[i]) { int j = e[i];//j是子节点的编号 if (st[j]) continue;//只访问没有访问过的点 int s = dfs(j);//子树的节点个数 size = max(size, s);//取一个max sum += s;//以u为根的子树的节点个数需要加上s } size = max(size, n - sum);//不要忘了父节点也构成了一个联通块 ans = min(ans, size);//更新答案 return sum; } int main() { scanf(\"%d\", &n); memset(h, -1, sizeof h);//别忘了初始化图 for (int i = 0; i 拓扑排序 #include using namespace std; const int N = 1e5 + 100, M = N; int h[N], e[M], ne[M], idx, din[N], n, m; vector v; void add(int x, int y) { e[idx] = y, ne[idx] = h[x], h[x] = idx++; } void topsort() { queue q; for (int i = 1; i > n >> m; for (int i = 1; i > x >> y; add(x, y); din[y] ++; } topsort(); if (v.size() != n) cout 最小生成树 Kruskal复杂度 $O(n*log(n))$ #include #define CLOSE ios::sync_with_stdio(false);cin.tie(0);cout.tie(0) using namespace std; typedef long long ll; typedef pairPII; const int N=1e5+100,M=2e5+100; struct node{ int x,y,w; }edge[M]; bool cmp(node a,node b) { return a.w>n>>m; for(int i=1;i>edge[i].x>>edge[i].y>>edge[i].w; } sort(edge+1,edge+1+m,cmp);//这里排序别排错了 int sum=0,cnt=0; for(int i=1;i 次小生成树 严格次小生成树：边权之和必须小于最小生成树 非严格次小生成树，边权之和可以与最小生成树相等 //离谱，卡vector！！！ #include #define CLOSE ios::sync_with_stdio(false);cin.tie(0);cout.tie(0) using namespace std; typedef long long ll; typedef pair PII; const int N = 1e5 + 10, M = 6e5 + 100, inf = 0x3f3f3f3f; int h[N], e[M], ne[M], w[M], idx; int f[N][18], depth[N], d1[N][18], d2[N][18], p[N]; int n, m; ll sum; void add(int x, int y, int z) { e[idx] = y, w[idx] = z, ne[idx] = h[x], h[x] = idx++; } int find(int x) { if (p[x] != x) p[x] = find(p[x]); return p[x]; } struct node { int x, y, w; bool used; } edge[M]; bool cmp(node a, node b) { return a.w q; q.push(1); while (!q.empty()) { int t = q.front(); q.pop(); for (int i = h[t]; ~i; i = ne[i]) { int j = e[i]; if (depth[j] > depth[t] + 1) { q.push(j); depth[j] = depth[t] + 1; f[j][0] = t; d1[j][0] = w[i], d2[j][0] = -inf; for (int k = 1; k d1[j][k]) { d2[j][k] = d1[j][k], d1[j][k] = distance[s]; } else if (distance[s] d2[j][k]) d2[j][k] = distance[s]; } } } } } } int lca(int x, int y, int w) { int distance[M]; int cnt = 0; if (depth[x] = 0; i--) { if (depth[f[x][i]] >= depth[y]) { distance[cnt++] = d1[x][i]; distance[cnt++] = d2[x][i]; x = f[x][i]; } } if (x != y) { for (int i = 17; i >= 0; i--) { if (f[x][i] != f[y][i]) { distance[cnt++] = d1[x][i]; distance[cnt++] = d2[x][i]; distance[cnt++] = d1[y][i]; distance[cnt++] = d2[y][i]; x = f[x][i], y = f[y][i]; } } distance[cnt++] = d1[x][0]; distance[cnt++] = d2[x][0]; distance[cnt++] = d1[y][0]; distance[cnt++] = d2[y][0]; } int dist1 = -inf, dist2 = -inf; for (int i = 0; i dist1) { dist2 = dist1, dist1 = it; } else if (it dist2) dist2 = it; } if (w > dist1) return w - dist1; else if (w > dist2) return w - dist2; else return inf; } int main() { scanf(\"%d%d\", &n, &m); for (int i = 1; i 最短路 Dijkstra 只能正权 #include typedef long long LL; const int N = 1e5 + 10, M = 2 * N; using namespace std; int h[N], e[M], ne[M], w[M], dist[N], idx; bool st[N]; int n, m; struct node{ int x, dist; }; struct cmp{ bool operator()(node a, node b){ return a.dist > b.dist; } }; void add(int x, int y, int z){ e[idx] = y, w[idx] = z, ne[idx] = h[x], h[x] = idx ++; } void dij(){ priority_queue, cmp> q; q.push({1, 0}); dist[1] = 0; while(!q.empty()){ node t = q.top(); q.pop(); if(st[t.x]) continue; st[t.x] = true; for(int i = h[t.x]; ~ i; i = ne[i]){ int j = e[i]; if(dist[j] > dist[t.x] + w[i]){ dist[j] = dist[t.x] + w[i]; q.push({j, dist[j]}); } } } } int main() { cin >> n >> m; memset(h, -1, sizeof h); memset(dist, 0x3f, sizeof dist); for(int i = 1; i > x >> y >> z; add(x, y, z); } dij(); cout 0x3f3f3f3f / 2 ? -1 : dist[n]); return 0; } 朴素版 #include using namespace std; const int N = 600; int a[N][N], bj[N], s[N]; int n, m, x, y, z; int f() { s[1] = 0; for (int i = 1; i 0x3f3f3f3f / 2) return -1; else return s[n]; } int main() { memset(a, 0x3f, sizeof a); memset(s, 0x3f, sizeof s); cin >> n >> m; for (int i = 1; i > x >> y >> z; a[x][y] = min(a[x][y], z); } int t = f(); cout Bellman-ford 给定一个 n 个点 m 条边的有向图，图中可能存在重边和自环， 边权可能为负数。 请你求出从 1 号点到 n 号点的最多经过 k 条边的最短距离，如果无法从 1 号点走到 n 号点，输出 impossible。 注意：图中可能存在负权回路 。 #include #include #include using namespace std; const int N = 510, M = 10010; struct Edge{ int a, b, c; }edges[M]; int n, m, k, dist[N], last[N]; void bellman_ford() { memset(dist, 0x3f, sizeof dist); dist[1] = 0; for (int i = 0; i 0x3f3f3f3f / 2) puts(\"impossible\"); else printf(\"%d\\n\", dist[n]); return 0; } Spfa 负权可以 #include using namespace std; const int N = 1e5 + 100, M = N; int h[N], e[M], ne[M], w[M], idx, n, m, st[N], dist[N]; void add(int x, int y, int z) { e[idx] = y, w[idx] = z, ne[idx] = h[x], h[x] = idx++; } void spfa() { queue q; memset(dist, 0x3f, sizeof dist); dist[1] = 0; st[1] = 1; q.push(1); while (!q.empty()) { int t = q.front(); q.pop(); st[t] = 0; for (int i = h[t]; ~i; i = ne[i]) { int j = e[i]; if (dist[j] > dist[t] + w[i]) { dist[j] = dist[t] + w[i]; if (!st[j]) { st[j] = 1; q.push(j); } } } } } int main() { memset(h, -1, sizeof h); cin >> n >> m; for (int i = 1; i > x >> y >> z; add(x, y, z); } spfa(); if (dist[n] >= 0x3f3f3f3f / 2) cout Floyd Floyd可以解决的问题：不含负环 1：最短路 floyd实际上是一种dp，d[k,i,j]表示从i出发到达j，只经过节点编号不超过k的最短路。 呢么可以分为两种，包含节点k和不包含节点k，因为没有负环，所以floyd算的的最短路中一定没有重复点，否则就有负环。 呢么d[k,i,j]=min(d[k-1,i,j],d[k-1,i,k]+d[k-1,k,j];发现可以去掉一维 d[i,j]=min(d[i,j],d[i,k]+d[k,j]); 注意floyd中有时候要用到一个函数memcpy(d,g,sizeof g);//d是dist数组，g是原图， 因为我们跑最短路最好不要在原图上面跑，后面可能会用到原图。 memset(g,0x3f,sizeof g);//初始化 for(int i=1;i>x>>y>>z; g[x][y]=g[y][x]=min(g[x][y],z);//防止有重边 } for(int k=1;kg[i][k]+g[k][j]) g[i][j]=g[i][k]+g[k][j]; 2：传递背包：离散数学i可以到k，k可以到j，呢么i就可以到j。初始化化可以到达的点为1,不能到达的点为0。 3：最小环 我们把环分类，按照环中最大节点的编号分类。 我们开始枚举k的时候，已经求得了任意两点只经过编号为1~k-1的最短路径，这恰好可以帮助我们求环。 环一定是i-k-j这是边，然后j-…-i，这个路径只能包含1~k-1，否则环还可以更短。 所以当k固定的时候，我们只需要枚举i和j（i和j应该都小于k）。 pos[i,j]表示i到j需要经过的中间节点 #include using namespace std; const int N = 105; int g[N][N], pos[N][N], d[N][N]; int n, m; vector v; void get_pos(int x, int y)//输出x到y之间不包括x和y的道路 { if (pos[x][y] == 0) return;//x和y之间没有中间点 get_pos(x, pos[x][y]); v.push_back(pos[x][y]); get_pos(pos[x][y], y); } int main() { cin >> n >> m; memset(g, 0x3f, sizeof g);//初始化 for (int i = 1; i > x >> y >> z; g[x][y] = g[y][x] = min(g[x][y], z); } memcpy(d, g, sizeof g);//一个存图，一个跑最短路！！！ int ans = 1e9; //如果存在一个最小环的话，呢么环上的点必定不重复 for (int k = 1; k j，最大点编号为k-1的最短路 //我们把环分类，以环中最大点的编号分类,呢么环一定是 i-k-j-...-i，注意i-k-j为两个边， //我们枚举i到j就行，所以只要i-j（只经过1~k-1个点）最小就行 for (int i = 1; i (long long) d[i][j] + g[i][k] + g[k][j])//有可能0x3f3f3f3f累加爆int { v.clear(); ans = d[i][j] + g[i][k] + g[k][j]; v.push_back(i); v.push_back(k); v.push_back(j); get_pos(j, i); } } } for (int i = 1; i d[i][k] + d[k][j]) { d[i][j] = d[i][k] + d[k][j]; pos[i][j] = k; } } } } if (ans == 1e9) puts(\"No solution.\"); else { for (auto i: v) { cout 4：恰好经过k条边的最短路 判负环 （1）由于负环可能并不包含1号点，或者图不连通。所以要建立一个超级源点，从这个点向各个点连一条边权是0的边，然后进行Spfa的第一次出队更新，这等价于初始就将所有点入队。 （2）所以当存在超级源点的时候，求负环时不需要将所有点全部入队了。 （3）求负环或者正环，与dist[i]初始值无关，因为有负环dist就会被不断更新为负无穷，反之为正无穷。但是求最短路顺便判断负环的时候，dist[i]设为正无穷。求最长路顺便判断正环的时候，dist[i]设为负无穷。 #include using namespace std; const int N = 1e5 + 100, M = N; int h[N], e[M], ne[M], w[M], idx, n, m, st[N], dist[N], cnt[N]; void add(int x, int y, int z) { e[idx] = y, w[idx] = z, ne[idx] = h[x], h[x] = idx++; } bool spfa() { queue q; memset(dist, 0x3f, sizeof dist); for (int i = 1; i dist[t] + w[i]) { dist[j] = dist[t] + w[i]; cnt[j] = cnt[t] + 1; if (cnt[j] >= n) return true; if (!st[j]) { st[j] = 1; q.push(j); } } } } return false; } int main() { memset(h, -1, sizeof h); cin >> n >> m; for (int i = 1; i > x >> y >> z; add(x, y, z); } if (spfa()) cout 差分约束 只要存在一个点可以走到所有点，呢么一定可以走到所有边。 当建立超级源点的边权为0时，等价于将所有点入队。边权不为0还是得建立一个超级源点。 有超级源点了，判断负环不需要将所有点入队，这个超级源点可能是题目中有，或者你自己建立。 如果所有边权都是>=0，并且要求最长路，呢么可以用强连通分量解决，正环等价于强连通分量不存再一条>0的边 最近公共祖先 #include #define CLOSE ios::sync_with_stdio(false);cin.tie(0);cout.tie(0) #define endl \"\\n\" typedef long long LL; const int N = 1e5 + 10, M = 2 * N, mod = 1e9 + 7; using namespace std; int h[N], e[M], ne[M], idx, root, n, m; int f[N][18], depth[N]; void add(int x, int y){ e[idx] = y, ne[idx] = h[x], h[x] = idx ++; } void dfs(int u, int fa){ for(int i = h[u]; ~i; i = ne[i]){ int j = e[i]; if(j == fa) continue; depth[j] = depth[u] + 1; f[j][0] = u; for(int k = 1; k = 0; k --){ if(depth[f[x][k]] >= depth[y]){ x = f[x][k]; } } if(x == y) return x; //没有depth了 for(int k = 17; k >= 0; k --){ if(f[x][k] != f[y][k]){ x = f[x][k]; y = f[y][k]; } } return f[x][0]; } int main() { cin >> n; memset(h, -1, sizeof h); for(int i = 1; i > x >> y; if(y == -1) { root = x; continue; } add(x, y), add(y, x); } depth[root] = 1; dfs(root, -1); cin >> m; while(m --){ int x, y; cin >> x >> y; int z = lca(x, y); if(z == x) cout 树上差分 将a到b路径上的所有边全部+c，可以先d[a]+=c，d[b]+=c , d[lca(a,b)]-=2c; 最后在一遍dfs求出最终的各边权值 以x节点为根，子树的点权值之和就是x节点与之父节点的边权。 #include using namespace std; typedef long long LL; const int N = 100010, M = 2 * N; int h[N], e[M], ne[M], idx; int depth[N], d[N], f[N][18]; int n, m; LL ans; void add(int x, int y) { e[idx] = y, ne[idx] = h[x], h[x] = idx++; } void bfs() { memset(depth, 0x3f, sizeof depth); depth[0] = 0, depth[1] = 1; queue q; q.push(1); while (!q.empty()) { int t = q.front(); q.pop(); for (int i = h[t]; ~i; i = ne[i]) { int j = e[i]; if (depth[j] > depth[t] + 1) { depth[j] = depth[t] + 1; q.push(j); f[j][0] = t; for (int k = 1; k = 0; i--) { if (depth[f[x][i]] >= depth[y]) { x = f[x][i]; } } if (x == y) return x; for (int i = 17; i >= 0; i--) { if (f[x][i] != f[y][i]) { x = f[x][i]; y = f[y][i]; } } return f[x][0]; } int dfs(int x, int fa) { LL res = d[x]; for (int i = h[x]; ~i; i = ne[i]) { int j = e[i]; if (j != fa) { res += dfs(j, x); } } if (x == 1) return 0; if (res == 0) ans += m; else if (res == 1) ans += 1; return res; } int main() { cin >> n >> m; memset(h, -1, sizeof h); for (int i = 1; i > x >> y; add(x, y), add(y, x); } bfs(); for (int i = 1; i > x >> y; d[x]++, d[y]++, d[lca(x, y)] -= 2; } dfs(1, -1); cout 强连通分量 有向图 时间复杂度线性 强连通分量一定有环，但是不一定就是一个环 环一定是强连通分量 /* 1. 加时间戳； 2. 放入栈中，做好标记； 3. 遍历邻点 1）如果没遍历过，tarjan一遍，用low[j]更新最小值low 2) 如果在栈中，用dfn[j]更新最小值low 4.找到最高点 1）scc个数++ 2）do-while循环： 从栈中取出每个元素；标志为出栈； 对元素做好属于哪个scc；该scc中点的数量++ */ // tarjan 算法求强连通分量 // 时间复杂度O(n+ m) void tarjan(int u){ // 初始化自己的时间戳 dfn[u] = low[u] = ++ timestamp; //将该点放入栈中 stk[++ top] = u, in_stk[u] = true; // 遍历和u连通的点 for(int i = h[u]; ~i; i = ne[i]){ int j = e[i]; if(!dfn[j]){ tarjan(j); // 更新u所能遍历到的时间戳的最小值 low[u] = min(low[u], low[j]); } // 如果当前点在栈中 //注意栈中存的可能是树中几个不同分支的点,因为有横叉边存在 // 栈中存的所有点，是还没搜完的点，同时都不是强连通分量的最高点 // 这里表示当前强连通分量还没有遍历完，即栈中有值 else if(in_stk[j]) //更新一下u点所能到的最小的时间戳 //此时j要么是u的祖先，要么是横叉边的点，时间戳小于u low[u] = min(low[u], dfn[j]); } // 找到该连通分量的最高点 if(dfn[u] == low[u]){ int y; ++ scc_cnt; // 强连通分量的个数++ do{// 取出来该连通分量的所有点 y = stk[top --]; in_stk[y] = false; id[y] = scc_cnt; // 标记点属于哪个连通分量 size_scc[scc_cnt] ++; } while(y != u); } } #include using namespace std; const int N = 1e4 + 100, M = 5e4 + 100; int h[N], e[M], ne[M], idx, n, m; int timestamp, dfn[N], low[N], scc_size[N], id[N], scc_cnt, dout[N]; bool in_stk[N]; stack stk; void add(int x, int y) { e[idx] = y, ne[idx] = h[x], h[x] = idx++; } void tarjan(int u) { dfn[u] = low[u] = ++timestamp; stk.push(u), in_stk[u] = true; for (int i = h[u]; ~i; i = ne[i]) { int j = e[i]; if (!dfn[j]) { tarjan(j); low[u] = min(low[j], low[u]); } else if (in_stk[j]) { low[u] = min(low[u], dfn[j]); } } if (dfn[u] == low[u]) { int y; scc_cnt++; do { y = stk.top(); stk.pop(); in_stk[y] = false; id[y] = scc_cnt; scc_size[scc_cnt]++; } while (y != u); } } int main() { memset(h, -1, sizeof h); cin >> n >> m; for (int i = 1; i > x >> y; add(x, y); } for (int i = 1; i 缩点 #include using namespace std; const int N = 1e4 + 100, M = 2e5 + 100;//记得边数翻两倍 int h[N], e[M], ne[M], idx, n, m; int hh[N]; int dfn[N], low[N], in_stk[N], timestamp, scc_cnt, scc_value[N], a[N], f[N], id[N]; stack s; void add(int h[], int x, int y) { e[idx] = y, ne[idx] = h[x], h[x] = idx++; } void tarjan(int u) { dfn[u] = low[u] = ++timestamp; s.push(u); in_stk[u] = 1; for (int i = h[u]; ~i; i = ne[i]) { int j = e[i]; if (!dfn[j]) { tarjan(j); low[u] = min(low[u], low[j]); } else if (in_stk[j]) { low[u] = min(low[u], dfn[j]); } } if (dfn[u] == low[u]) { int y; scc_cnt++; do { y = s.top(); s.pop(); in_stk[y] = 0; id[y] = scc_cnt; scc_value[scc_cnt] += a[y]; } while (y != u); } } int main() { memset(h, -1, sizeof h); memset(hh, -1, sizeof hh); cin >> n >> m; for (int i = 1; i > a[i]; for (int i = 1; i > x >> y; add(h, x, y); } for (int i = 1; i = 1; i--) { for (int j = hh[i]; ~j; j = ne[j]) { int k = e[j]; f[k] = max(f[k], f[i] + scc_value[k]); } } int ans = -0x3f3f3f3f; for (int i = 1; i 双联通分量 无向图 割边（桥） 边的双连通分量 ： 极大的不包含桥的连通块 边的双连通分量 任何两个点之间至少存在两个不相交路径 x和y之间是桥 low[y] >dfn[x] y无论如何往上走不到x 将一个图变为边的双联通分量最少要添加几条边：缩点后，（度为1的点的个数+1）/2向下取整 void tarjan(int u, int from) { dfn[u] = low[u] = ++ timestamp; stk[ ++ top] = u; for (int i = h[u]; i!=-1; i = ne[i]) { int j = e[i]; if (!dfn[j])//j未遍历过 { tarjan(j, i);//dfs(j) low[u] = min(low[u], low[j]);//用j更新u if (dfn[u] #include using namespace std; const int N = 5010, M = 20010; int h[N], e[M], ne[M], idx, n, m; int dfn[N], low[N], id[N], dcc_cnt, timestamp; stack stk; bool is_bridge[M]; int d[N]; void add(int x, int y) { e[idx] = y, ne[idx] = h[x], h[x] = idx++; } void tarjan(int u, int from)//from表示从哪个边来的 { dfn[u] = low[u] = ++timestamp; stk.push(u); for (int i = h[u]; ~i; i = ne[i]) { int j = e[i]; if (!dfn[j]) { tarjan(j, i); low[u] = min(low[u], low[j]); if (low[j] > dfn[u]) is_bridge[i] = is_bridge[i ^ 1] = true;//j无论如何也走不到u，说明只有一条路径，是割边（桥） } else if (i != (from ^ 1))//肯定不能往回更新，相当于只能往下 { low[u] = min(low[u], dfn[j]); } } if (low[u] == dfn[u]) { int y; dcc_cnt++; do { y = stk.top(); stk.pop(); id[y] = dcc_cnt; } while (y != u); } } int main() { cin >> n >> m; memset(h, -1, sizeof h); for (int i = 1; i > x >> y; add(x, y), add(y, x); } for (int i = 1; i 二分图 判断 二分图存在的充分必要条件是没有奇数边的环，二分图一般指的都是无向图 时间复杂度：$O(n+m)$ 棋盘问题，很多都是二分图，$(i+j)$为偶数染成1，为奇数染成2 #include using namespace std; const int N = 2e5 + 10; int h[N], e[N], ne[N], col[N], idx, n, m, x, y;//col中，0表示未染色，1,2表示两种颜色 void add(int x, int y) { e[idx] = y; ne[idx] = h[x]; h[x] = idx++; } bool dfs(int x, int q)//x表示正在搜索哪个点，q表示要染色的颜色 { col[x] = q;//染色 for (int i = h[x]; i != -1; i = ne[i]) { int j = e[i]; if (col[j] == 0)//如果这个点没被染过色的话 { if (dfs(j, 3 - q) == false) return false;//对这个点染色失败了，就可以返回false }//3-q可以让1和2相互转化 else if (col[j] == q) return false;//这个点已经被染过了，并且颜色和父节点相同 } return true;//不矛盾，染色成功 } int main() { memset(h, -1, sizeof h); cin >> n >> m; while (m--) { scanf(\"%d%d\", &x, &y); add(x, y), add(y, x); } int flag = 0; for (int i = 1; i 最大匹配 增广路径，从一个非匹配点出发，依次走非匹配边、匹配边、非匹配边、匹配边，最后走到一份非匹配边的算法 二分图的最大匹配等价于不存在增广路径。 实际复杂的$O(nm)$,但是实际上复杂度远小于$O(nm)$ #include using namespace std; const int N = 1e5 + 10;//match放的是女朋友对应的男朋友是谁，bj是为了防止男生重复匹配一个人 int h[N], e[N], ne[N], idx, bj[N], match[N], n1, n2, m, x, y, ans; void add(int x, int y) { e[idx] = y, ne[idx] = h[x], h[x] = idx++; } bool find(int x) { for (int i = h[x]; i != -1; i = ne[i])//遍历与这名男生相联系的女生 { int j = e[i]; if (bj[j] == 1) continue;//如果这个女生已经匹配过了，就不在匹配了 bj[j] = 1; if (match[j] == 0 || find(match[j]) == true)//如果女生没有对象，或者女生的对象还可以匹配另一个女生 { match[j] = x;//以这位女生建立关系 return true;//成功匹配，返回true } } return false;//没有找到对象，返回false } int main() { cin >> n1 >> n2 >> m; memset(h, -1, sizeof h); while (m--) { scanf(\"%d%d\", &x, &y); add(x, y); } for (int i = 1; i 二分图中： 最小点覆盖定义： 二分图中，选出最少的点，使每一条边的两个端点，至少有一个被选中。 最大独立集：二分图中，选出最多的点，使得选出的点之间没有边 最少路径覆盖：用最少的互不相交的路径，将所有点都覆盖住。 最少路径重复的覆盖： 欧拉路径（回路） 欧拉路径：是否存在一种路径，可以使所有的边恰好走一次； 欧拉回路：是否存在一种回路，是所有边恰好走一次并且回到原点 欧拉路径也就是一笔画问题，起点的度必然是奇数，终点的度也是奇数，其他点的度必须是偶数 存在欧拉路径和欧拉回路的前提是所有边联通！！！ 无向图存在欧拉路径的充分必要条件：度数为奇数的点只能为2个或者0个（起点=终点）； 无向图存在欧拉回路的充分必要条件：度数为奇数的点只能为0个； 有向图存在欧拉路径的充分必要条件：要么所有点的入度都等于出度（起点=终点）；要么除了两个点外其余的所有点，入度等于出度，这两个点，一个出度比入度多1（起点），另一个入度比出度多1（终点）； 有向图存在欧拉回路的充分必要条件：所有点的入度都等于出度； 数论 质数定理：1~n中质数的个数为$n/ln(n)$ $1~n$中能被$x$整除的个数为$\\lfloor$n/x$\\rfloor$$下取整 质数 试除法判定质数 #include using namespace std; const int N = 1e6; bool f(int x) { if (x > n; for (int i = 1; i > x; if (f(x)) cout 埃氏筛 bool st[N]; int primes[N], cnt; void init(int n) { st[0] = st[1] = true; for (int i = 2; i 线性筛法 #include #define endl \"\\n\" const int N = 1e8 + 10, M = N, mod = 1e9 + 7; using namespace std; int primes[N], cnt; bool st[N]; void init(int n){ st[0] = st[1] = true; for(int i = 2; i > n >> q; init(n); while(q --){ int k; cin >> k; cout 质因数和约数 质因数 #include using namespace std; void f(int n) { for (int i = 2; i 1) cout > n; for (int i = 1; i > x; f(x); cout 约数 试除法 #include using namespace std; void f(int x) { vector v; for (int i = 1; i > n; for (int i = 1; i > x; f(x); } return 0; } 求约数个数 $N=p_1^{\\alpha_1}p_1^{\\alpha_2}\\cdots*p_1^{\\alpha_k}$ $cnt=(\\alpha_1+1)(\\alpha_2+1)\\cdots*(\\alpha_k+1)$ #include typedef long long LL; using namespace std; const int N = 1e6, mod = 1e9 + 7; map mp; void solve(int n) { for (int i = 2; i 1) mp[n]++; } int main() { int T; cin >> T; while (T--) { int n; cin >> n; solve(n); } LL sum = 1; for (auto it: mp) { sum = sum * (it.second + 1) % mod; } cout 约数之和 $N=p_1^{\\alpha_1}p_1^{\\alpha_2}\\cdots*p_1^{\\alpha_k}$ $sum=(p_1^0+p_1^1+\\cdots+p_1^{\\alpha_1})\\cdots(p_2^0+p_2^1+\\cdots+p_2^{\\alpha_2})\\cdots(p_k^0+p_k^1+\\cdots +p_k^{\\alpha_k}) $ 如何求出$p_k^0+p_k^1+\\cdots +p_k^{\\alpha_k}$,当成$p$进制，就可以，每一位都是$1*权重$ #include using namespace std; typedef long long LL; const int N = 1e6, mod = 1e9 + 7; map mp; void solve(int n) { for (int i = 2; i 1) mp[n]++; } int main() { int T; cin >> T; while (T--) { int n; cin >> n; solve(n); } LL sum = 1; for (auto it: mp) { LL x = 0; for (int i = it.second; i >= 0; i--)//类似于进制转化 { x = (x * it.first + 1) % mod; } sum = sum * x % mod; } cout 最大公约数 欧几里得算法: $gcd(a,b)=gcd(b,a%b)$ 性质：$ d|a$，$d|b$则 $d|（a+b）$ d能整除a,d能整除b，则d能整除（a+b）很好证明，直接设就行 int gcd(int a, int b) { return b ? gcd(b, a % b) : a; } 扩展欧几里得：求$ax+by=gcd(a,b)$的一组解 通解$x=x_0-b/d*k$ $k$是整数 ​ $y=y_0+a/d*k$ x最小非负数是多少？另$t=b/d$ 则 $x=x_0-t*k$ $x_{min}=(x_0\\%t+t)\\%t$ 如何求$ax+by=d$的解？ 有解的充分必要条件是$gcd(a,b)|d$ 扩展欧几里得求出$ax+by=gcd(a,b)$的一组解，乘上系数即可。 #include #define CLOSE ios::sync_with_stdio(false);cin.tie(0);cout.tie(0) using namespace std; typedef long long ll; typedef pair PII; const int N = 1e6; int exgcd(int a, int b, int &x, int &y) { if (b == 0) { x = 1, y = 0; return a; } int d = exgcd(b, a % b, y, x);//交换一下顺序 y = y - a / b * x; return d; } int main() { int n; scanf(\"%d\", &n); while (n--) { int a, b, x, y; scanf(\"%d%d\", &a, &b); int d = exgcd(a, b, x, y); printf(\"%d %d\\n\", x, y); } return 0; } 欧拉函数 $\\phi(n)$为$1$~$n$直接中与$n$互质数的个数 快速幂 快速幂求逆元，要求逆元存在（a和p互质），并且p是质数。 LL qpow(LL a, LL b, LL p) { LL ans = 1; while (b) { if (b & 1) ans = ans * a % p; b >>= 1; a = a * a % p;//这里别忘了mod } return ans; } 矩阵快速幂 #include #define CLOSE ios::sync_with_stdio(false);cin.tie(0);cout.tie(0) #define endl \"\\n\" typedef long long LL; const int N = 1e5 + 10, M = N, mod = 1e9 + 7; using namespace std; struct Mat{ LL a[105][105]; int r, c; Mat(int _r, int _c){ r = _r, c = _c; memset(a, 0, sizeof a); } //单位矩阵 void unit(){ memset(a, 0, sizeof a); for(int i = 1; i >= 1; } return ans; } void output(){ for(int i = 1; i > n; Mat a(2, 2), temp(1, 2); temp.a[1][1] = temp.a[1][2] = 1, a.a[1][1] = 0, a.a[1][2] = a.a[2][1] = a.a[2][2] = 1; Mat ans = temp * a.pow(n - 2); cout 龟速乘 和快速幂思想一样，快速幂是把乘法变成乘法，龟速乘是把乘法变成加法，速度变慢了，但是不会爆long long LL qadd(LL a, LL b, LL p) { LL ans = 0; while (b) { if (b & 1) ans = (ans + a) % p; b >>= 1; a = (a + a) % p; } return ans; } 组合数 $C_n^m$ $=$ $n(n-1)(n-2)\\cdot\\cdot\\cdot(n-m+1) \\over m(m-1)\\cdot\\cdot*1$ $C_a^b=$$a!\\over b!*(a-b)!$ C[a,b]=C[a-1,b]+C[a-1,b-1]; 预处理时间复杂度：$O(N^2)$ const int N = 2005, mod = 1e9 + 7; int c[N][N]; void init() { for (int i = 0; i 预处理时间复杂度$O(N*log(N))$ #include using namespace std; typedef long long LL; const LL N = 1e5 + 8, mod = 1e9 + 7; LL fact[N], infact[N]; LL qmi(LL a, LL k, LL p) { LL ans = 1; while (k != 0) { if (k & 1) ans = ans * a % p; k = k >> 1; a = a * a % p; } return ans; } void init() { fact[0] = infact[0] = 1;//0的阶乘为1，除0的阶乘也为1 for (int i = 1; i > n; init(); while (n--) { scanf(\"%lld%lld\", &a, &b); printf(\"%lld\\n\", fact[a] * infact[b] % mod * infact[a - b] % mod);//要及时取模 } } $1 lucas #include using namespace std; typedef long long LL; int n; LL qmi(LL a, LL k, LL p) { LL ans = 1; while (k) { if (k & 1) ans = ans * a % p; k = k >> 1; a = a * a % p; } return ans; } LL p; LL c(LL a, LL b) { if (a > n; while (n--) { LL a, b; cin >> a >> b >> p; cout 容斥原理 时间复杂度$O(2^n)$ 分析，有$C_n^1+C_n^3+C_n^2+\\cdot\\cdot\\cdot+C_n^n$，可以补上一个$C_n^0$，所以有$2^n-1$项 /*容斥原理的应用 能被p1,p2,…,pm 中的至少一个数整除的整数数量 就是p1∪p2∪p3---∪pm，可以由容斥原理算出； [1,n]中能被x整除的数有[n/x]个； 其中因为p是素数，所以既能被p1整除又能被p2整除的话， 就相当于被p1*p2整除，如何枚举呢？ 采用二进制枚举法，需要枚举m个位置，所以从1枚举到pow(2,m-1);不能是从0开始枚举，因为 全是0表示都没选，不符合题意 */ #include using namespace std; typedef long long LL; const int N = 20; int a[N]; int n, m; int main() { cin >> n >> m; for (int i = 0; i > a[i]; } LL ans = 0; for (int i = 1; i n,这样不合法,但是最终答案是对的； for (int j = 0; j > j & 1) { sum++; if (t * a[j] > n) { flag = 1; break; } t = t * a[j]; } } if (flag == 0) { if (sum % 2 == 1) ans += n / t; else ans -= n / t; } } cout DP 背包问题 01背包 #include using namespace std; const int N = 1010; int n, m; int dp[N][N], v[N], w[N]; int main() { cin >> n >> m; for (int i = 1; i > v[i] >> w[i];//输入物品的体积和价值 } for (int i = 1; i 空间优化 #include using namespace std; const int N = 1010; int n, m; int dp[N], v[N], w[N]; int main(){ cin >> n >> m; for (int i = 1; i > v[i] >> w[i]; for (int i = 1; i = v[i]; j --){//从大到小枚举，注意j >= v[i]，并且是j -- dp[j] = max(dp[j], dp[j - v[i]] + w[i]); } } cout 求方案数 #include #define CLOSE ios::sync_with_stdio(false);cin.tie(0);cout.tie(0) using namespace std; typedef long long ll; typedef pairPII; const int N=110; int f[N][10010];//从前i个里面选，恰好可以表示出来j int a[N]; int main() { int n,m; cin>>n>>m; for(int i=1;i>a[i]; f[0][0]=1; for(int i=1;ij) f[i][j]=f[i-1][j]; else f[i][j]=f[i-1][j]+f[i-1][j-a[i]]; } } cout 完全背包 时间复杂度$O(n^2)$ #include using namespace std; const int N = 1010; int n, m; int dp[N][N], v[N], w[N]; int main() { cin >> n >> m; for (int i = 1; i > v[i] >> w[i];//输入物品的体积和价值 } for (int i = 1; i 优化版本 #include using namespace std; const int N = 1010; int n, m; int dp[N], v[N], w[N]; int main(){ cin >> n >> m; for (int i = 1; i > v[i] >> w[i]; } for (int i = 1; i 求方案数 #include using namespace std; const int N = 1005; int f[2][N], w[N], v[N], n, m; int main() { cin >> n >> m; for (int i = 1; i > v[i] >> w[i]; for (int i = 1; i 多重背包 暴力写法时间复杂度$O(n^3)$ #include using namespace std; const int N = 110; int n, m; int v[N], w[N], s[N], f[N][N]; int main() { cin >> n >> m; for (int i = 1; i > v[i] >> w[i] >> s[i]; } for (int i = 1; i 二进制优化写法 时间复杂度$O(n^2*log(s))$ 建议数组开$10*n$ #include using namespace std; const int N = 12010, M = 2010;//建议N = 10 * n int n, m, cnt;//cnt是打包后的物品数量 int v[N], w[N], f[M]; int main() { cin >> n >> m; for (int i = 1; i > a >> b >> s; int k = 1; while (k 0){ cnt ++; v[cnt] = a * s; w[cnt] = b * s; } } n = cnt;//新物品的数量是cnt //这样就做01背包就行 for (int i = 1; i = v[i]; j -- ){ f[j] = max(f[j], f[j - v[i]] + w[i]); } } cout using namespace std; const int N = 2010; int n, m, f[N]; int main() { cin >> n >> m; for(int i = 1; i> a >> b >> s; //开始打包 for(int k = 1; k = k * a; j --){ f[j] = max(f[j], f[j - k * a] + k * b); } s -= k; } //剩下的就自己打包成一份 if(s){ for(int j = m; j >= s * a; j --){ f[j] = max(f[j], f[j - s * a] + s * b); } } } cout 分组背包 时间复杂度$O(n^3)$ #include using namespace std; const int N = 110; int n, m; int v[N][N], w[N][N], s[N]; int f[N][N]; int main() { cin >> n >> m; for (int i = 1; i > s[i]; for (int j = 0; j > v[i][j] >> w[i][j]; } } for (int i = 1; i = 0; j -- ){ f[i][j] = f[i - 1][j];//第i组一个都不选 for (int k = 0; k 优化空间 #include using namespace std; const int N = 110; int n, m; int v[N][N], w[N][N], s[N]; int f[N]; int main() { cin >> n >> m; for (int i = 1; i > s[i]; for (int j = 0; j > v[i][j] >> w[i][j]; } } for (int i = 1; i = 0; j -- ){ for (int k = 0; k 混合背包 混合背包就是将前面三种的背包问题混合起来，有的只能取一次，有的能取无限次，有的只能取$k$次。 先将01背包和完全背包转化成多重背包。 01背包，物品数量为1的多重背包， 完全背包，物品数量为$V_{背包体积}/v_i$的多重背包，虽然物品数量是无限的，但是背包容量是有限的。 再按照多重背包来做即可。 #include using namespace std; const int N = 1005 * 20; int f[N], w[N], v[N], n, m; int main() { int cnt = 0; cin >> n >> m; for (int i = 1; i > a >> b >> s; if (s == 0) s = m / a;//如果是完全背包，则看成多重背包做 if (s == -1)//01背包 { cnt ++; v[cnt] = a, w[cnt] = b; } else//多重背包二进制拆分 { int k = 1; while (k = v[i]; j --) f[j] = max(f[j], f[j - v[i]] + w[i]); cout 思路和代码2 01背包当成数量为1的多重背包，然后进行二进制优化。 完全背包的求法与01背包不同，所以单独算完全背包部分。 #include using namespace std; const int N = 1010; int n, m; int f[N]; int main() { cin >> n >> m; for (int i = 0; i > v >> w >> s; if (!s)//完全背包 { for (int j = v; j = k * v; j --) f[j] = max(f[j], f[j - k * v] + k * w); s -= k; } if (s) { for (int j = m; j >= s * v; j --) f[j] = max(f[j], f[j - s * v] + s * w); } } } cout 二维费用背包 #include using namespace std; const int N = 1010, K = 110; int n, V, M; int v[N], m[N], w[N]; int f[K][K]; int main() { cin >> n >> V >> M; for (int i = 1; i > v[i] >> m[i] >> w[i]; for (int i = 1; i = v[i]; -- j) { for (int k = M; k >= m[i]; -- k) { f[j][k] = max(f[j][k], f[j - v[i]][k - m[i]] + w[i]); } } } cout 区间dp 按照区间长度递增枚举，才可以dp #include using namespace std; const int N = 505; int s[N], f[N][N]; int main() { int n; cin >> n; for (int i = 1; i > x; s[i] = s[i - 1] + x; } //长度为1都是0 for (int len = 2; len 数位dp 数位DP笔记(DFS做法) - AcWing #include using namespace std; const int N = 1e6; int a[30]; int f[30][15]; int dfs(int pos, int pre, int limit, int lead) { if (!pos) return 1;//递归终点 else if (!limit && !lead && f[pos][pre] != -1) return f[pos][pre];//记忆化，只有无限制、无前导零才算，不然都是未搜索完的情况。 int res = 0, up = limit ? a[pos] : 9; for (int i = 0; i > a >> b; cout 状态压缩DP 一般要用long long #include using namespace std; typedef long long ll; typedef pair PII; const int N = 12; ll n, s, k, f[N][105][1 state;//合法的状态 vector h[1 > i) & 1 && (x >> (i + 1)) & 1)//相邻的1 { return false; } } return true; } int count(int x) { int res = 0; for (int i = 0; i > i) & 1; return res; } int main() { cin >> n >> k; for (int i = 0; i j) f[i][j][b] = f[i - 1][j][b];//类似于01背包，当前状态选不选 else f[i][j][b] += f[i - 1][j - cnt][a]; } } } } cout 博弈论 先手必胜状态：存在一种方式让对手变成先手必败状态。 先手必败状态：无论采取哪种方式，留给对手的都是先手必胜状态。 Nim游戏 先手必胜：$a_1\\bigoplus a_2 \\bigoplus \\cdots \\bigoplus a_n \\neq 0$ 先手必败：$a_1\\bigoplus a_2 \\bigoplus \\cdots \\bigoplus a_n=0$ 证明：https://www.acwing.com/solution/content/14269/ #include using namespace std; int main() { int n; cin >> n; int ans = 0; for (int i = 1; i > x; ans = ans ^ x; } if (ans) cout 台阶-Nim游戏 先手必胜：$a1\\bigoplus a_3 \\bigoplus \\cdots \\bigoplus a{2n+1} \\neq 0$ 先手必败：$a2\\bigoplus a_4 \\bigoplus \\cdots \\bigoplus a{2n}=0$ 证明：和NIm游戏类似，若奇数台阶异或不为0，我一定可以操作让奇数台阶异或变为0，之后若对手拿奇数台阶，呢么留给我的必然是异或不为0，若对手拿偶数台阶，我把他往下拿的，在往下拿，这样局面不改变。一直进行下去，必然是对手先遇到奇数台阶为0，之后对手怎么拿石子下去，我把他的石子往下拿，这样我必胜。 SG函数 $mex$运算：找到一个集合里面不存在的最小的自然数，比如$mex{1,2,3}=0$ $sg(x)=k$,则由定义地$x$可以走到的点的$sg$一定包含$[0,k-1]$里面的任何数 $sg$函数性质：多个独立局面的$sg$值，等于这些局面$sg$值的异或和 $sg=0$是先手必败状态，它只能走下$sg$不为0的状态。 $sg$不等于0，是先手必胜状态，它可以走向一个$sg$为0（必败状态） #include using namespace std; int a[105], f[10005]; int k, n; int sg(int x) { if (~f[x]) return f[x]; unordered_set s; for (int i = 1; i = 0) s.insert(sg(x - a[i])); } for (int i = 0;; i++) { if (!s.count(i)) return f[x] = i; } } int main() { memset(f, -1, sizeof f); cin >> k; for (int i = 1; i > a[i]; cin >> n; int res = 0; for (int i = 1; i > x; res ^= sg(x); } if (res) cout Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-02-22 18:26:10 "},"Chapter4/算法题.html":{"url":"Chapter4/算法题.html","title":"面试算法题","keywords":"","body":"实现两个线程进行打排球 思路： 通过使用wait()和notify()进行控制顺序或者通过使用信号量Semapore类 import java.util.Random; import java.util.concurrent.Semaphore; public class Main { public static void main(String[] args) { System.out.println(\"Hello World!\"); MyRun t = new MyRun(); Thread t1 = new Thread(t, \"1\"); Thread t2 = new Thread(t, \"2\"); t1.start(); t2.start(); try { t1.join(); t2.join(); } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } System.out.println(\"main thread exit\"); } static class MyRun implements Runnable{ static volatile int cnt = 0; public Object lock = new Object(); public final Semaphore sem = new Semaphore(1); public final Semaphore sem2 = new Semaphore(0); public void run() { while(true) { synchronized(lock) { if(cnt == -1) break; Random r = new Random(); int n = r.nextInt(100); String name = Thread.currentThread().getName(); if(n % 13 == 0) { System.out.println(name + \"won!!!!\"); cnt = -1; lock.notify(); break; } cnt ++; System.out.println(name + \" : \" + cnt); try{ lock.notify(); lock.wait(); } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } } } } // public void run() { // while(true) { // try{ // String name = Thread.currentThread().getName(); // if(\"1\".equals(name)) // sem.acquire(); // else // sem2.acquire(); // if(cnt == -1) // break; // Random r = new Random(); // int n = r.nextInt(100); // if(n % 13 == 0) { // System.out.println(name + \"miss!!!!\"); // cnt = -1; // sem.release(); // sem2.release(); // break; // } // cnt ++; // System.out.println(name + \" : \" + cnt); // if(\"1\".equals(name)) // sem2.release(); // else // sem.release(); // } catch (Exception e) { // e.printStackTrace(); // } // } // } } } LRU 缓存实现 思路：通过双向链表，实现movenode 和 addtohead两个函数，对双向链表进行操作，利用map记录key对应的node /** * Your LRUCache object will be instantiated and called as such: * LRUCache* obj = new LRUCache(capacity); * int param_1 = obj->get(key); * obj->put(key,value); */ struct LRUNode{ int key, value; LRUNode* pre; LRUNode* nxt; LRUNode():key(0),value(0),pre(nullptr),nxt(nullptr){} LRUNode(int _key, int _value) : key(_key),value(_value),pre(nullptr),nxt(nullptr){} LRUNode(int _key, int _value, LRUNode* _pre, LRUNode* _nxt) { key = _key; value = _value; pre = _pre; nxt = _nxt; } }; class LRUCache { private: int capacity; int size; LRUNode* head; LRUNode* tail; map cache; public: LRUCache(int _capacity) { capacity = _capacity; size = 0; head = new LRUNode(); tail = new LRUNode(); head -> nxt = tail; tail -> pre = head; } int get(int key) { if(!cache.count(key)) { return -1; } LRUNode* cur = cache[key]; moveToHead(cur); return cur -> value; } void put(int key, int value) { if(!cache.count(key)) { LRUNode* node = new LRUNode(key, value); cache[key] = node; addToHead(node); size ++; if(size > capacity) { LRUNode* cur = moveTail(); cache.erase(cur -> key); delete cur; size --; } } else { LRUNode* cur = cache[key]; cur -> value = value; moveToHead(cur); } } LRUNode* moveTail() { LRUNode* cur = tail -> pre; removeNode(cur); return cur; } void moveToHead(LRUNode* node){ removeNode(node); addToHead(node); } void removeNode(LRUNode* node) { node -> pre -> nxt = node -> nxt; node -> nxt -> pre = node -> pre; } void addToHead(LRUNode* node) { node -> pre = head; node -> nxt = head -> nxt; head -> nxt -> pre = node; head -> nxt = node; } }; 二叉树的前序、中序、后序 class Solution { public: vector preorderTraversal(TreeNode* root) { vector ans; if(root == nullptr) return ans; stack s; s.push(root); while(!s.empty()) { TreeNode* t = s.top(); s.pop(); if(t != nullptr) { if(t -> right) s.push(t -> right); if(t -> left) s.push(t -> left); s.push(t); s.push(nullptr); } else { TreeNode* t = s.top(); s.pop(); ans.emplace_back(t -> val); } } return ans; } }; 输入n个整数，进行加减组合，能够组成计算结果为s的方案数 #include #include #include using namespace std; int main(){ int n, s; int a[i]; int dp[2][2000]; cin >> n; int tot = 0; for(int i = 0; i > a[i]; tot += (a[i] >= 0 ? a[i] : -a[i]); } cout = a[i]) { dp[t][j] = dp[k][j - a[i]]; } if(j + a[i] # 题目，给你一堆信封大小，问你最多能相互套多少个，只有第二的信封长和宽都大于前一个才能套进去。 排序加单调队列 #include #include #include using namespace std; vector> q; struct cmp{ bool operator()(const vector& a, const vector& b)const { if(a[0] == b[0]) return a[1] > b[1]; return a[0] >& envelopes) { int n = envelopes.size(); if(n f = {envelopes[0][1]}; for(int i = 1; i f.back()) { f.emplace_back(num); } else { int l = 0, r = f.size(); while(l > 1; if(f[mid] >= num) { r = mid; } else { l = mid + 1; } } cout > n; for(int i = 0; i > x >> y; q.push_back({x, y}); } ans = maxEnvelopes(q); cout 给你一个单向链表，获得倒数第n个值 思路：通过快慢指针进行计算 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-03-20 21:57:37 "},"Chapter4/面试题.html":{"url":"Chapter4/面试题.html","title":"面试题","keywords":"","body":"11. java创建对象有哪些方式？ 1. 使用new关键字 2. 反序列化对象数据 3. 反射机制：Class类的forName方法的newInstance() 4. 反射机制: 对应类的getConstructor()的newInstance() 5. 通过clone()函数复制，必须实现Cloneable接口 4. 获取 Class 对象的四种方式 1. 直接该类名称的.class 2. 通过Class.forName传入类的全路径获取 3. 通过对象实例getClass函数获取 4. 通过类加载器ClassLoader.loadClass()传入类的路径获取 3. 动态代理 就是给已经开发好的实体类增加功能的手段，不改变原有的处理逻辑 jdk动态代理类使用步骤 1.定义一个接口及其实现类； 2.自定义 InvocationHandler 并重写invoke方法，在 invoke 方法中我们会调用原生方法（被代理类的方法）并自定义一些处理逻辑； 3.通过 Proxy.newProxyInstance(ClassLoader loader,Class[] interfaces,InvocationHandler h) 方法创建代理对象； JDK 动态代理有一个最致命的问题是其只能代理实现了接口的类。 GCLIB动态代理类使用步骤 1.定义一个类 2.自定义MethodIntercepter并重写intercept方法， 用于拦截增强被代理类的方法，和JDK动态代理的invoke方法类似 3. 通过Enhancer类的create创建代理类 写一个线程安全的懒加载单例 import java.util.Scanner; class Main{ private volatile static Main instance; public Main() { } public static Main getInstance() { if(instance == null) { synchronized (Main.class) { if(instance == null) { instance = new Main(); } } } return instance; } } 2. I/O 流为什么要分为字节流和字符流呢? 防止转换成字节流进行信息交互，出现乱码问题 为什么ArrayList不是线程安全的，具体来说是哪里不安全？ 不是 arraylist大体分三部分： 需不需要扩容 size位置设置值 将当前集合的大小加1 部分值为null 索引越界异常 size与add数量不一致 2. 比较 HashSet、LinkedHashSet 和 TreeSet 三者的异同 线程不安全，都是实现set接口 Hashset是哈希表 LinkedHashset是哈希表+链表，保证了插入和取出顺序满足FIFO TreeSet是红黑树 1. HashMap 和 Hashtable 的区别 线程安全：HashMap不安全，HashTable安全 效率：HashMap效率会高 对null key和nullvalue 支持：HashMap支持，HashTable不支持 初始容量和每次扩容容量大小不同：① 创建时如果不指定容量初始值，Hashtable 默认的初始大小为 11，之后每次扩充，容量变为原来的 2n+1。HashMap 默认的初始化大小为 16。之后每次扩充，容量变为原来的 2 倍。② 创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 HashMap 会将其扩充为 2 的幂次方大小。也就是说 HashMap 总是使用 2 的幂作为哈希表的大小,后面会介绍到为什么是 2 的幂次方。 底层结构：JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）时，将链表转化为红黑树（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树），以减少搜索时间（后文中我会结合源码对这一过程进行分析）。Hashtable 没有这样的机制。 哈希函数的实现：HashMap 对哈希值进行了高位和低位的混合扰动处理以减少冲突，而 Hashtable 直接使用键的 hashCode() 值。 HashMap 的底层实现 HashMap 的长度为什么是 2 的幂次方 位运算效率更高：位运算(&)比取余运算(%)更高效。当长度为 2 的幂次方时，hash % length 等价于 hash & (length - 1)。 可以更好地保证哈希值的均匀分布：扩容之后，在旧数组元素 hash 值比较均匀的情况下，新数组元素也会被分配的比较均匀，最好的情况是会有一半在新数组的前半部分，一半在新数组后半部分。 扩容机制变得简单和高效：扩容后只需检查哈希值高位的变化来决定元素的新位置，要么位置不变（高位为 0），要么就是移动到新位置（高位为 1，原索引位置+原容量）。 位运算效率更高，数据分布更均匀，扩容机制变得简单 列举HashMap在多线程下可能会出现的问题？ 尾插法：会出现数据覆盖 头插法：会在扩容机制中出现死循环，扩容后需要重新分配位置 6. 列举HashMap在多线程下可能会出现的问题？ 7. ConcurrentHashMap 和 Hashtable 的区别 底层数据结构：分段数组和链表。jdk8之后是数组和链表/红黑树；hashtable是数组和链表 实现线程安全的方式：ConcurrentHashMap直接对node加锁 cas； hashtable是整个一把锁，效率低下，使用 put 添加元素，另一个线程不能使用 put 添加元素，也不能使用 get，竞争会越来越激烈效率越低。 ConcurrentHashMap已经用了synchronized，为什么还要用CAS（乐观锁）呢 为什么HashMap要用红黑树而不是平衡二叉树？ 平衡二叉树追求的是一种 “完全平衡” 状态：任何结点的左右子树的高度差不会超过 1，优势是树的结点是很平均分配的。这个要求实在是太严了，导致每次进行插入/删除节点的时候，几乎都会破坏平衡树的第二个规则，进而我们都需要通过左旋和右旋来进行调整，使之再次成为一颗符合要求的平衡树。 红黑树不追求这种完全平衡状态，而是追求一种 “弱平衡” 状态：整个树最长路径不会超过最短路径的 2 倍。优势是虽然牺牲了一部分查找的性能效率，但是能够换取一部分维持树平衡状态的成本。与平衡树不同的是，红黑树在插入、删除等操作，不会像平衡树那样，频繁着破坏红黑树的规则，所以不需要频繁着调整，这也是我们为什么大多数情况下使用红黑树的原因。 HashMap的扩容机制介绍一下 hashMap默认的负载因子是0.75，即如果hashmap中的元素个数超过了总容量75%，则会触发扩容，扩容分为两个步骤： 第1步是对哈希表长度的扩展（2倍） 第2步是将旧哈希表中的数据放到新的哈希表中。 只需要看原来的hash值新增的那个bit是0还是1，0是不动，1是移动原始容量距离 请简要描述线程与进程的关系,区别及优缺点？ 线程是进程划分成的更小的运行单位,一个进程在其执行的过程中可以产生多个线程。 线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。 线程执行开销小，但不利于资源的管理和保护；而进程正相反。 有了进程为什么还需要线程? 进程切换是一个开销很大的操作，线程切换的成本较低。 线程更轻量，一个进程可以创建多个线程。 多个线程可以并发处理不同的任务，更有效地利用了多处理器和多核计算机。而进程只能在一个时间干一件事，如果在执行过程中遇到阻塞问题比如 IO 阻塞就会挂起直到结果返回。 同一进程内的线程共享内存和文件，因此它们之间相互通信无须调用内核 4. 如何创建线程？ 继承Thread，实现Runnable接口、实现Callable接口，实现线程池 最终都是 new Thread.start() 说说线程的生命周期和状态? 初始 运行 阻塞 等待 超时等待 终止 7. Thread#sleep() 方法和 Object#wait() 方法对比 共同点：两者都可以暂停线程的执行。 区别： sleep() 方法没有释放锁，而 wait() 方法释放了锁 。 wait() 通常被用于线程间交互/通信，sleep()通常被用于暂停执行。 wait() 方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的 notify()或者 notifyAll() 方法。sleep()方法执行完成后，线程会自动苏醒，或者也可以使用 wait(long timeout) 超时后线程会自动苏醒。 sleep() 是 Thread 类的静态本地方法，wait() 则是 Object 类的本地方法。 3. 并发编程三个重要特性 原子性： 可见性：修改是可见的 有序性：volatile关键字可以禁止指令进行重排序优化。 4. volatile 关键字 4. 如何实现乐观锁？ 版本号控制：一般是在数据表中加上一个数据版本号 version 字段，表示数据被修改的次数。当数据被修改时，version 值会加一。当线程 A 要更新数据值时，在读取数据的同时也会读取 version 值，在提交更新时，若刚才读取到的 version 值为当前数据库中的 version 值相等时才更新，否则重试更新操作，直到更新成功 CAS:CAS 涉及到三个操作数： V：要更新的变量值(Var) E：预期值(Expected) N：拟写入的新值(New) 4. synchronized 和 volatile 有什么区别？ 互补 volatile 关键字只能用于变量而 synchronized 关键字可以修饰方法以及代码块 。 volatile 关键字能保证数据的可见性，但不能保证数据的原子性。synchronized 关键字两者都能保证。 volatile关键字主要用于解决变量在多个线程之间的可见性，而 synchronized 关键字解决的是多个线程之间访问资源的同步性。 synchronized 和 ReentrantLock 有什么区别？ 都是可重入锁 synchronized是依赖于JVM，ReentrantLoack依赖于API synchronized是非公平的，ReentrantLoack可以是公平或者非公平 ReentrantLoack可实现等待可中断 可实现选择性通知（锁可以绑定多个条件）: synchronized关键字与wait()和notify()/notifyAll()方法相结合可以实现等待/通知机制。ReentrantLock类当然也可以实现，但是需要借助于Condition接口与newCondition()方法。 支持超时 ：ReentrantLock 提供了 tryLock(timeout) 的方法，可以指定等待获取锁的最长等待时间，如果超过了等待时间，就会获取锁失败，不会一直等待 ThreadLocal 内存泄露问题是怎么导致的？如何避免？ ThreadLocal 实例不再被强引用；key为null 线程持续存活，导致 ThreadLocalMap 长期存在。 如何避免： 在使用完 ThreadLocal 后，务必调用 remove() 方法。 这是最安全和最推荐的做法。 remove() 方法会从 ThreadLocalMap 中显式地移除对应的 entry，彻底解决内存泄漏的风险。 即使将 ThreadLocal 定义为 static final，也强烈建议在每次使用后调用 remove()。 在线程池等线程复用的场景下，使用 try-finally 块可以确保即使发生异常，remove() 方法也一定会被执行。 5. 如何跨线程传递 ThreadLocal 的值？ InheritableThreadLocal ：InheritableThreadLocal 是 JDK1.2 提供的工具，继承自 ThreadLocal 。使用 InheritableThreadLocal 时，会在创建子线程时，令子线程继承父线程中的 ThreadLocal 值，但是无法支持线程池场景下的 ThreadLocal 值传递。 TransmittableThreadLocal ： TransmittableThreadLocal （简称 TTL） 是阿里巴巴开源的工具类，继承并加强了InheritableThreadLocal类，可以在线程池的场景下支持 ThreadLocal 值传递 3. 如何创建线程池？常用方法 start启动线程，线程池：submit对于callable，execute对于runable。 7. 线程池的拒绝策略有哪些？ 12. 线程池中线程异常后，销毁还是复用？ 3. 一个任务需要依赖另外两个任务执行完之后再执行，怎么设计？ 1. 对象的创建过程 内存分配和回收原则 死亡对象判断方法 4. 如何判断一个常量是废弃常量？ 如何判断一个类是无用的类？ 垃圾收集算法 垃圾收集器 类加载过程 JVM 中内置了三个重要的 ClassLoader 类加载器和双亲委派机制。 1. 说说自己对于 Spring MVC 了解? MVC 是模型(Model)、视图(View)、控制器(Controller)的简写，其核心思想是通过将业务逻辑、数据、显示分离来组织代码。 3. 将一个类声明为 Bean 的注解有哪些? @Component：通用的注解，可标注任意类为 Spring 组件。如果一个 Bean 不知道属于哪个层，可以使用@Component 注解标注。 @Repository : 对应持久层即 Dao 层，主要用于数据库相关操作。 @Service : 对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao 层。 @Controller : 对应 Spring MVC 控制层，主要用于接受用户请求并调用 Service 层返回数据给前端页面。 11. Bean 的生命周期了解么? 创建 Bean 的实例：Bean 容器首先会找到配置文件中的 Bean 定义，然后使用 Java 反射 API 来创建 Bean 的实例。 Bean 属性赋值/填充：为 Bean 设置相关属性和依赖，例如@Autowired 等注解注入的对象、@Value 注入的值、setter方法或构造函数注入依赖和值、@Resource注入的各种资源。 Bean 初始化 如果 Bean 实现了 BeanNameAware 接口，调用 setBeanName()方法，传入 Bean 的名字。 如果 Bean 实现了 BeanClassLoaderAware 接口，调用 setBeanClassLoader()方法，传入 ClassLoader对象的实例。 如果 Bean 实现了 BeanFactoryAware 接口，调用 setBeanFactory()方法，传入 BeanFactory对象的实例。 与上面的类似，如果实现了其他 *.Aware接口，就调用相应的方法。 如果有和加载这个 Bean 的 Spring 容器相关的 BeanPostProcessor 对象，执行postProcessBeforeInitialization() 方法 如果 Bean 实现了InitializingBean接口，执行afterPropertiesSet()方法。 如果 Bean 在配置文件中的定义包含 init-method 属性，执行指定的方法。 如果有和加载这个 Bean 的 Spring 容器相关的 BeanPostProcessor 对象，执行postProcessAfterInitialization() 方法。 销毁 Bean ：销毁并不是说要立马把 Bean 给销毁掉，而是把 Bean 的销毁方法先记录下来，将来需要销毁 Bean 或者销毁容器的时候，就调用这些方法去释放 Bean 所持有的资源。 如果 Bean 实现了 DisposableBean 接口，执行 destroy() 方法。 如果 Bean 在配置文件中的定义包含 destroy-method 属性，执行指定的 Bean 销毁方法。或者，也可以直接通过@PreDestroy 注解标记 Bean 销毁之前执行的方法。 整体上可以简单分为四步：实例化 —> 属性赋值 —> 初始化 —> 销毁。 初始化这一步涉及到的步骤比较多，包含 Aware 接口的依赖注入、BeanPostProcessor 在初始化前后的处理以及 InitializingBean 和 init-method 的初始化操作。 销毁这一步会注册相关销毁回调接口，最后通过DisposableBean 和 destory-method 进行销毁。 3. SpringMVC 工作原理了解吗? 客户端（浏览器）发送请求， DispatcherServlet拦截请求。 DispatcherServlet 根据请求信息调用 HandlerMapping 。HandlerMapping 根据 URL 去匹配查找能处理的 Handler（也就是我们平常说的 Controller 控制器） ，并会将请求涉及到的拦截器和 Handler 一起封装。 DispatcherServlet 调用 HandlerAdapter适配器执行 Handler 。 Handler 完成对用户请求的处理后，会返回一个 ModelAndView 对象给DispatcherServlet，ModelAndView 顾名思义，包含了数据模型以及相应的视图的信息。Model 是返回的数据对象，View 是个逻辑上的 View。 ViewResolver 会根据逻辑 View 查找实际的 View。 DispaterServlet 把返回的 Model 传给 View（视图渲染）。 把 View 返回给请求者（浏览器） 上述流程是传统开发模式（JSP，Thymeleaf 等）的工作原理。 对于前后端分离时，后端通常不再返回具体的视图，而是返回纯数据（通常是 JSON 格式，Spring 会自动将其转换为 JSON 格式），由前端负责渲染和展示。 1. Spring 循环依赖了解吗，怎么解决？ 两个或多个 Bean 之间相互持有对方的引用。 @Component public class CircularDependencyA { @Autowired private CircularDependencyB circB; } @Component public class CircularDependencyB { @Autowired private CircularDependencyA circA; } 有三级缓存： 一级缓存（singletonObjects）：存放最终形态的 Bean（已经实例化、属性填充、初始化），单例池，为“Spring 的单例属性”⽽⽣。一般情况我们获取 Bean 都是从这里获取的，但是并不是所有的 Bean 都在单例池里面，例如原型 Bean 就不在里面。 二级缓存（earlySingletonObjects）：存放过渡 Bean（半成品，尚未属性填充，未进行依赖注入），也就是三级缓存中ObjectFactory产生的对象，与三级缓存配合使用的，可以防止 AOP 的情况下，每次调用ObjectFactory#getObject()都是会产生新的代理对象的。 三级缓存（singletonFactories）：存放ObjectFactory，ObjectFactory的getObject()方法（最终调用的是getEarlyBeanReference()方法）可以生成原始 Bean 对象或者代理对象（如果 Bean 被 AOP 切面代理）。三级缓存只会对单例 Bean 生效。 先去 一级缓存 singletonObjects 中获取，存在就返回； 如果不存在或者对象正在创建中，于是去 二级缓存 earlySingletonObjects 中获取； 如果还没有获取到，就去 三级缓存 singletonFactories 中获取，通过执行 ObjectFacotry 的 getObject() 就可以获取该对象，获取成功之后，从三级缓存移除B，并将B对象加入到二级缓存中。 步骤 1：创建 Bean A Spring 容器开始创建 Bean A，首先将 Bean A 的创建状态标记为 “正在创建”。 实例化 Bean A，将 Bean A 的早期引用（一个 ObjectFactory 对象）放入三级缓存 singletonFactories 中。这个 ObjectFactory 对象可以在需要时返回 Bean A 的早期实例。 开始对 Bean A 进行属性注入，发现 Bean A 依赖于 Bean B。 步骤 2：创建 Bean B Spring 容器开始创建 Bean B，同样将 Bean B 的创建状态标记为 “正在创建”。 实例化 Bean B，将 Bean B 的早期引用放入三级缓存 singletonFactories 中。 开始对 Bean B 进行属性注入，发现 Bean B 依赖于 Bean A。 步骤 3：解决 Bean B 对 Bean A 的依赖 当 Bean B 需要注入 Bean A 时，Spring 容器首先从一级缓存 singletonObjects 中查找 Bean A，发现没有找到。 接着从二级缓存 earlySingletonObjects 中查找，也没有找到。 然后从三级缓存 singletonFactories 中查找，找到了 Bean A 的早期引用（ObjectFactory 对象）。 调用 ObjectFactory 的 getObject() 方法，获取 Bean A 的早期实例。如果 Bean A 需要进行 AOP 代理，会在这里创建代理对象，并将代理对象放入二级缓存 earlySingletonObjects 中，同时从三级缓存 singletonFactories 中移除该引用。 将获取到的 Bean A 的早期实例注入到 Bean B 中。 步骤 4：完成 Bean B 的创建 Bean B 完成属性注入后，进行初始化操作。 将完全创建好的 Bean B 实例放入一级缓存 singletonObjects 中，并从二级缓存 earlySingletonObjects 和三级缓存 singletonFactories 中移除相关引用。 步骤 5：完成 Bean A 的创建 由于 Bean B 已经创建完成，将 Bean B 实例注入到 Bean A 中。 Bean A 完成属性注入后，进行初始化操作。 将完全创建好的 Bean A 实例放入一级缓存 singletonObjects 中，并从二级缓存 earlySingletonObjects 和三级缓存 singletonFactories 中移除相关引用。 1. 什么是 Spring Boot Starters? Spring Boot Starters 是一组便捷的依赖描述符，它们预先打包了常用的库和配置。当我们开发 Spring 应用时，只需添加一个 Starter 依赖项，即可自动引入所有必要的库和配置，快速引入相关功能。 在没有 Spring Boot Starters 之前，开发一个 RESTful 服务或 Web 应用程序通常需要手动添加多个依赖，比如 Spring MVC、Tomcat、Jackson 等。这不仅繁琐，还容易导致版本不兼容的问题。而有了 Spring Boot Starters，我们只需添加一个依赖，如 spring-boot-starter-web，即可包含所有开发 REST 服务所需的库和依赖。 这个 spring-boot-starter-web 依赖包含了 Spring MVC（用于处理 Web 请求）、Tomcat（默认嵌入式服务器）、Jackson（用于 JSON 处理）等依赖项。这种方式极大地简化了开发过程，让我们可以更加专注于业务逻辑的实现。 2. 介绍一下@SpringBootApplication 注解 @EnableAutoConfiguration: 启用 Spring Boot 的自动配置机制。它是自动配置的核心，允许 Spring Boot 根据项目的依赖和配置自动配置 Spring 应用的各个部分。 @ComponentScan: 启用组件扫描，扫描被 @Component（以及 @Service、@Controller 等）注解的类，并将这些类注册为 Spring 容器中的 Bean。默认情况下，它会扫描该类所在包及其子包下的所有类。 @Configuration: 允许在上下文中注册额外的 Bean 或导入其他配置类。它相当于一个具有 @Bean 方法的 Spring 配置类。 3. Spring Boot 的自动配置是如何实现的? Spring Boot 通过@EnableAutoConfiguration开启自动装配，通过 SpringFactoriesLoader 最终加载META-INF/spring.factories中的自动配置类实现自动装配. HTTP和HTTPS二者区别 HTTP 是超文本传输协议，信息是明文传输，存在安全风险的问题。HTTPS 则解决 HTTP 不安全的缺陷，在 TCP 和 HTTP 网络层之间加入了 SSL/TLS 安全协议，使得报文能够加密传输。 HTTP 连接建立相对简单， TCP 三次握手之后便可进行 HTTP 的报文传输。而 HTTPS 在 TCP 三次握手之后，还需进行 SSL/TLS 的握手过程，才可进入加密报文传输。 两者的默认端口不一样，HTTP 默认端口号是 80，HTTPS 默认端口号是 443。 HTTPS 协议需要向 CA（证书权威机构）申请数字证书，来保证服务器的身份是可信的。 对称+非对称、摘要算法+数字签名、身份证书 TCP、UDP区别、应用场景 MySQL 基础架构 1. 结构 连接器： 身份认证和权限相关(登录 MySQL 的时候)。 查询缓存： 执行查询语句的时候，会先查询缓存（MySQL 8.0 版本后移除，因为这个功能不太实用）。 分析器： 没有命中缓存的话，SQL 语句就会经过分析器，分析器说白了就是要先看你的 SQL 语句要干嘛，再检查你的 SQL 语句语法是否正确。 优化器： 按照 MySQL 认为最优的方案去执行。 执行器： 执行语句，然后从存储引擎返回数据。 执行语句之前会先判断是否有权限，如果没有权限的话，就会报错。 插件式存储引擎：主要负责数据的存储和读取，采用的是插件式架构，支持 InnoDB、MyISAM、Memory 等多种存储引擎。InnoDB 是 MySQL 的默认存储引擎，绝大部分场景使用 InnoDB 就是最好的选择。 2. SQL语句在MySQL中的执行过程 查询语句的执行流程如下：权限校验（如果命中缓存）--->查询缓存--->分析器--->优化器--->权限校验--->执行器--->引擎 更新语句执行流程如下：分析器---->权限校验---->执行器--->引擎---redo log(prepare 状态)--->binlog--->redo log(commit 状态) Redis单线程怎么监视IO连接？ Redis 通过 IO 多路复用程序 来监听来自客户端的大量连接（或者说是监听多个 socket），它会将感兴趣的事件及类型（读、写）注册到内核中并监听每个事件是否发生。 I/O 多路复用技术的使用让 Redis 不需要额外创建多余的线程来监听客户端的大量连接，降低了资源的消耗。 Redis I/O复用的三种实现方式？复杂度 建议deepseek问一下。 select：轮寻，O(n) poll：轮寻，O(n) Epoll: 基于事件触发，O(1) Redis ZSet的底层实现 重点关注跳表的原理，复杂度，为什么要采用两种实现。 缓存击穿、缓存穿透、缓存雪崩 如何解决？是结合黑马点评问的。 如何保障数据库和缓存的一致性。 旁路缓存模式：先修改数据库，在删除缓存。 说说hashmap，线程安全吗？哈希冲突怎么解决？说说扩容？升级红黑树的阈值，降为链表的阈值？为什么二者不一样。 看javaguide。 阈值是8和6，防止频繁的转换。 聊聊JVM的内存区域 很重要，建议javaguide。 线程的：栈、程序计数器。 进程的：堆。方法区 聊聊垃圾回收器G1的回收过程？优点是什么？ javaguide。 聊聊对象在JVM的移动（Eden->S1和S2->老年代），为什么老年区占空间大于新生区？ 很重要，注意数组等大对象直接放在老年区。 面向对象三个特点？多态的实现方式？ 见javaguide。 重载和重写的区别？ 见javaguide。关注一下重载发生在编译器，重写发生在运行期。 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-02-23 15:02:48 "},"Chapter4/MySQL.html":{"url":"Chapter4/MySQL.html","title":"MySQL","keywords":"","body":"MySQL的插入、修改删除 插入操作 方式一：经典的插入 insert into table_name(col, ...) value(val, ...) 注意点： 插入的值的类型必须和列的类型一致或者兼容 不允许为null的必须插入值 列的顺序可以调换 列的数量和值的数量一致 可以省略列名，但值必须顺序一致 方式二： insert into table_name set col_name=val, cal_name1=val1... 两个方式对比： 方式一可以一次插入多条数据，方式二不行 方式一可以插入子查询结果，方式二不行修改操作 语法：update table_name set col=val where condition # 修改多表 update 表1 别名,表2 别名 set 列=值,... where 连接条件 and 筛选条件; # 修改多表2 update 表1 别名 inner/left/right join 表2 别名 on 连接条件 set 列=值,... where 筛选条件 案例: 修改没有男朋友的女神的男朋友都为2号update boys bo right join beauty b on bo.id = b.boyfriend_id set b.boyfriend_id = 2 where b.id is null 删除操作 方式一：delete 语法: 1.单表的删除★★★ delete from 表名 where 筛选条件 2.多表的删除[补充] 语法: delete 表1/表2的别名 from 表1 别名,表2 别名 where 连接条件 and 筛选条件; 案例：删除张无忌女朋友的信息delete b from beauty b inner join boys bo on b.boyfriend_id=bo.id where bo.name='zhangwuji; 语法: delete 表1/表2的别名 from 表1 别名 inner/left/right join 表2 别名 on 连接条件 where 筛选条件; 案例：删除张无忌女朋友的信息以及张无忌的信息delete b, bo from beauty b inner join boys bo on b.boyfriend_id=bo.id where bo.name='zhangwuji; 方式二:truncate 或者drop语法:truncate table 表名;drop table 表名;truncate table boys; 两者比较1.truncate删除,效率高一丢丢, 不允许加where2.假如要删除的表中有自增长列,如果用delete删除后,再插入数据,自增长的值从断点开始, 而truncate删除后,再插入数据,自增长列的值从1开始.3.truncate删除没有返回值,delete删除有返回值4.truncate删除不能回滚,delete删除可以回滚 5.truncate会清空表中的所有行，但表结构及其约束、索引等保持不变；drop会删除表的结构及其所依赖的约束、索引等。可以简单理解为：一本书，delete是把目录撕了，truncate是把书的内容撕下来烧了，drop是把书烧了 Copyright © 版权信息 all right reserved，powered by aspire-zero and Gitbook该文件修订时间： 2025-03-18 21:51:08 "}}